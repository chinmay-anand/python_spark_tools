{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enabling Jupyter shell to print multiple results form a single shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable the shell to print multiple results (instead of only the last result)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import modules form another ipynb (jupuyter notebook written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs  # Boilerplate required\n",
    "\n",
    "# Do a full import\n",
    "# from .full.Chinmay_Utilities import foo\n",
    "\n",
    "# Do a definitions-only import\n",
    "from .defs.Chinmay_Utilities import getCallLogXmlFromSuperbackup, getSparkDFfromPandasDF, getJsonFromSparkDF, printTextFile  #, getPandasDFfromSparkDF\n",
    "\n",
    "# We can \"import ipynb.fs.defs.Chinmay_Utilities\" instead of two imports \"import ipynb.fs\" followed by \".defs.Chinmay_Utilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing contents from a plain text file (.txt, csv, .xml, .csv etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "#from codecs import open\n",
    "#Print contents of a file\n",
    "def printTextFile(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf-8')\n",
    "    file_contents = f.read()\n",
    "    print (file_contents)\n",
    "    f.close()\n",
    "</ore>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First define all the local sample dat files in local variables to be used in the exercises below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_csv_app_stocks = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/appl_stock.csv\"\n",
    "my_csv_sales_info = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/sales_info.csv\"\n",
    "my_csv_contains_null = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/ContainsNull.csv\"\n",
    "my_json_people = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/people.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer Documentation for pyspqrk.sql package at https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting help on a method in a builder pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the help of a method / attribute in a builder pattern, \n",
    "#     split the pattern just before that method\n",
    "#     set a variable with the builder pattern result just before that method call, \n",
    "#         so that the method call can be performed on the variable.\n",
    "#     Now execute (SHIFT+ENTER) the help syntax i.e. \"method?\"\" NOT \"method()?\"\" on that variable\n",
    "#     \n",
    "# Below is an example for getting help on getOrCreate() method in \"SparkSession.builder.appName('Basics').getOrCreate()\"\n",
    "#     \n",
    "bld = SparkSession.builder.appName('Basics')\n",
    "# bld.getOrCreate??  ### Uncomment this line to get the help (\"?\") and code implementation (\"??\")\n",
    "#     \n",
    "# Here we can not use \"SparkSession.builder.appName('Basics').getOrCreate?\", because there is a use input involved (i.e. parameter of appName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment specific function below and run this shell to get help\n",
    "# getCallLogXmlFromSuperbackup?\n",
    "# getSparkDFfromPandasDF?\n",
    "# getJsonFromSparkDF?\n",
    "# printTextFile??\n",
    "# getPandasDFfromSparkDF?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read a json using SparkSession and analyse the result databrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a SparkSession\n",
    "spark1 = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\n",
      "{\"name\":\"Andy\", \"age\":30}\n",
      "{\"name\":\"Justin\", \"age\":19}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a sample spark session into a spark dataframe\n",
    "printTextFile(my_json_people)\n",
    "sdf = spark1.read.json(my_json_people)\n",
    "\n",
    "# Types of files that can be read csv/format/jdbc/json/load/option/options/orc/parquet/schema/table/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[age: bigint, name: string]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema automatically decides the schema based on data.\n",
    "sdf.printSchema()\n",
    "\n",
    "sdf.columns\n",
    "\n",
    "sdf.describe\n",
    "\n",
    "sdf.describe()\n",
    "\n",
    "# describe() given summary of numeric columns in the dataframe\n",
    "sdf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modify the JsonSchema using a user defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many times spark can not determine the data types in a json correctly and specifies each of the fields as String.\n",
    "# In this case we can define a schema and attach it to the json\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,IntegerType,false),StructField(name,StringType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "StructField(age,IntegerType,false)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "StructField(name,StringType,true)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StructField(field_name, field_type(), is_field_nullable)\n",
    "# To enforce the user defined scheme to a json pass a list of structfields one for each column\n",
    "data_schema = [StructField ('age', IntegerType(), False), \n",
    "                StructField('name', StringType(), True)]\n",
    "\n",
    "final_type = StructType(fields=data_schema)\n",
    "\n",
    "final_type\n",
    "final_type[\"age\"]\n",
    "final_type[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the same json using the user defined schema (earlier it was the default one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\n",
      "{\"name\":\"Andy\", \"age\":30}\n",
      "{\"name\":\"Justin\", \"age\":19}\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printTextFile(my_json_people)\n",
    "sdf2 = spark1.read.json(my_json_people, schema=final_type)\n",
    "\n",
    "sdf2.printSchema()\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer back the spark dataframe with builtin default schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking a method to get the sample call log using a module from another ipynb file in the same folder\n",
    "* ###### sparkCallSession.createDataFrame(p_df) --> converts pandas dataframe into spark dataframe\n",
    "* ###### s_df.select(\"*\").toPandas() --> converts spark dataframe into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the call log xml data in a Pandas DataFrame\n",
    "dfCallLogs = getCallLogXmlFromSuperbackup()   ## Calling from Chinmay_Utilities.ipynb\n",
    "\n",
    "# Convert the Pandas DataFrame into Spark DataFrame\n",
    "sdfCallLogs = getSparkDFfromPandasDF(dfCallLogs)   ## Calling from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Select statements with Spark\n",
    "* We can use limited select statement (upto selection of columns, adding computed columns and without any where clause)\n",
    "* * spark dataframe can use complex where clause as explained below\n",
    "* To use full version of select sql along with where clause we need to register the Spark DataFrame as a table using the method below.\n",
    "* * sparkDF.createOrReplaceTempView(pseudo_tableView_name)\n",
    "* * This is used only with sparkSessn.sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the spark dataframe as a table/view to be used like standard sql using sparkSession.sql()\n",
    "sdfCallLogs.createOrReplaceTempView(\"call_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using pure SQL with DataFrames\n",
    "* Register the dataframe as a table and\n",
    "* Use sparkSession1.sql(SQL_STMT_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering using SQL\n",
    "* ###### sparkSesn.sql(full_sql)\n",
    "* ###### This works but supports limited where clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Using pure SQL with DataFrames\n",
    "* Register the dataframe as a table and\n",
    "* Use sparkSession1.sql(SQL_STMT_table)spark1.sql(\"SELECT * FROM call_logs WHERE dur > 100 ORDER BY dur DESC\").show()\n",
    "* The where clause of this sql is not supporting LIKE clause\n",
    "* This complex where clause (LIKE clause) is possible through direct \"where\" clause on spark dataframe (next statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Quick filtering: (one of the two alternate ways on sparkDF)\n",
    "* ###### sparkDF.select(*).where(my_condition)\n",
    "* ###### sparkDF.filter(my_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_where_clause = \"upper(name) like '%SEEC%QA%' AND dur > 50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the Spark DataFrame directly with WHERE clause\n",
    "# This sort of complex where clauses (e.g. LIKE clauses) are not possible with pandas dataframe\n",
    "\n",
    "# sdfCallLogs.select(\"*\").where(\"upper(name) like '%SEEC%QA%'\").show()\n",
    "\n",
    "# Convert the filtered data into pandas data frame which can be processed or outputted into a file\n",
    "df_qa = sdfCallLogs.select(\"*\").where(my_where_clause).toPandas()\n",
    "df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfCallLogs.filter(my_where_clause).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns before converting to Pandas DataFrame\n",
    "df_qa2 = sdfCallLogs.filter(my_where_clause).select('name', 'number', 'dur', 'time') \\\n",
    "    .withColumnRenamed('dur', 'Duration (Sec)').withColumnRenamed('name', 'Name')\\\n",
    "    .withColumnRenamed('time', 'Date').withColumnRenamed('number', 'Phone Number').toPandas()\n",
    "\n",
    "# Below line inserts a new column with value double of the current 'dur' columns value\n",
    "df_qa2B = sdfCallLogs.filter(my_where_clause).select('name', 'number', 'dur', 'time') \\\n",
    "            .withColumn('double_duration',sdfCallLogs['dur']*2).toPandas()\n",
    "\n",
    "# To show a pd.DataFrame without column index\n",
    "df_qa2.style.hide_index()\n",
    "df_qa2B.style.hide_index()\n",
    "\n",
    "# To write an dataframe to an excel file without index column\n",
    "df_qa2.to_excel('1.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfCallLogs.filter(my_where_clause).filter((sdfCallLogs['dur']>300) & ~(sdfCallLogs['dur']<1000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfCallLogs.filter(my_where_clause).filter('dur>300 and dur>=1000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Convert the Spark DataFrame into Json\n",
    "###### Convert a Pandas DataFrame into Json by first converting into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getJsonFromSparkDF(sdfCallLogs)   ## Calling from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get Json from a pandas DataFrame fist convert itinto a Spark DataFrame and then get Json from it\n",
    "getJsonFromSparkDF(getSparkDFfromPandasDF(dfCallLogs))   ## Calling from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check types of DataFrame, Columns and displaying selected columns as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| Age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type(sdf)\n",
    "type(sdf['Age'])\n",
    "#sdf['Age'].show()         # This line does nto work as we can not display columns\n",
    "sdf.select('Age').show()   # This returns a dataframe of selected columns\n",
    "type(sdf.select('Age'))\n",
    "\n",
    "sdf[sdf['Age']>0].show()    # This works similar to regular pandas dataframe filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display rows form top of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.head(10) # display atmost 10 rows from top of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming a column and Inserting a computed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|old_age|   name|\n",
      "+-------+-------+\n",
      "|   null|Michael|\n",
      "|     30|   Andy|\n",
      "|     19| Justin|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1 = sdf.withColumnRenamed('age', 'old_age')\n",
    "type(sdf1)\n",
    "sdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming a column and Inserting a computed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2 = sdf.withColumn('double_age',sdf['age']*2)\n",
    "type(sdf2)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with GroupBy and Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEATING DEFINITION (for readability)\n",
    "my_csv_sales_info = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/sales_info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSesnGrpby = SparkSession.builder.appName(\"chin_groupby\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf_sales = sparkSesnGrpby.read.csv(my_csv_sales_info, inferSchema=True, header=True)\n",
    "# allow spark to assume first row as the column names and to decide the data type from data value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_sales.printSchema()\n",
    "\n",
    "sdf_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1c140c1dfc8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|Company|    Average Sales|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_sales.groupBy(\"Company\")    # returns pysdf_sales.groupBy(\"Company\").mean().show()spark.sql.group.GroupedData\n",
    "\n",
    "# Aggregators on top of GroupedData returns a spark DataFrame for our consumption\n",
    "# Various gorubby aggregator functions: mean/sum/max/min\n",
    "sdf_sales.groupBy(\"Company\").count().show()\n",
    "sdf_sales.groupBy(\"Company\").mean().withColumnRenamed('avg(Sales)','Average Sales').show()\n",
    "sdf_sales.groupBy(\"Company\").sum().show()\n",
    "sdf_sales.groupBy(\"Company\").max().show()\n",
    "sdf_sales.groupBy(\"Company\").min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Company='APPL', sum(Sales)=1480.0),\n",
       " Row(Company='GOOG', sum(Sales)=660.0),\n",
       " Row(Company='FB', sum(Sales)=1220.0),\n",
       " Row(Company='MSFT', sum(Sales)=967.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_sales.groupBy(\"Company\").sum().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data_by_company = sdf_sales.groupBy('Company')\n",
    "group_data_by_company.sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Company\": \"APPL\", \"sum(Sales)\": 1480.0}, {\"Company\": \"GOOG\", \"sum(Sales)\": 660.0}, {\"Company\": \"FB\", \"sum(Sales)\": 1220.0}, {\"Company\": \"MSFT\", \"sum(Sales)\": 967.0}'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getJsonFromSparkDF(group_data_by_company.sum())  # using function from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
