{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online ML repositories\n",
    "Sources: Kaggle, Google, Amazon, Microsoft, ML Repositories at UC Irvine and Carnegie Melon and LionBridge\n",
    "<details><summary>Online ML datasets..EXPAND</summary>\n",
    "\n",
    "*. https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378b\n",
    "1. Goole's Datasets Search Engine (https://datasetsearch.research.google.com/)\n",
    "2. Kaggle  (https://www.kaggle.com/datasets)\n",
    "3. Amazon  (https://registry.opendata.aws/)\n",
    "4. UC Irvine ML Repository (https://archive.ics.uci.edu/ml/index.php)\n",
    ">* https://archive.ics.uci.edu/ml/datasets.php\n",
    "5. Visual Data on computer vision (https://www.visualdata.io/)\n",
    "6. Carnegi Melon Datasets (https://guides.library.cmu.edu/machine-learning/datasets)\n",
    "7. Microsoft Datasets (https://msropendata.com/)\n",
    "8. Government Datasets (EU, US, NZ, India, Ireland)\n",
    "9. Awesome Public Datasets at GitHub (https://github.com/awesomedata/awesome-public-datasets)\n",
    "10. LionBridge Dataset (https://lionbridge.ai/datasets/)\n",
    "4. https://elitedatascience.com/datasets\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised ML Algorithms\n",
    "* <b>Clustering:</b> K-Means Clutering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEPS\n",
    "* Load Data\n",
    "* Prepare Data\n",
    "    * null handling (na handling),\n",
    "    * category indexing (StringIndexer) and binary vectoriation (OneHotEncoder)\n",
    "    * vecor assembly transformation from input columns to 'features', the combined vector column (.transform)\n",
    "    * scaling of combined 'features' column to 'scaledFeatures' using StandardScaler (.fit.transform)\n",
    "        * Scaling is very useful when there is a big difference between features and scaledFeatures.\n",
    "        * Scaling is also useful when fields vary in orders of large magnitudes, say one column expressed in thousands of miles and another column in milimeters, then scaling helps.\n",
    "* Create KMeans model with 'scaledFeatures' and desired K value (matching with number of suspected clusters) in the constructor.\n",
    "* Train this model with the vectorized scaled input dataframe and that is our final kmodel (.fit)\n",
    "* kmodel.clusterCenters() gives an list of feature arrays, each array representing a centroid of a cluster\n",
    "    * Each array represents a single point of n-dimension and is called a centroid.\n",
    "    * The number of arrays will be same as value of k used, i.e. number of clusters.\n",
    "    * The count of elements in each array is equal to the number of features.\n",
    "* Run trained_model.transform(sdfScaledVectorFeatures) to get the result dataframe with predictions, where we can check the 'prediction' column. We can also check the 'prediction' column from the model (kmodel.summary.predictions.select('prediction'))\n",
    "* We can evaluate the model using one of the following ways.\n",
    "    1. kmodel.computeCost(sdfScaledVectorFeatures)\n",
    "    2. ClusteringEvaluator('prediction', 'scaledFeatures').evaluate(kmodel.summary.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisd ML Algorithms\n",
    "* <b>Regression:</b> LinearRegression\n",
    "* <b>Classification:</b> LogisticRegression, DecisionTreeClsssifier, RandonForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning with various regressor and classifiers\n",
    "\n",
    "<pre>\n",
    "<details><summary>BUILD SESSION & LOAD DATA</summary>\n",
    "###* <b>It is same for all models</b>\n",
    "spark1 = SparkSession.builder.appName('lr_example').getOrCreate()\n",
    "##all_data = spark1.read.format(\"libsvm\").option(\"numFeatures\", 10).load(\"sample_linear_regression_data.txt\")\n",
    "all_data = spark1.read.csv('Ecommerce_Customers.csv', inferSchema=True, header=True)\n",
    "\n",
    "</details>\n",
    "<details><summary>PREPARE DATA</summary>\n",
    "<i>\n",
    "###* <b>It is same for all models</b>\n",
    "#* Use StringIndexer and optionally OneHotEncoder to convert categorical values into integers and then to binary vectors (optional)\n",
    "#* For categorical columns use only the indices or binary vectorized category indices or binary vectors\n",
    "#* Categorical text values need to be replaced with categorical indices using StringIndexer.\n",
    "#* If there is an ordinal relationship between the values of a categorical field then the categorical indices can seve as a feature. E.g. different shades of a  color  or temperature such as Cold, Warm, and Hot.\n",
    "#* If there is no relation in the values of the field, then we need to convert those categorical indices into binary vector where each value represents exactly single one value and rest zeroes, and this binary vector is more expressive for the ML algorithms to handle\n",
    "#* The cateorical indices can be converted into binary vector using OneHotEncoder if no ordinal relation is detected between the different values of a specific categorical field.\n",
    "#*   * e.g. temperature of 'Cold, 'Warm, 'Hot' are having ordinal relationship but a animal field with values 'dog'and 'cat' does not have any ordinal relationship.\n",
    "#* Finally VectorAssembler is used to prepare the 'features' column for use with spark MLlib functions\n",
    "</i>\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['col1','col2','ol3'], outputCol='features')\n",
    "final_allfield_data = assembler.transform(all_data)   # This adds 'features' column to the result\n",
    "final_data = final_allfield_data.select('features', 'my_label')  #* Spark ML algos need only these two columns\n",
    "</code>\n",
    "\n",
    "#* MLlib from spark needs only two columns 'features' and the label (the field to be predicted)\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])    #Data is split into 70% (i.e. 0.7) for train_data and 30% for test_Data into two dataframes\n",
    "\n",
    "</details>\n",
    "<details><summary>TRAIN THE MODEL WITH TRAINING DATA</summary>\n",
    "###* <b>It is similar, but</b> the constructors for different models differ in their parameters according to algorithm implementation.\n",
    "<u>Create the models</u>\n",
    "* linear=LinearRegression(labelCol='label', featuresCol='features', predictionCol='prediction')\n",
    "* logistic=LogisticRegression(labelCol='label', featuresCol='features', predictionCol='prediction')\n",
    "* dtc = DecisionTreeClassifier(labelCol='PrivateIndex',featuresCol='features')  ##* Here label is a categorical value which is indexed using StringIndexer and used in the tree algorithm\n",
    "* rfc = RandomForestClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "* gbt = GBTClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "\n",
    "<u>Train the models</u>\n",
    "trained_mdoel = model.fit(train_data)  #* Here 'model'  is one from linear / logistic / dtc / rfc / gbt\n",
    "</details>\n",
    "<details><summary>EVALUATE THE MODEL WITH TEST DATA (optional but helps improving the model by finetuning manually)</summary>\n",
    "* test_results = trained_model.evaluate(test_data)  #* For Linear Regression and Logistic Regression use .evaluate()\n",
    "* test_results = trained_model.transform (test_data) #* For tree based classifiers use .transform()\n",
    "#* THIS EVALUATION helps us to finetune by changing model construction parameters and train_test_split porportion.\n",
    "#* Test data evaluation result can be treated similar to trained_model.summary\n",
    "#* the trained_model<b>.featureImportances</b> actually tells the relative importance of the various features of th etrained model that contributes maximum towardds the label prediction.\n",
    "\n",
    "    <details><summary>What to check for evaluation</summary>\n",
    "    * For Linear Regression check test_results.residuals DF, which shows a list of errors i.e. difference of predictions from the observations\n",
    "    * For all classifiers check test_results.predictions DF, which shows the binary 'prediction' column (0/1) to be compared against the label field (0/1) along with two more fields rawPrediction and probability\n",
    "        * This applies to Logistic Regression, DecisionTreeClassifier, RandomForestClassfier, GBTClassifier (GBT = GradientBoostedTree.\n",
    "        * test_result.predictions is a dataframe with 5 fields [ 2 input fields (label, features) + 3 fields rawPrediction, probability, prediction (0 or 1)]\n",
    "        * In \"predictions\" DF we need to compare prediction field against the label field to find the model performance.\n",
    "    </details>\n",
    "    <details><summary>Using Evaluators for classifiers</summary>\n",
    "        * We can use BinaryClassificationEvaluator to find AUC (Area Under Curve [metricName=areaUnderROC or areaUnderPR] -- 0.5 means random guess, higher is better. ROC (Receiver Operator Curve) is default, Other is PR (Precision Recall curve)\n",
    "        * We can use MulticlassClassifiactionEvaluator to find metrics f1(default), accuracy, weightedPrecision, weightedRecal\n",
    "        * Most of these metrics are also available directly from the evaluation result \"test_result\".\n",
    "        * I could not find for areaUnderPR and f1 attributes in test_result.\n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "<details><summary>DEPLOY THE MODEL (i.e. DO PREDICTION)</summary>\n",
    "unlabeled_data = test_data.select('features')\n",
    "predicted_data = trained_model.transform(unlabeled_data)\n",
    "    \n",
    "<details><summary>If the unlabeled data is a new data set</summary>\n",
    "* If there is a separate dataset available for deployment, then once we are satisfied with the test_data results we can retrain the model with the entire pre-split data (i.e. train+test) and use this full_model to transform the deployment data.\n",
    "##Use same model instance 'model' but \n",
    "#* use the full pre-split data set 'final_data' instead of train_data (70% of final_data)M\n",
    "final_model = model.fit(final_data)\n",
    "\n",
    "#* Prepare the 'features' column for the new unlabeled data\n",
    "new_customers = spark.read.csv('new_customers.csv',inferSchema=True, header=True)\n",
    "test_new_customers = assembler.transform(new_customers)\n",
    "\n",
    "#* To Predict, the trained model needs only 'features' column, and rest of the fields are not utilised, so we need not remove these extra columns\n",
    "final_predicted_result = final_model.transform(test_new_customers);\n",
    "</details>\n",
    "    \n",
    "</details>\n",
    "<details><summary>Using a PIPELINE (to prepare data, train a model and evaluate it)</summary>\n",
    "We can optionally use pipeline (for simplicity) if there are a number of repeatitive tasks such as StringIndexer and OneHotEncoder  need to be applied for multiple categorical features in the input.\n",
    "\n",
    "* my_pipeline = Pipeline(stages=[genderIndexer, embarkIndexer, genderEncoder, embarkEncoder, vecAssembler, myLogisticRegressionModel])\n",
    "\n",
    "* Train the model with train data using\n",
    "my_pipeline.fit(train_data)\n",
    "\n",
    "* Evaluate the model by predicting test data using\n",
    "my_pipeline.transform(test_data)\n",
    "\n",
    "* pipeline.transform() uses the field 'prediction' not 'rawPrediction', so we need to explicitly tell Binary evaluator to use 'prediction column instead of default pick of rawPrediction field. This we can achieve by using rawPrediction = 'prediction' in binary evaluator constructor.\n",
    "* So when using binary evaluator to evaluate the result from a pipeline we need to use rawPrediction='prediction' in binary evaluator contstructor.\n",
    "* With MulticlassClassificationEvaluator default is 'prediction' column so no issues here.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Linear Regression</b> - Some Theory...EXPAND</summary>\n",
    "    \n",
    "* Spark Machine Learning algorithms work only on two columns (features and label) for superised and one column (features) for unsupervised models\n",
    "* Data_Transformations.ipynb\n",
    "    * <b>StringIndexer : </b> converts catgorical values (or group names) found in a column into a numeric categorical index so that this categorical field can be used as an input feature.\n",
    "    * <b>VectorAssembler : </b> combines multiple numeric,boolean, or vector input columns into a single vectorized feature column (default name is 'features')\n",
    "    * Model training method \".fit()\" uses this 'features' combined vectorized column.\n",
    "    * the data needs to be in the form of two columns (\"label\",\"features\") before Spark can accept the data!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Linear Regression</b> - Steps... EXPAND</summary>\n",
    "\n",
    "<details><summary>Load input data</summary>\n",
    "    \n",
    "`    sdf_initial_data = spark1.read.csv('cruise_ship_info.csv',inferSchema=True,header=True)`\n",
    "</details>\n",
    "<details><summary>Prepare the data</summary>\n",
    "    \n",
    "    * Convert categorical values into categorical index values using StirngIndexer\n",
    "    * Prepare a combined 'features' vectorized column (using VectorAssembler) holding all the numeric, boolean of other Vector.dense() types\n",
    "    * The input dataframe for the training dataset need to be in the form of two columns (\"label\",\"features\") for supervised model.\n",
    "        * 'features' - It is the vector  column holding all the numeric, boolean or another vector types\n",
    "        * 'label' - It is the column we want to predict for an unlabeled data set later.\n",
    "    * Split the final input data into training_set and test_set\n",
    "        * use sdf_final_data.randomSplit([0,7, 0.3]) to split the data into 70:30 percent proportaion.\n",
    "        * each with two columns \"label\",\"features\"\n",
    "<b>CODE</b>\n",
    "    <code>\n",
    "    indexer = StringIndexer(inputCol=\"Cruise_line\", outputCol=\"cruise_cat\")\n",
    "    indexed = indexer.fit(sdf_initial_data).transform(sdf_initial_data)\n",
    "    assembler = VectorAssembler( inputCols=['Age',...], outputCol=\"features\")\n",
    "    output = assembler.transform(indexed)\n",
    "    final_data = output.select(\"features\", \"crew\")\n",
    "    train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "    </code>\n",
    "</details>\n",
    "<details><summary>Train the model with the training data</summary>\n",
    "\n",
    "* Train the model with the training data\n",
    "    <code>\n",
    "    lr = LinearRegression(labelCol='Yearly Amount Spent')   ## Create a Linear Regression Model object\n",
    "    lrModel = lr<b>.fit</b>(train_data)   # Fit the model to the data and call this model lrModel\n",
    "    </code>\n",
    "</details>\n",
    "<details><summary>Evaluate the performance of the trained model</summary>\n",
    "\n",
    "* Evaluate the performance of the model by running it on the test data\n",
    "    * `test_results = lrModel<b>.evaluate</b>(test_data)`\n",
    "    * From the trained model (lrModel) check the coefficients and intercept of the trained model\n",
    "    * From the evaluated test result check the tresiduals. We can ALSO check the residuals from the \"summary\" member of trained model (lrModel). Find the discrepancy between to two for your reference.\n",
    "    * Check `test_results.residuals.show()` and `lrModel.summary.residuals.show()`\n",
    "</details>\n",
    "<details><summary>Run the model on unlabelled data</summary>\n",
    "\n",
    "* Run the model on unlabvelled data (assuming production data)\n",
    "    * Assumption is that machine must not have seen the production data on which it si going to be deployed, in a simple case test data is unseen by the model as it is not used in training the model\n",
    "    * If we donot have separate unlabeled data\n",
    "        * prepare unlabeled data by stripping off the labels from the test data\n",
    "        * Transform() the unlabelled data using the available model\n",
    "        * The result will have a 'prediction' column against for the 'features'.\n",
    "        <code>\n",
    "            * unlabeled_data = test_data.select('features')    #Remember only 'features' column in the unlabelled data\n",
    "            * predicted_results = lrModel.transform(unlabeled_data)\n",
    "        </code>\n",
    "    * If we have separate unlabeled data, then we need to do somne extra work.\n",
    "        * We need to use StringIndexer and VectorAssembler to prepare a dataframe with single vectorized 'features' column.\n",
    "        * We can now create a new trained model using the entire initial input data without any train-test-split (as more data means better training).\n",
    "        * Use transform() method of this \"all-data\" trained model to get the prediction result dataframe.\n",
    "        * The result will have a 'prediction' column against for the 'features'.\n",
    "</details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<details><summary><b>Linear Regression</b> - A complete example</summary>\n",
    "\n",
    "<pre>\n",
    "<b><u>LOAD INPUT DATA</u></b>\n",
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.appName('cruise').getOrCreate()\n",
    "df = spark1.read.csv('cruise_ship_info.csv',inferSchema=True,header=True)\n",
    "\n",
    "<u>Check Loaded Data</u>\n",
    "df.printSchema()\n",
    "df.show()\n",
    "df.describe().show()\n",
    "\n",
    "##Dealing with the Cruise_line categorical variable\n",
    "#Ship Name is a useless arbitrary string, but the cruise_line itself may be useful. Let's make it into a categorical variable!\n",
    "\n",
    "df.groupBy('Cruise_line').count().show()\n",
    "\n",
    "<b><u>PREPARE THE DATA</u></b> and <u>format for MLlib</u>\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Cruise_line\", outputCol=\"cruise_cat\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.head(5)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "indexed.columns\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=['Age',\n",
    "             'Tonnage',\n",
    "             'passengers',\n",
    "             'length',\n",
    "             'cabins',\n",
    "             'passenger_density',\n",
    "             'cruise_cat'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(indexed)\n",
    "\n",
    "final_data = output.select(\"features\", \"crew\")\n",
    "\n",
    "<u>Test Train Split</u>\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "<b><u>TRAIN THE MODEL WITH TRAINING DATA</u></b>\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Create a Linear Regression Model object\n",
    "lr = LinearRegression(labelCol='crew')\n",
    "\n",
    "#Fit the model to the data and call this model lrModel\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "<b><u>EVALUATE THE PERFORMANCE OF THE TRAINED MODEL</u></b>\n",
    "#Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))\n",
    "\n",
    "test_results = lrModel.evaluate(test_data)\n",
    "\n",
    "print(\"RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"MSE: {}\".format(test_results.meanSquaredError))\n",
    "print(\"R2: {}\".format(test_results.r2))\n",
    "\n",
    "#R2 of 0.86 is pretty good, let's check the data a little closer\n",
    "\n",
    "<u>Find the correlation between the label('crew') and the predictors or features('passengers', 'cabins' etc) </u>\n",
    "##If any of the predictors or features has high correelation, then the training may not be accurate.\n",
    "\n",
    "from pyspark.sql.functions import corr\n",
    "df.select(corr('crew','passengers')).show()\n",
    "df.select(corr('crew','cabins')).show()\n",
    "\n",
    "##Here we notice that there is a high correlation between our label (the 'crew' field) and some of the features such as cabins and the passenger count. So this prediction may not be accurate.\n",
    "\n",
    "<b><u>PREDICT THE UNLABELLED (simulted) DATA USING THE MODEL</u></b>\n",
    "unlabeled_data = test_data.select('features')    #Remember only 'features' column in the unlabelled data\n",
    "predicted_results = lrModel.transform(unlabeled_data)\n",
    "\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Various Converters for data preparation\n",
    "vector columns\n",
    "* \"StringIndexer\"   [.fit().transform()]\n",
    "    * This is used to convert categorical values into numeric categorical indices to be fed into the vector assembler input.\n",
    "    * The resultant index column is used in input to vectorizer if the different values of the categorical field are related to each other.\n",
    "* \"OneHotEncoder\"  [.transform()]\n",
    "    * This is used to convert the numeric categorical indices into binary vector.\n",
    "    * The resultant index column is used in input to vectorizer if the different values of the categorical field are not related.\n",
    "* \"VectorAssembler\"  [.transform()]\n",
    "    * This generates a vecor column (default named 'features') combining all the passed in numeric, boolean or other individual vectors\n",
    "    * \"StandardScaler\"  [.fit().transform()]\n",
    "    * This is used to scale the vectorized 'features' into 'scaledFeatures' for K-Means clustering algorithm\n",
    "----------\n",
    "* StringIndexer and OneHotEncoder transform a single column before vectorization\n",
    "* StandardScaler scales the vectorized column.\n",
    "* So if all are to be used at same time then below is the flow:\n",
    "    * (String Indexer, OneHotEncoder)-->VectorAssembler-->StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Spark DataFrames\n",
    "<details><summary>Notes on Sparks Dataframes</summary>\n",
    "\n",
    "Source: \n",
    "* chin_1_Spark DataFrame Basics.ipynb\n",
    "* chin_1_Spark DataFrame Basics.ipynb\n",
    "    * SQL operations on sdf\n",
    "    * groupBy and aggregations\n",
    "    * orderBy default ascending and descending\n",
    "    * Handling missing data (na drop, replacing with fixed string  or with a computed value)\n",
    "* chin_3_Spark DataFrame Exercise 9.1.ipynb\n",
    "    * groupBy\n",
    "    * changing data type of data frame column using sdf[\"col1\"].cast('float') or sdf.selectExpr(\"cast(col1 as double\") with selectExpr or \n",
    "\n",
    "<pre>\n",
    "<details><summary>Loading a json file into a spark dataframe...EXPAND</summary>\n",
    "    \n",
    "* Use the .read() method from the spark session to create a DataFrame from a local file (.csv, .json etc)\n",
    "my_json_people = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/people.json\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.appName('Basics').getOrCreate()\n",
    "sdf = spark1.read.json(my_json_people)\n",
    "\n",
    "</details>\n",
    "<details><summary>Changing the schema of a json file while reading into the dataframe...EXPAND</summary>\n",
    "\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType\n",
    "data_schema = [StructField ('age', IntegerType(), False), \n",
    "                StructField('name', StringType(), True)]\n",
    "\n",
    "final_type = StructType(fields=data_schema)\n",
    "sdf2 = spark1.read.json(my_json_people, schema=final_type)\n",
    "\n",
    "* Set a custom schema to the DataFrame through the sparkSession.read.json() operation\n",
    ">* Use StructField, StructType and various base types IntegerType, StringType from pyspark.sql.types to change the schema of a json being imported into a spark DataFrame.\n",
    ">* Use StructField(fieldName, StringType(), True) to define json type of a field of String type. For int type use IntegerType()\n",
    ">* Use a list of StructField entries representing all the columns of the json file\n",
    ">* Create the StructType schema (i.e. StructType(fields=list_of_StructFields)) and pass it as \"schema\" parameter to the sparksession read method (i.e. .read.json(jsonFile,schema=new_json_schema))\n",
    ">* The DataFrame returned above will have specific data types instead of Strings. Validate it using sparkDF.printSchema()\n",
    "\n",
    "</details>\n",
    "<details><summary>Methods from my library tool \"chinmay_tools.py\"</summary>\n",
    "\n",
    "* Make notebook shells enabled to print multiple results form a single shell\n",
    "* Read and print a plain text (csv, txt, xml etc) file\n",
    "\n",
    "* Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pandasDF = sparkDF.select(\"*\").toPandas()\n",
    "\n",
    "* Get the call log xml data in a Pandas DataFrame\n",
    "dfCallLogs = getCallLogXmlFromSuperbackup()   ## Calling from Chinmay_Utilities.ipynb uses xml.etree.ElementTree and pandas\n",
    "\n",
    "* Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "sparkDF = tempSparkSession.createDataFrame(pandasDF)\n",
    "sdfCallLogs = getSparkDFfromPandasDF(dfCallLogs)   ## Calling from Chinmay_Utilities.ipynb\n",
    "</details>\n",
    "<details><summary>Using RDBMS SQL Select statements on spark dataframes (session.sql())...EXPAND</summary>\n",
    "\n",
    "#Register the spark dataframe as a table/view to be used like standard sql using sparkSession.sql()\n",
    "sdfCallLogs.createOrReplaceTempView(\"call_logs\")\n",
    "#####Using pure SQL with DataFrames\n",
    "* Register the dataframe as a table and\n",
    "* Use sparkSession1.sql(SQL_STMT_table)spark1.sql(\"SELECT * FROM call_logs WHERE dur > 100 ORDER BY dur DESC\").show()\n",
    "* The where clause of this sql is not supporting LIKE clause\n",
    "* This complex where clause (LIKE clause) is possible through direct \"where\" clause on spark dataframe (next statement)\n",
    "</details>\n",
    "<details><summary>SQL operations on dataframes (filter, select, where), computed column, renaming column etc...EXPAND </summary>\n",
    "\n",
    "* Selecting / filtering data from DataFrame.\n",
    ">* sparkSessn.sql(...) : Use direct Select SQL statements on the dataframe using sparkDF.sql(SQL_SELECT_STMT) e.g. \"select col1, col2 from table1 where age > 60 and duration < 120\"\n",
    ">* sDF.select(...).where(...) : This returns subset of DataFrame(sDF) with specified columns(in select clause)  satisfying the condition passed is where clause.\n",
    ">* sDF.filter(...) : This returns subset of DataFrame(sDF) with all columns satisfying the condition passed as filter e.g. \"age > 18\"\n",
    "\n",
    "######Quick filtering: (one of the two alternate ways on sparkDF)\n",
    "######sparkDF.select(*).where(my_condition)\n",
    "######sparkDF.filter(my_condition)\n",
    "\n",
    "my_where_clause = \"upper(name) like '%SEEC%QA%' AND dur > 50\"\n",
    "\n",
    "#Querying the Spark DataFrame directly with WHERE clause\n",
    "#This sort of complex where clauses (e.g. LIKE clauses) are not possible with pandas dataframe\n",
    "\n",
    "#sdfCallLogs.select(\"*\").where(\"upper(name) like '%SEEC%QA%'\").show()\n",
    "\n",
    "#Convert the filtered data into pandas data frame which can be processed or outputted into a file\n",
    "df_qa = sdfCallLogs.select(\"*\").where(my_where_clause).toPandas()\n",
    "sdfCallLogs.filter(my_where_clause).toPandas()\n",
    "#Rename the columns before converting to Pandas DataFrame\n",
    "df_qa2 = sdfCallLogs.filter(my_where_clause).select('name', 'number', 'dur', 'time') \\\n",
    "    .withColumnRenamed('dur', 'Duration (Sec)').withColumnRenamed('name', 'Name')\\\n",
    "    .withColumnRenamed('time', 'Date').withColumnRenamed('number', 'Phone Number').toPandas()\n",
    "\n",
    "#Below line inserts a new column with value double of the current 'dur' columns value\n",
    "df_qa2B = sdfCallLogs.filter(my_where_clause).select('name', 'number', 'dur', 'time') \\\n",
    "            .withColumn('double_duration',sdfCallLogs['dur']*2).toPandas()\n",
    "\n",
    "#To show a pd.DataFrame without column index\n",
    "df_qa2.style.hide_index()\n",
    "df_qa2B.style.hide_index()\n",
    "\n",
    "#To write an dataframe to an excel file without index column\n",
    "df_qa2.to_excel('1.xlsx',index=False)\n",
    "\n",
    "sdfCallLogs.filter(my_where_clause).filter((sdfCallLogs['dur']>300) & ~(sdfCallLogs['dur']<1000)).show()\n",
    "sdfCallLogs.filter(my_where_clause).filter('dur>300 and dur>=1000').show()  ## Filters can be cascaded\n",
    "\n",
    "###My Library methods\n",
    "######Convert the Spark DataFrame into Json\n",
    "######Convert a Pandas DataFrame into Json by first converting into a Spark DataFrame\n",
    "getJsonFromSparkDF(sdfCallLogs)   ## Calling from Chinmay_Utilities.ipynb\n",
    "#To get Json from a pandas DataFrame fist convert itinto a Spark DataFrame and then get Json from it\n",
    "getJsonFromSparkDF(getSparkDFfromPandasDF(dfCallLogs))   ## Calling from Chinmay_Utilities.ipynb\n",
    "\n",
    "#####Check types of DataFrame, Columns and displaying selected columns as DataFrame\n",
    "sdf.select('col1', 'col2').show() --> returns one or more columns as a dataframe --> Pass one column to select clause to get that column as a DF\n",
    "sdf.select('Age').show()   # This returns a dataframe of selected columns\n",
    "sdf[sdf['Age']>0].show()    # This works similar to regular pandas dataframe filtering\n",
    "sdf1 = sdf.withColumnRenamed('age', 'old_age') ## Renames the column 'age' as 'old_age'\n",
    "sdf2 = sdf.withColumn('double_age',sdf['age']*2)\n",
    "* Inserts a computed column named 'double_age' contaning comutation result from one or more other columns\n",
    "* If the name supplied matches with name of an existing column then that column is renamed with new data ? - TO VALIDATE\n",
    "</details>\n",
    "<details><summary>GroupBy and aggregates in a sparks dataframe, Handling missing data (<b>chin_2_Spark DataFrame Basics-withGroupBy.ipynb</b>)</summary>\n",
    "\n",
    "* Use \"groupBy('col1')\" to group the data by the specified column in a DataFrame and run aggregate functions on the groupby object.\n",
    "* Multiple aggregations can be done with one aggregation per column\n",
    "sdf_stock.groupBy('Year').agg({'Price':'max', 'Close':'avg', 'Open':'min', 'High':'max', 'Low':'min'})\\\n",
    "                .select('Year', \\\n",
    "                    format_number('max(Price)', 2).alias('max_price'), \\\n",
    "                    format_number('avg(Close)', 2).alias('avg_close'), \\\n",
    "                    format_number('min(Open)', 2).alias('min_open'), \\\n",
    "                    format_number('max(High)', 2).alias('max_high'), \\\n",
    "                    format_number('min(Low)', 2).alias('min_low'))\n",
    "sdf_stock.orderBy('Year').show()\n",
    "\n",
    "* Handling missing values\n",
    "\n",
    "#sdf_missing.na.fill(999).na.fill('CHINMAY').show()\n",
    "sdf_missing.na.fill('ANAND', subset='Name').show()\n",
    "sdf_missing.na.fill('ANAND', ['Name', 'Sales']).show()  # integer column is ignored as passed value is of string type\n",
    "* We can fill the na values using \"fillna()\" method from pandas dataframe using a constant value or with 'ffill' for forward fill and 'ackfill' for backward fill from the nearest non-null value of tjhe field.\n",
    "\n",
    "from pyspark.sql.functions import mean, avg\n",
    "mean_sales = sdf_missing.select(mean(sdf_missing['Sales'])).collect()[0][0]\n",
    "sdf_missing.na.fill(mean_sales, ['Sales']).show()\n",
    "</details>\n",
    "</pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Applying multiple aggregates on a groupby object for a datafrrame of multiple numeric columns</b></summary>\n",
    "    \n",
    "* We can apply multiple aggregates to a groupby object of a DataFrame with one aggregate per each numeric column of the DataFrame\n",
    "* Syntax:\n",
    ">* grpByYr.agg({'colNm1':'aggFn1','colNm2':'aggFn2', 'colNm3':'aggFn3'}).show()\n",
    ">* grpByYr.agg({'Open':'max','Close':'min', 'Volume':'avg'}).show()\n",
    "* If a specific column is repeated for more than one aggregate functions, then the last aggregate function is operated for the column, all peevious ones on the same column are ignored\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSION EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression models like Linear Regression predict continuous values\n",
    "* Categorical values are Classification models which categorize the items into distinct categories\n",
    "* Metrics like \"accuracy\" or \"recall\" are not useful for regression problems\n",
    "* Evaluation metrics for Regression are designed for continuous values, some of these are:\n",
    ">* Mean Absolute Error ($\\frac{1}{n} \\sum_{i=1}^n |y_i-\\hat{y_i}|$)\n",
    ">* Mean Squared Error ($\\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y_i})^2$)\n",
    ">* Root Mean Squared Error ($\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y_i})^2}$)\n",
    ">* R Squared Values\n",
    ">> R square (or .r2) is basically statistical property of our model, it is not a evaluation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE):\n",
    "\n",
    "* ### $\\frac{1}{n} \\sum_{i=1}^n |y_i-\\hat{y_i}|$\n",
    "* This is the mean of the absolute value of errors i.e. just the average error\n",
    "* Example: If we are predicting the price of a house based on its features then MAE says on average how far we are off on hte hous price.\n",
    "* N.B. The error is the difference between the predicted label ($\\hat{y_i}$), which is on the regression line) and the actual label ($y_i$) for a value of predictor x i.e. on the same vertical line. If the actual point falls below the regression line then this difference will be negateive and we cake the absolute value as the negative ones will cancel out the positive values giving wrong result. Hence we take the absolute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE):\n",
    "\n",
    "* ### $\\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y_i})^2$\n",
    "* This is the mean of the squared errors i.e. just the average error\n",
    "* Being squared, larger errors are noted more than with MAE. Hence MSE is more popular than MAE.\n",
    "* If the error i.e difference between actual value($y_i$) and predicted value ($\\hat{y_i}$) is more, then the difference is punished more and we take the average we can see that error more. MSE is more popu;lar as ti takes into account when we are off by very large amount.\n",
    "* The issue will be in unit, which is a squared unit here, so we take suare root of MSE  to get RMSE (Root Mean Absolute Error)\n",
    "* N.B. The error is the difference between the predicted label ($\\hat{y_i}$), which is on the regression line) and the actual label ($y_i$) for a value of predictor x i.e. on the same vertical line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Mean Absolute Error (RMSE):\n",
    "\n",
    "* ### $\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y_i})^2}$\n",
    "* This is root of MSE i.e. root of the mean of the squared errors\n",
    "It is most popular as its unit is same as that of y, and at the same time errors are punishedd more for higher difference.\n",
    "* Being squared, larger errors are punished more than with MAE. Hence MSE is more popular than MAE.\n",
    "* If the error i.e difference between actual value($y_i$) and predicted value ($\\hat{y_i}$) is more, then the difference is punished more and we take the average we can see that error more. MSE is more popu;lar as ti takes into account when we are off by very large amount.\n",
    "* N.B. The error is the difference between the predicted label ($\\hat{y_i}$), which is on the regression line) and the actual label ($y_i$) for a value of predictor x i.e. on the same vertical line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R Squared Values (R2)\n",
    "\n",
    "* R squared values are also known as \"Coefficient of determination\"\n",
    "* It is not an error metric, more of a statistical measure of our regression model.\n",
    "    * It is the proportion of the variance in the dependent variable ($y_i$) that is predictable from the independent variable(s), i.e. $x1_i, x2_i etc$\n",
    "* This tells how much variance our model accounts for.\n",
    "* Its values lie between 0 and 1 (i.e between 0% to 100%)\n",
    "* Example: if the value is 0.9 means it explains 90% of the variance of the data.\n",
    "* Adjusted R squared value is a different way of obtaininng $R^2$\n",
    "    * During machine learning if we get a negative value for $R^2$, then it is calculated using a different method.\n",
    "* When we want to compare models to each other, we should use Adjsuted $R^2$ not a normal one.\n",
    "    * Simple numerical comparison fo $R*2$ is nto enough a thorough analysis is needed.\n",
    "* We can use this to understand our model better or compare models but it is not the sole source of evaluating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT: Linear Regression Example Code Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: \n",
    "* ###### Here we are trying to predict a customer's Total Amount Expenditure (a continuous money value)\n",
    "* ###### We will then convert the realistic data into a format acceptable by Spark's MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
