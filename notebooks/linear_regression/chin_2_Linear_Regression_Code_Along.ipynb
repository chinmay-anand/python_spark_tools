{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Code Along\n",
    "###### (Chinmay's Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Purpose of this exercise:\n",
    "* Here we examine a dataset with Ecommerce Customer Data for a company's website and mobile app. Then we want to see if we can build a regression model that will predict the customer's yearly spend on the company's product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the utility functions from Chinmay for convenience\n",
    "import sys\n",
    "sys.path.append('C:/Users/nishita/exercises_chinmay')\n",
    "from tools.chinmay_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ACTIVITIES:\n",
    "    * Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSesn = SparkSession.builder.appName('code_along').getOrCreate()\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "data = sparkSesn.read.csv('Linear_Regression/Ecommerce_Customers.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTextFile('Linear_Regression/Ecommerce_Customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted(\"Printing the first row from the data\")\n",
    "i=0\n",
    "row = data.head(1)[0]\n",
    "for item in row:\n",
    "    print (data.columns[i] +' = '+str(item))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup data from Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOT USED: from pyspark.ml.regression import LinearRegression\n",
    "* Import <b>VectorAsembler</b> - this is a feature transformer that merges multiple columnsinto a vector column which si the purpose of this step as spark expects all features in a single column.\n",
    ">* Optionally import \"Vector\", probably not needed\n",
    ">* Search at https://spark.apache.org/docs/latest/api/python/index.html\n",
    ">* You can use <b>VectorAssembler?</b> to get the same help locally and more conveniently \n",
    "* We will now operate on numeric columns to chagne tehm to a vector column using VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's assume the fields 4th, 5, 6, 7th columns as features and last and the 8th column as the Label that we want to predict (\"Yearly Amount Spent\")\n",
    "* Create an assempbler (<b>VectorAssembler</b>) with 4,5,6,7th columns as input columns and a new column with name say \"our_Features\" which will hold all the features in vectorized form from the 4 columns 4,5,6,7th ones.\n",
    "* Now transform the data through out vector assembler with the ccolumns passed and the output feature column that we passed to the assembler will eb create with vectorized output from the list of input columns from \"data\"\n",
    "* Spark, for any ML algorithm, needs this vectorized feature column with all the pre-existing numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###### Create a VectorAssembler using the numeric features or coumns form our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Avg Session Length', \n",
    "                                       'Time on App', \n",
    "                                       'Time on Website', \n",
    "                                       'Length of Membership'], \n",
    "                            outputCol='our_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###### Get the transformed result from original input data to get a single verctor column that contains all the desired feature columns as the members of the vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)\n",
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Please note here that the vector feature column is a list of all the numeric columns we passed as inptuCols to the vercot assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.select('our_features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###### Finally prepare a 2 column DataFrame (as desired by spark for ML algos) having the vectorized feature column and the Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('our_features', 'Yearly Amount Spent')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split the final data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe().show()\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the regression model on the training data and we will evaluate that on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =LinearRegression(featuresCol='our_features', labelCol='Yearly Amount Spent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### residuals in the test_result are actually the differences of the prediceted values from te actual labels.\n",
    "* i.e. this is the list of vertical distances of the predicted labels from the regression line i.e distances from the labbls in test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.r2\n",
    "test_result.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RMS or rootMeanSquaredError = $\\sqrt{\\frac{1}{n}{\\sum{(y_i-\\hat{y_i})^2}}}$\n",
    "* And ($y_i-\\hat{y_i}$) or the errors are nothing but the elements of <b>residual</b>.\n",
    "* So RMS is the square root of the average of squares of the residual elements\n",
    "\n",
    "* .r2 (r squared) of 0.98 says that our model explains 98% of the variance in the data, which is very good\n",
    "* now that RMS and r squared are very good, we should think of double checking our data and double check the way we fitted our model to be more realistic\n",
    "\n",
    "* Compare the RMS values of mean and standard deviation of the final data (final_Data.describe().show()), if the RMS is much less than the stddev then it is good, so be suspicious.\n",
    "\n",
    "* If you get results very good with Linear Regression be siuspicious and check how you fit the model\n",
    "* Did we evaluate our model also on the training data which is already known to the model, which is a common mistake.\n",
    "\n",
    "######  points to note\n",
    "* we can expect more advanced and more complex models to have better fit, but if it is still a very good fit even with simple LinearRegression, double check the data and fitting way with a realistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to deploy this model on some unlabeled data.\n",
    "* For this we need some customer data with featuires alone, and without any label and that our model must not be trained with that data.\n",
    "* To mimic the data for deployment we will remove the labels test_data and mark the data with feature set as unlabeled datam as  for now we don't have realistic data, so we will mimic some production data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = test_data.select('our_features')\n",
    "unlabeled_data.printSchema()\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = lr_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here basic flow of data is as follows\n",
    "* EVALLUATION:  test_data-->test_result\n",
    "    * This compares the labels in test_result against test_data\n",
    "* PREDICTION:   test_data-->unlabeled_data->predicted_data  [IDEALLY: It should have been NewData(unlabeled)-->predicted_data]\n",
    "    * Here we need to manually compare the label of predicted_data against test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here we make a select join to find the difference between labeled field of input and prediction label of deployed data\n",
    "* register both the dataframes as temporary view or table using sparkSession.createOrReplaceTempView()\n",
    "* use a standard select join on the two views using standard sql stmt where condition and create a \"differece\" column to show the difference between the actial label value \"Yearly Amound Spent\" and predicted labels \"prediction\"\n",
    "* Note: we have used <b>reverse single quote</b> to specify columns with spaces in its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using SQL\n",
    "#### To use SQL queries directly with the dataframe, you will need to register it to a temporary view:\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "test_data.createOrReplaceTempView(\"my_test_data\")\n",
    "predicted_data.createOrReplaceTempView(\"my_predicted_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_df = sparkSesn.sql(\"SELECT t.*, p.prediction, (t.`Yearly Amount Spent` - p.prediction) as difference  \\\n",
    "                                FROM my_test_data t, my_predicted_data p WHERE t.our_features==p.our_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_df.count()\n",
    "input_output_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we find the differnece between our deviation (\"difference\" field) and the deviation from the evaluation step on test data (test_result.residuals) to find how accurate was our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_result.residuals.printSchema()\n",
    "#input_output_df.printSchema()\n",
    "test_result.residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If we observe the values in test_result.residuals is probably idntical to our predicted result.\n",
    "* i.e. the test_data evaluation result is identical to the final prediction on the unlabeled data for the same feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: {}\".format(test_result.rootMeanSquaredError))\n",
    "print(\"MSE: {}\".format(test_result.meanSquaredError))\n",
    "print(\"MAE: {}\".format(test_result.meanAbsoluteError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
