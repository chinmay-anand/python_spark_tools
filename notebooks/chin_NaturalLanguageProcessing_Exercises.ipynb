{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Books on NLP:</b></summary>\n",
    "    \n",
    "* Wikipedia article on NLP (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "* NLTK Book (Natural Language Processing with Python from Oreilly (downloaded)\n",
    "* Foundations of Statistical Natural Language Processing from Manning publications\n",
    "</details>\n",
    "\n",
    "<details open><summary><b>Examples of NLP:</b></summary>\n",
    "    \n",
    "* Clustering News Articles\n",
    "* Suggesting similar books\n",
    "* Grouping Legal Documents\n",
    "* Analyzing Consumer Feedback\n",
    "* Spam Email Detaction\n",
    "</details>\n",
    "\n",
    "<details open><summary><b>Our basic process for NLP:</b></summary>\n",
    "    \n",
    "* Compile all documents (Corpus)\n",
    "* Featurize the words to numerics\n",
    "* Compile features of documents\n",
    "</details>\n",
    "\n",
    "<details><summary><b>Detailed process steps:</b></summary>\n",
    "    \n",
    "* A standard way of doing this is through the use of what is known as \"TF-IDF\" (called Term Frequency - Inverse Document Frequency)\n",
    "* Say we have 2 documents \"Blue House\" and \"Red House\"\n",
    "    * Here total words list is (red, blue, house)\n",
    "* Featurize the two documents based on word count\n",
    "    * \"Blue House\" --> (red,blue,house) --> (0,1,1)\n",
    "    * \"Red House\" --> (red,blue,house) --> (1,0,1)\n",
    "        * Thus vectors represented as a vector of word counts is called a <b>\"Bag of Words\"</b>.\n",
    "        * These are now vectors in an N-dimensional space.\n",
    "        * we can compare vector with cosine similarity.\n",
    "            $Sim(A,B) = Cos(\\theta) = \\frac{A.B}{||A||.||B||}$\n",
    "            * Here $\\theta$ is the angle between the two vectors A and B\n",
    "            \n",
    "![image.png](attachment:image.png)\n",
    "</details>\n",
    "\n",
    "<details><summary><b>Tokenizer and Tokenization in NLP</b> with <i>Tokenizer(), RegexTokenizer()</i></summary>\n",
    "\n",
    "* Tokenization - a process of taking a text (sentence) and breaking it into individual tokens (usually words)\n",
    "* Tokenizer - Normal tokenizer &\n",
    "    * <u>Tokenizer()</u> -- Normal tokenizer which treats a SPACE character as a word/token separator\n",
    "    * <u>RegexTokenizer(pattern)</u> -- Regular expression tokenizer - allows advanced tokenization based on regular expressios. This treats given pattern as the token separator.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Bag of Words and TF-IDF:</b></summary>\n",
    "\n",
    "* We can imporve on \"Bag of Words\" by adjusting word counts based on their frequency in corpus (the group of all documents).\n",
    "* We can use Term Frequency Inverse Document Frequency\n",
    "* Term Frequency - Importance of the term with in that document.\n",
    "    * TF(x.y) = Number of occurrences of term x in doucment y\n",
    "* Inverse Document Frequency - Importance of the term in the corpus\n",
    "    * IDF(t) = log(N/dfx)\n",
    "        * N = Total number of documents\n",
    "        * dfx = Number of documents with the term.\n",
    "\n",
    "* TF-IDF = $W_{x,y} = t_{x,y}$ X $log(\\frac{N}{df_x})$\n",
    "    * This is mathematical expression for TF-IDF\n",
    "    * $W_{x,y}$ is TF-IDF of the term x within document y = How imprtanta word to a document in a collection of corpus.\n",
    "    * $t_{x,y}$ is the term frequency i.e. frequency of term x in document y\n",
    "    * $df_{x}$ is the number of documents containing x\n",
    "    * $N$ is the total number of documents \n",
    "    \n",
    "* TF-IDF is a numerical statistics that reflects how imprtant a word is to a document in a collection of corpus.\n",
    "* TF-IDF is often used as a weighting factor in searches of information retrieval, text mining and user modeling\n",
    "\n",
    "* Various spark tools from pyspark.ml.feature helps with this entire feature behind the scene.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/nishita/exercises_udemy/MyTrials/tools')\n",
    "from chinmay_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ############################################################."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP: NLP_Code_Along (Spam Message Classification)\n",
    "\n",
    "* DATA PREPARATION STEPS\n",
    "    * csv--(read, rename columns)-->data_in--(StringIndexer)-->data_labeled--(Tokenizer)-->data_words--(StopWordsRemover)-->data_filtered\n",
    "    * data_filtered--(CountVectorizer)-->data_TF--(IDF)-->data_TFIDF--(VectorAssembler)-->data_features\n",
    "    * data_features.select(['label', 'features'])-->data_clean\n",
    "* SPAM PREDICTION (ML PROCESSIGN & EVALUATION)\n",
    "    * clean_data--(randomSplit)-->train_data,test_data\n",
    "    * NaiveBays().fit(train_data)-->spam_predictor\n",
    "    * spam_predictor.transform(test_data)-->predicted_test_results\n",
    "    * MulticlassClassificationEvaluator().evaluate(predicted_test_results)-- compares 'prediction' column against 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('nlp_sms_classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = spark1.read.csv('Natural_Language_Processing/smsspamcollection/SMSSpamCollection', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c0='ham', _c1='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'),\n",
       " Row(_c0='ham', _c1='Ok lar... Joking wif u oni...')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in.columns\n",
    "data_in.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rename the columns to meaningful ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['class', 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Rename the columns to meaningful ones')\n",
    "data_in = data_in.withColumnRenamed('_c0', 'class').withColumnRenamed('_c1', 'text')\n",
    "data_in.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled = indexer.fit(data_in).transform(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = tokenizer.transform(data_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = remover.transform(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate word counts before and after stopword removal usind a count_words user defined function\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda word_list: len(word_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+--------------------+--------------------+----------+--------------+\n",
      "|class|                text|label|               words|            filtered|word_count|filtered_count|\n",
      "+-----+--------------------+-----+--------------------+--------------------+----------+--------------+\n",
      "|  ham|Go until jurong p...|  0.0|[go, until, juron...|[go, jurong, poin...|        20|            16|\n",
      "|  ham|Ok lar... Joking ...|  0.0|[ok, lar..., joki...|[ok, lar..., joki...|         6|             6|\n",
      "| spam|Free entry in 2 a...|  1.0|[free, entry, in,...|[free, entry, 2, ...|        28|            23|\n",
      "|  ham|U dun say so earl...|  0.0|[u, dun, say, so,...|[u, dun, say, ear...|        11|             9|\n",
      "|  ham|Nah I don't think...|  0.0|[nah, i, don't, t...|[nah, think, goes...|        13|             7|\n",
      "| spam|FreeMsg Hey there...|  1.0|[freemsg, hey, th...|[freemsg, hey, da...|        32|            18|\n",
      "|  ham|Even my brother i...|  0.0|[even, my, brothe...|[even, brother, l...|        16|             9|\n",
      "|  ham|As per your reque...|  0.0|[as, per, your, r...|[per, request, 'm...|        26|            16|\n",
      "| spam|WINNER!! As a val...|  1.0|[winner!!, as, a,...|[winner!!, valued...|        26|            19|\n",
      "| spam|Had your mobile 1...|  1.0|[had, your, mobil...|[mobile, 11, mont...|        29|            19|\n",
      "|  ham|I'm gonna be home...|  0.0|[i'm, gonna, be, ...|[gonna, home, soo...|        21|            12|\n",
      "| spam|SIX chances to wi...|  1.0|[six, chances, to...|[six, chances, wi...|        26|            21|\n",
      "| spam|URGENT! You have ...|  1.0|[urgent!, you, ha...|[urgent!, won, 1,...|        26|            19|\n",
      "|  ham|I've been searchi...|  0.0|[i've, been, sear...|[searching, right...|        37|            15|\n",
      "|  ham|I HAVE A DATE ON ...|  0.0|[i, have, a, date...|[date, sunday, wi...|         8|             3|\n",
      "| spam|XXXMobileMovieClu...|  1.0|[xxxmobilemoviecl...|[xxxmobilemoviecl...|        19|            13|\n",
      "|  ham|Oh k...i'm watchi...|  0.0|[oh, k...i'm, wat...|[oh, k...i'm, wat...|         4|             4|\n",
      "|  ham|Eh u remember how...|  0.0|[eh, u, remember,...|[eh, u, remember,...|        19|            13|\n",
      "|  ham|Fine if thats th...|  0.0|[fine, if, thats...|[fine, thats, wa...|        13|             9|\n",
      "| spam|England v Macedon...|  1.0|[england, v, mace...|[england, v, mace...|        24|            21|\n",
      "+-----+--------------------+-----+--------------------+--------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_filtered.withColumn('word_count', count_tokens(col('words'))).withColumn('filtered_count', count_tokens(col('filtered'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get TF-IDF using CountVectorizer and IDF\n",
    "c_vec = CountVectorizer(inputCol='filtered', outputCol='c_vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = c_vec.fit(data_filtered).transform(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf = idf.fit(data_tf).transform(data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['label','tf_idf'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = assembler.transform(data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_final.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_clean.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()  # Default columns expected are 'features', 'label', 'prediction' etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_predictor = nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = spam_predictor.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate our prediction using MulticlassClassificaitonEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166200239195221"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9076281287246722"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9431122470843195"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9076281287246721"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9166200239195221"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default params expected are 'prediction', 'label', metricName='f1'\n",
    "# The prediciton an dlabel column names match in the test_result, so nto passing here\n",
    "\n",
    "MulticlassClassificationEvaluator().evaluate(test_result)\n",
    "MulticlassClassificationEvaluator(metricName='accuracy').evaluate(test_result)\n",
    "MulticlassClassificationEvaluator(metricName='weightedPrecision').evaluate(test_result)\n",
    "MulticlassClassificationEvaluator(metricName='weightedRecall').evaluate(test_result)\n",
    "MulticlassClassificationEvaluator(metricName='f1').evaluate(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9076281287246722"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MulticlassClassificationEvaluator(metricName='accuracy').evaluate(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ############################################################."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Ch-2: Tools_for_NLP (part-2).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>TF-IDF</b>\n",
    "    * TF-IDF : Term Frequency Inverse Document Frequency - is basically feature of vectorization used with text to reflect the <b>importance of a term to a document in the corpus</b> itself.\n",
    "    * STEPS:\n",
    "        1. tokenize the sentences using Tokenizer() or RegexTokenizer(pattern)\n",
    "        * Apply HashingTF to generate rawFeatures (a preliminiary vectorization)\n",
    "        * Apply IDF on result from HashingTF to generate final vectorized 'features'\n",
    "        * This 'features' field from TF-IDF output can now be used as input to a ML algorithms.\n",
    "* <b>CounterVectorizer</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Explanation on TF-IDF</b></summary>\n",
    "\n",
    "<p><a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Term frequency-inverse document frequency (TF-IDF)</a> \n",
    "is a feature vectorization method widely used in text mining to reflect the importance of a term \n",
    "to a document in the corpus. Denote a term by <code>$t$</code>, a document by  d , and the corpus by D.\n",
    "Term frequency <code>$TF(t, d)$</code> is the number of times that term <code>$t$</code> appears in document <code>$d$</code>, while \n",
    "document frequency <code>$DF(t, D)$</code> is the number of documents that contains term <code>$t$</code>. If we only use \n",
    "term frequency to measure the importance, it is very easy to over-emphasize terms that appear very \n",
    "often but carry little information about the document, e.g. &#8220;a&#8221;, &#8220;the&#8221;, and &#8220;of&#8221;. If a term appears \n",
    "very often across the corpus, it means it doesn&#8217;t carry special information about a particular document.\n",
    "Inverse document frequency is a numerical measure of how much information a term provides:\n",
    "\n",
    "$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1} $$\n",
    "\n",
    "where |D| is the total number of documents in the corpus. Since logarithm is used, if a term \n",
    "appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid \n",
    "dividing by zero for terms outside the corpus (Here smoothing term 1 is added to both numeraor and denominator to avoid 'divide by zero error, in case the rm does not appear in any document). The TF-IDF measure is simply the product of TF and IDF:\n",
    "$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Explanation on CountVectorizer</b></summary>\n",
    "    \n",
    "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n",
    "\n",
    "During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "# Feature Extractors\n",
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tf-idf\">TF-IDF</h2>\n",
    "\n",
    "<p><a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Term frequency-inverse document frequency (TF-IDF)</a> \n",
    "is a feature vectorization method widely used in text mining to reflect the importance of a term \n",
    "to a document in the corpus. Denote a term by <code>$t$</code>, a document by  d , and the corpus by D.\n",
    "Term frequency <code>$TF(t, d)$</code> is the number of times that term <code>$t$</code> appears in document <code>$d$</code>, while \n",
    "document frequency <code>$DF(t, D)$</code> is the number of documents that contains term <code>$t$</code>. If we only use \n",
    "term frequency to measure the importance, it is very easy to over-emphasize terms that appear very \n",
    "often but carry little information about the document, e.g. &#8220;a&#8221;, &#8220;the&#8221;, and &#8220;of&#8221;. If a term appears \n",
    "very often across the corpus, it means it doesn&#8217;t carry special information about a particular document.\n",
    "Inverse document frequency is a numerical measure of how much information a term provides:\n",
    "\n",
    "$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1} $$\n",
    "\n",
    "where |D| is the total number of documents in the corpus. Since logarithm is used, if a term \n",
    "appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid \n",
    "dividing by zero for terms outside the corpus (Here smoothing term 1 is added to both numeraor and denominator to avoid 'divide by zero error, in case the rm does not appear in any document). The TF-IDF measure is simply the product of TF and IDF:\n",
    "$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark2 = SparkSession.builder.appName('nlp_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe holding the words of sentences.\n",
    "# To the \"creteDataFRame() we will pass two lists, \n",
    "#     1st parameter is a list of tupples represengting the individual rows and \n",
    "#     second param is a list of column names for each tupple record in 1st parameter\n",
    "sentence_data = spark2.createDataFrame([\n",
    "        (0, 'Hi I hear about Spark'),\n",
    "        (1, 'I wish java could use case classes'),\n",
    "        (2, 'Logistic regression models are neat')\n",
    "    ], ['label', 'sentence']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "words_data = tokenizer.transform(sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generate TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)\n",
    "# There are 16 distinct words in the original dataframe sent_df, we we can pass around 20 as 'numFeatures' parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In HashingTF(), if 'numFeatures' is passed in, it must be more than the number of distinct tokens in the dataframe else there will be collision. i.e. collision means two or more words getting same hashed number. Default value is 2**18 i.e. 262144\n",
    "* HashingTF uses 'feature hashing' also called 'hashing trick' for vectorizing features.\n",
    "* Refer: https://en.wikipedia.org/wiki/Feature_hashing\n",
    "\n",
    "For HashingTF Refer\n",
    "* https://stackoverflow.com/questions/44966444/what-is-the-relation-between-numfeatures-in-hashingtf-in-spark-mllib-and-actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureized_data = hashingTF.transform(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now generate TF-IDF by generating IDF on the result from tF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_model = idf.fit(featureized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featureized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we had the first column of the initial sentence DF as 'label' and TF-IDF output as 'features', then we can use 'label' and 'feature' column to use with other ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n",
    "\n",
    "During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data: Each row is a bag of words with a ID.\n",
    "sdf = spark2.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CountVectorizer - Extracts a vocabulary from document collections and generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here vocabulary size (vocabSize) is 3, i.e a, b, c - it is the number of distinct tokens or words\n",
    "# vocab size must be atleast higher than number of distinct words or vocabulary\n",
    "# We want a token to be included in the vocabulary only if it is part of atleast 2 documents i.e. minDF=2\n",
    "# minDF is the minimum number of documents a term must appear in to be included in the vocabulary.\n",
    "\n",
    "cvec = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_result = cvec.fit(sdf).transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of count vectorization model\n",
    "###### The 'features' column can be explained as below\n",
    "* Second document (for id==1) is:  [a, b, b, c, a]\n",
    "* features(for second document) is: (3,[0,1,2],[2.0,2.0,1.0])\n",
    "    * First element in 'features' is 3 - it represents the vocabulary size, i.e. number of distinct terms : a, b, c\n",
    "    * Second element is [0,1,2] -- it show which terms form the vocabulary appear in 'words' column.\n",
    "    * Third element is [2.0,2.0,1.0] -- it show frequency of the occurring terms in order. In this example the term frequency for [0th, 1st, 2nd] terns i.e. [a,b,c] are [2.0, 2.0, 1.0)] respectively. I.e. in the second document i.e for sdf_result[id==2]. That means a appears twice, b appears twice, but c appears once.\n",
    "* This is essentially the \"Bag of Words\" method w\n",
    "    \n",
    "    \n",
    "    \n",
    "# ASK Ashish Lal: HOW TO PRINT CountVectorizer vocabulary in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will use count vectorizer with the original words data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec2 = CountVectorizer(inputCol='words', outputCol='features', vocabSize=20, minDF=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = cvec2.fit(words_data).transform(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ############################################################."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Ch-1: Tools_for_NLP(part-1).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('nlp_1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>'col'</b> function is for calling columns\n",
    "* <b>'udf'</b> stands for user defined functions, we can create a function using lambda expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe holding the sentences to be processed.\n",
    "# To the \"creteDataFRame() we will pass two lists, \n",
    "#     1st parameter is a list of tupples represengting the individual rows and \n",
    "#     second param is a list of column names for each tupple record in 1st parameter\n",
    "sent_df = spark1.createDataFrame([\n",
    "        (0, 'Hi I hear about Spark'),\n",
    "        (1, 'I wish java could use case classes'),\n",
    "        (2, 'Logistic,regression,models,are,neat')\n",
    "    ], ['id', 'sentence']\n",
    ")\n",
    "\n",
    "# Here in the 3rd sentence we have purposefully put comma in place of space in the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For regular expressions Refer https://www.geeksforgeeks.org/write-regular-expressions/\n",
    "* We can use Notepad++ to search for the patterns\n",
    "* \\\\W matches a single non-word character like a space or a comma etc (\\\\W+ actually matchees a consecutive seqiuence of such non-word characters. To pass a single reverse slash character we need to type two consecutive reverse slashes as escape sequence.\n",
    "* Check out the resource links @ 09:10 / 16:12 in the video \"NLP Tools Part One\" for more on expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer() uses a SPACE as the word separator while tokenizing the sentence\n",
    "* RegexTokenizer uses the supplied pattern as the word separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the 'words' column from the 'senence' columnusing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_tokenized = tokenizer.transform(sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a user defined function using lambda to count number of words in a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda word_list: len(word_list), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_tokenized.withColumn('wordcount', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we see that the normal 'Tokenizer' could not split the sentence using comma as separator, so third entry show only one big joint word. We need to use RegexTokenizer here with comma i.e. a non-word charcter (\\\\W) as a separator which will match space and comma both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Behaviour of RegexTokenizer without 'pattern' parameter is same as that of Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_tokenized = regex_tokenizer.transform(sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_tokenized.withColumn('word_count', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now to remove \"stop words\" which are very common and frequently occurring words and does nto carry much meaning, like : 'or', 'the' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use \"StopwordsRemover\" from spark to filter out the common stop words from our tokens or words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note the <b>truncate=False</b> parameter to sdf.show(trun..) to expand the columns to show all the values instead of putting tripple dots after a fixed width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_filtered = remover.transform(sent_df_tokenized).withColumn('word_count', count_tokens(col('filtered')))\n",
    "sent_df_filtered.show()\n",
    "\n",
    "sent_df_filtered.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another example of stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark1.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "remover.transform(sentenceData).show()\n",
    "remover.transform(sentenceData).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" We can even add our common words appended to the spark provided list to ignore them, may be due to a regulatory / domain requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram\n",
    "* N-Gram is a sequence of tokens typically words for some integer\n",
    "* It is a sequence of N tokens for some integer N @ 13:02 / 16:12\n",
    "* ngrams show sequence of 'n' consecutive words in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n=2, inputCol='filtered', outputCol='ngrammed')\n",
    "\n",
    "sent_df_ngrammed = ngram.transform(sent_df_filtered)\n",
    "\n",
    "sent_df_ngrammed.withColumn('ngram_count', count_tokens(col('ngrammed'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_ngrammed.select('ngrammed').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram3 = NGram(n=3, inputCol='filtered', outputCol='ngrammed')\n",
    "\n",
    "ngram3.transform(sent_df_filtered).withColumn('ngram_count', count_tokens(col('ngrammed'))).select('ngrammed', 'ngram_count').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### This kind of ngrams are very useful in finding the relationship between say two words - which words always appear next to each other etc..\n",
    "###### In more advanced Natural Language Processing N-Grams may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
