{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JULIA- The High Performance Data Language\n",
    "* https://towardsdatascience.com/bye-bye-python-hello-julia-9230bff0df62\n",
    "* https://julialang.org/learning/\n",
    "* https://juliaacademy.com/\n",
    "* https://juliaacademy.com/courses/375479/lectures/16882463\n",
    "    * Check authentication for \"sso.teachable.com\" for logging into this tutorial form Julia team.\n",
    "    * Install nteract.io which will help us running julia with notebook\n",
    "    * Install julialang.org\n",
    "    * Follow the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi-Supervised Learning:\n",
    "* https://towardsdatascience.com/simple-explanation-of-semi-supervised-learning-and-pseudo-labeling-c2218e8c769b\n",
    "* https://medium.com/inside-machine-learning/placeholder-3557ebb3d470\n",
    "* https://www.geeksforgeeks.org/ml-semi-supervised-learning/\n",
    "* https://en.wikipedia.org/wiki/Semi-supervised_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Predictive Analytics with Python - Ashish Kumar.pdf\n",
    "* Book Locaiton:E:\\Documents\\books_tech\\DataScience\n",
    "* Datasets used in the book: https://goo.gl/zjS4C6   (pg86/493)\n",
    "* pd.read_csv(): pg 94-101/493 - detailed example and various tricks\n",
    "* Handling null values\n",
    "    * na deletion - 121 pg\n",
    "    * na imputation - filling null values - 121-124 pgs\n",
    "    * na imputation - forward fill fillna('ffill')  and backward fill fillna('bbackfill') \n",
    "    * Visualization / plotting (Scatter plot, Histogram, Boxplot) - 128/493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark API Documentation:\n",
    "* http://spark.apache.org/docs/latest/\n",
    "* http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Jupyter Notebooks as Modules\n",
    "https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Importing%20Notebooks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Analysis by Gareth James\n",
    "#### (ISLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References:\n",
    "* The PDF Book for ISLR http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf\n",
    "* The StatLearning statistical https://statistics.stanford.edu/statlearning-online\n",
    "* Statistics Simplified: https://statisticsbyjim.com/glossary/regression-coefficient/\n",
    "\n",
    "\n",
    "###### Introduction\n",
    "* Statistical Learning is a subset of Statistics\n",
    "* Points to consider\n",
    ">* Methods of Least Square\n",
    ">* Linear Regression\n",
    ">* Linmear Discriminant Analysis\n",
    ">* Logistic Regression\n",
    ">* Generated Linear Models\n",
    ">* Classification & Regresssion Trees\n",
    ">* Cross-validation for model selection\n",
    ">* Generalized Additive Models\n",
    ">* Supervised Modeling\n",
    ">* Unsupervised Modeling\n",
    ">* Prediction\n",
    "* Modeling for Prediction vs Inference\n",
    "* Example:\n",
    ">* Prediction example: Identifying individuals who will respond positively to a mail based on various parameters.\n",
    ">* Inference example: What effect will changing the price of a product have on sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Premises for applying Statistical Learning\n",
    "* For applying statistical learning methods to real-world problems\n",
    ">* we should know the functionality of the algorithms\n",
    ">* but not necessary to know in depth how these algorithms are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data files from ISLR (www.statlearning.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click for details</summary>\n",
    "\n",
    "* www.statlearning.com\n",
    "<pre>\n",
    "Name Description\n",
    "Auto Gas mileage, horsepower, and other information for cars.\n",
    "Boston Housing values and other information about Boston suburbs.\n",
    "Caravan Information about individuals offered caravan insurance.\n",
    "Carseats Information about car seat sales in 400 stores.\n",
    "College Demographic characteristics, tuition, and more for USA colleges.\n",
    "Default Customer default records for a credit card company.\n",
    "Hitters Records and salaries for baseball players.\n",
    "Khan Gene expression measurements for four cancer types.\n",
    "NCI60 Gene expression measurements for 64 cancer cell lines.\n",
    "OJ Sales information for Citrus Hill and Minute Maid orange juice.\n",
    "Portfolio Past values of financial assets, for use in portfolio allocation.\n",
    "Smarket Daily percentage returns for S&P 500 over a 5-year period.\n",
    "USArrests Crime statistics per 100,000 residents in 50 states of USA.\n",
    "Wage Income survey data for males in central Atlantic region of USA.\n",
    "Weekly 1,089 weekly stock market returns for 21 years.\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "* REFER: Introduction to Statistical Learning by Gareth James\n",
    "* The PDF Book for ISLR http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf\n",
    "* The StatLearning statistical https://statistics.stanford.edu/statlearning-online\n",
    "* Statistics Simplified: https://statisticsbyjim.com/glossary/regression-coefficient/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3 Prediction Accuracy vs Model Interpretability\n",
    "<details><summary>for various algorithms (Lease squares Lin reg, Lasso, Generalized Additive Methods, Bagging Boosting, Support Vector Machines)</summary>\n",
    "\n",
    "* Relatively inflexible approaches like Linear Regression can only generate linmear functions.\n",
    "* Statistical learning methods in the incresaing order of flxibility are:\n",
    "    * \"Subset Selection\"-->Lasso-->Least Squares-->Generatlized Additive Models-->Trees-->Support Vecrot Machines-->Bagging, Boosting\n",
    "* As the flexibility of a method increases it interpretability decreases. This is the trade-off.\n",
    "* \"Least squares\" linear regression(ch-3) is relatively inflexible but quite interpretable\n",
    "* \"Lasso\" (ch-6) relies upon the linear model but uses an alternative fitting procedure to estimate teh coefficients : $\\beta_0,\\beta_1,..,\\beta_p,$, in fact sets a number of them to exactly zero, hence more rstrictive than the linear model and more interpretable than linear modesls  as the response from the final model will be realted to a smaller set i.e. non zero coefficient estimates.\n",
    "* \"Generalized Additive Models\" extend the linear model and allow for certain non-linear relationships, and hene are more flexible and less interpretable than linear regression\n",
    "* \"Bagging, Boosting\" (ch-8) is a fully non-linear model, harder to interpret. It is with non-linear kernels.\n",
    "* \"Support Vector Machines\" (ch-9) is also a fully non-linear model, harder to interpret. It is also with non-linear kernels.\n",
    "* When \"Inference\" is the goal then simple, relatively inflexible or restrictive models like linear models are much more interpretable.\n",
    "* When \"Prediction\" is the goal and intrpretability of the predictive model is not of our interest, it is bets to use a most flexible model. BUT many times we can obtain moe accurate predictions using a less flexible model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4 Supervised vs Unsupervised Learning\n",
    "<details><summary>When to use what?</summary>\n",
    "\n",
    "* In <i>supervised learning</i> domain for each observation of the peedictor measurements $x_i$, i = 1,...,n, there is an associated response measurement $y_i$\n",
    "* In <i>unsuprevised learning</i> domain for each observation i = 1,...,n we observe a vector of measurements $x_i$, but no associated response $y_i$. There is no response to supervise our analysis  and hence it is referred as <i>unsupervised</i>.\n",
    "* Classical methods like Linear regression, Logistic regression, and modern methods like GAM, Boosting, and SVM operate in supervised learning domain.\n",
    "* In Unsupervised learninng domain we can seek to understand the relationship between the variales or observations\n",
    "* Statistical learning tools like \"Cluster Anlaysis or \"clustering\"can be used to ascertain whether the observations fall into relatively distinct groups. Problem becomes even mor3 challenging when these groups overlap.\n",
    "* Sometimes for a subset of the observations we may not have responses, but for another subset we may have responses, I may happen usually when prdictors can be measured cheaply but responses are much more expensive to collect.\n",
    "    * This setting is called <i>Semi-Supervised</i> Learning.\n",
    "    * Here the unlabelled data when used in conjunction with a small amount of labelled data, can produce considerable improvement in learning accuracy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression vs Classification Problems\n",
    "<details><summary>When to use what?</summary>\n",
    "\n",
    "* Regresssion problems are problems with a quantitative response.\n",
    "    * Quantitative parameters could be person's weight, age etc\n",
    "* Classificaiton problems are problems with a qualitative response i.e. classifying the response to eb one of a defined group of categories.\n",
    "    * Qualitative parameters could be person's gender, brand of a product, type of cancer etc.\n",
    "* Statistical Methods used:\n",
    "    * Least squares linear regression (ch-3) is used with a quantitative response.\n",
    "    * Logistic regression (ch-4) is typically used with a qualitative (two-class or binary) response. Logistic regresssion is mostly used for classification, but it estimates the class probabilities as well, hence can be thought as a regression method as well.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Assessing Model Accuracy:\n",
    "* No one method dominates all others over all possible data sets.\n",
    "* Different methods may work best on different data sets even though these data may be similar.\n",
    "* Selecting the best approach is the most challenging part of statistical learning.\n",
    "\n",
    "##### 2.2.1 Few concepts of Statistical Learning Procedures:\n",
    "<details><summary>MSE, Cross-validation and impact of model flexibility .. EXPAND</summary>\n",
    "\n",
    "* Mean Squared Error $\\frac{1}{n}\\sum\\limits_{i=1}^n({y_i-\\hat{y_i}})^2$\n",
    "* This is called MSE or mean square error\n",
    "* The MSE with training data set is called training MSE and that on an unseen test data, is called test MSE.\n",
    "* When a given method yields a small training MSE but a large test MSE, we are sai to be overfitting the data, because the supposed patterns found in training data simply does not exist in test data. We almost always expect the training MSE to be smaller than test MSE.\n",
    "* \"Cross-validation\" is a method for estimating test MSE using training data.\n",
    "\n",
    "* MSE curve when plotted against flexibility takes a shape of 'U' curve, which decreases rapidly as the flexibility (degrees of freedom???)  incrases, shosing a good sign, but after some point it starts increasing making it appear U-shape. We need to find this minimum point.\n",
    "* But adjusting the level of flexibility of the \"smoothing spline fit\" we can produce many different fits to the data.\n",
    "</details>\n",
    "\n",
    "    * MSE = Mean Squared Error = $\\frac{1}{n}\\sum\\limits_{i=0}^n(y_i-\\hat{y_i})^2$\n",
    "    * RMSE = Root Mean Squared Error = $\\sqrt{MSE} = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=0}^n(y_i-\\hat{y_i})^2}$\n",
    "    * We can compare RMSE to Standard Deviation($\\sigma$) and MSE to Variance($\\sigma^2$) where the predicted response ($\\hat{y_i}$) can be compared with the mean of the variables in the smaple (MY THEORY)\n",
    "\n",
    "<details><summary>Standard terminologies in Statistics (mean, median, mode, variance, standard deviation)...EXPAND</summary>\n",
    "    \n",
    "* Variance = $\\sigma^2$ = $\\frac{1}{n}\\sum\\limits_{i=0}^n{(y_i-y_{mean})^2}$ =  Average of squared differences from the mean\n",
    "* Standard deviation = $\\sigma $ = $\\sqrt{Variance}$ = $\\sqrt{\\frac{1}{n}\\sum\\limits_{i=0}^n{(y_i-y_{mean})^2}}$\n",
    "* Mode = Most frequenty occurring element\n",
    "* Mean = $y_{mean}$ = $\\frac{1}{n}\\sum\\limits_{i=1}^ny_i$\n",
    "* Median = The Median is the \"middle\" of a sorted list of numbers OR the average of two middle values if there are even number of items.\n",
    "* Refer: https://www.mathsisfun.com/\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 The Bias-Variance Trade-Off\n",
    "<details><summary>Bias - Variance - impact on MSE, the Mean Squared Error .. EXPAND</summary>\n",
    "\n",
    "* The MSE with training data set is called training MSE and that on an unseen test data, is called test MSE.\n",
    "\n",
    "###### VARIANCE\n",
    "* Variance is the amount by which the fitting method (f)  i.e. Y, would change if we estimated it using a a different training data set (i.e. with different set of X).\n",
    "* Ideally this variance should be small between training sets.\n",
    "* If this variance is high, then a small change in X will result in a large change in Y.\n",
    "* In general more flexible statistical methods have higher variance.\n",
    "* Linear regression methods are inflexible, so few change sto the data points (x) will cause only a small shift in the line with out much changes to Y (MY THEORY).\n",
    "\n",
    "###### BIAS\n",
    "* Refers to the error that is introduced by approximating a real-life problem.\n",
    "* Linear Regression methods will have high bias.\n",
    "\n",
    "* As we use more flexible methods, the variance will increase and bias will decrease.\n",
    "* Here is the Bias-Variance Trade-Off\n",
    "    * As we increase the flexibility of a class of methods, bias tends to initially decrease faster than the variance increases. Consequentially the expected test MSE declines.\n",
    "    * However, as some point increasing flexibility has little impact on the bias but starts to significantly increase the variance, and thus increasing test MSE again.\n",
    "\n",
    "* Our goal is to reduce both variance and squared bias for a better prediction (MY THEORY)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 The Classification Setting\n",
    "* Bias-Variance trade off discussed above are for regression setting, but can also be applied to classification with only few modifications.\n",
    "* In Classification the Y is qualitative but not any more numerical.\n",
    "* For a given data if the predicted class level does not match with the actual class level then it is said to be misclassified. \"Training Error Rate\" is the number of misclassifications divided by the total population. Similarly Test Error Rate is the one associated with test data.\n",
    "* A good classifier is the one for which the test error rate is smallest.\n",
    "\n",
    "###### 2.2.3.a The Bayes Classifier\n",
    "* a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch-8: TREE BASED METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bagging, Random Forests and Boosting - all these approaches produce multiple trees which are then combined into a single consensus prediction. This combining of a large number of trees can ofter improve prediction accuracy by a large extend.\n",
    "* Tree based methods are useful for Interpretation\n",
    "* Decision Trees (pgs 311-314)\n",
    "    * Regression trees\n",
    "    * Classification trees [ $E = 1 - max_k(\\hat{p}_{mk})$ ]\n",
    "    * Classification trees with Gini index [ $G = \\sum\\limits_{k=1}^K\\hat{p}_{mk}(1 - \\hat{p}_{mk})$ ]\n",
    "    * Classification trees with entropy [ $D = -\\sum\\limits_{k=1}^K\\hat{p}_{mk}log\\hat{p}_{mk}$ ]\n",
    "    \n",
    "<Details>\n",
    "    <Summary show=\"expanded\">Terminology in Decision Trees</Summary>\n",
    "    \n",
    "* Regression Trees\n",
    "* Teermincal node leaf\n",
    "* Internal node\n",
    "* Branch\n",
    "</Details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Regression Trees...EXPAND</summary>\n",
    "    \n",
    "* Regression tree consits of series of splitting rules, starting at the top of the tree.\n",
    "* <b>Prediction via Stratification of the Feature Space</b> Two Steps:\n",
    "    * Divide the predictor space ($X_1, X_2,..,X_p$ into J distinct non overlapping regions $R_1, R_2,...,R_J$)\n",
    "    * For every observation that falls into the region $R_j$, which is simply the mean of the response values for the training observations in $R_j$\n",
    "* To construct the regions $R_1,R_2,...,R_J$ our goal should be to minimize the RSS \n",
    "$\\sum\\limits_{{j=1}}^J$ $\\sum\\limits_{i \\in R_j} (y_i - \\hat{y_{R_j}})^2$\n",
    "    * Where $y_{R_j}$ is the mean training response in $j^{th}$ set $R_j$\n",
    "* RSS is the square of the errors [$(\\sum{y_i-\\hat{y_i}})^2$]\n",
    "\n",
    "* <b>Recursive Binary Splitting</b> is a top-down greedy approach. For steps check Pg 307/434 of the book\n",
    "* Tree Pruning\n",
    "    * This process may produce good predictions on training set but is likely to overfit the data leading to poor test performance.\n",
    "    * One alternative is \"build the tree only so long as the decrease in the 'RSS due to each split' exceeds some (high) threshold\". It has disadvantages (refer pg 307).\n",
    "    * Better strategy is to grow a very large tree $T_0$ and then <i>prune</i> it back to obtain a <i>subtree</i>.\n",
    "    * For every observation that falls into the region $R_j$,\n",
    "        * in Regresssion trees we use the mean of the response values for the training observations in $R_j$ is usedd in Regression trees. where as \n",
    "        * in Classification trees we use the most commonly occurring class (\"Mode\" or most frequent class in statistical term) of training observations\n",
    "    \n",
    "Algorithm for Building a Regression Tree (Refer pg 309).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Classification Trees...EXPAND</summary>\n",
    "\n",
    "    * Classification tree is very similar to Regression tree, except that it is used to predict qualitative response rather than a quantitative one.\n",
    "    * A classification tree can be grown using the same \"recursive binary splitting\" method bu tusing <i>classification error rate</i>, for binary splitting criteria, in place of RSS in case of regression trees.\n",
    "    * Classification error rate used is the fraction of the training observations in that region that do not belong to the most common class\n",
    "    * For every observation that falls into the region $R_j$,\n",
    "        * in Regresssion trees we use the mean of the response values for the training observations in $R_j$ is usedd in Regression trees. where as \n",
    "        * in Classification trees we use the most commonly occurring class (\"Mode\" or most frequent class in statistical term) of training observations\n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree vs Linear Models\n",
    "* Linear Regresssion model assumes the form: $f(X) = \\beta_0 + \\sum\\limits_{j=1}^pX_j\\beta_j$\n",
    "* Tree model assumes the form: $f(X) = \\sum\\limits_{m=1}^Mc_m . 1_{(X \\in R_m)}$ where $R_1,\\dots \\R_M$ reprsent a partition of feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage and Disadvantags of Trees\n",
    "* [+] Trees are easy to explain people even easier to explain than linear regression.\n",
    "* [+] Trees can be displayed graphically and can be interpreted by all if trees are small.\n",
    "* [+] Tree approaches look more similar to human thnking process than the regression or classification approaches.\n",
    "* [+] Trees can handle qualitative prdictors without the need to create dummy variables.\n",
    "* [-] Trees can be very non-robust. A small change in data can cause a large change in the final estimated tree.\n",
    "* [-] Predictive accuracy of Trees are less than regression and classification approaches.\n",
    "    * [+] Preditive performance of trees can be improved greatly by aggregating many trees using methods like bagging, random forests and boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging, Random Forests and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bagging, Random forests, and Boosting use trees as building blocks to constriuct more powerful prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How to enhance prediction accuracy of a statistical model</b>\n",
    "* Take many training set from the population and build a separate prediction model using each training set, and average the resulting predictions.\n",
    "* In other words, we could calculate $f^1(x),f^2(x),\\dots,f^B(x)$ using B separate training sets, and average\n",
    "them in order to obtain a single low-variance statistical learning model given by $\\hat{f_{avg}}(x) = \\frac{1}{B}\\sum\\limits_{b=1}^B \\hat{f^b}(x)$\n",
    "\n",
    "* Given a set of n independent observations $Z_1,\\dots,Z_n$ each with variance $\\sigma^2$, the variance of the mean $Z_{mean}$ of the observations is given by $\\frac{\\sigma^2}{n}$.\n",
    "    * This means <i>averaging a set of observations reduces variance</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8.2.1 Bagging\n",
    "* Refer pgs 316 - 319\n",
    "* Bootstrap Aggregation or <u>Bagging</u> is a general-purpose procedure for reducing variance in statistical learning methods.\n",
    "* It is not practical to get access to multiple training sets.\n",
    "    * Instead, we can bootstrap, by taking repeated samples from the (single) training data sets.\n",
    "    * In this approach we generate B different bootstrapped training data sets.\n",
    "    * We then train our method on the $b^{th}$ bootstrapped training set to get $\\hat{f^{*b}}(x)$, and finally average all the predictions, to obtain $\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum\\limits_{b=1}^B \\hat{f^{*b}}(x)$. This is called <b>bagging</b>\n",
    "    * Bagging improves prediction accuracy at the expense of interpretability\n",
    "* Few other important points\n",
    "    * Out-of-Bag Error Estimation\n",
    "    * Variable Importance Measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8.2.2 Random Forests\n",
    "* Ref: pg 319\n",
    "* Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees.\n",
    "* Similar to BAgging, a number of decision trees are built on bootstrapped training samples (i.e. repeated samples form the same training set)\n",
    "* Difference or tweak is: Each time a split in a tree is considered, a <i>random sample of m predictors</i> is choosenas split candidates from the full set of p predictors.\n",
    "    * A fresh sample of m predictors is taken at each split.\n",
    "    * Typically we choose $m \\approx \\sqrt{p}$, i.e. aproximately equal to square root of total number of predictors. This reduces both test error and Out-ofBag error over bagging method.\n",
    "    * On average $\\frac{p-m}{p}$ of the splits will not even consider the strong predictor, so other predictors will have nore of a chance.\n",
    "* In contrast, Bagging considers all p prdictors as split candidates for each split, where presence of a few strong predictors will make all the trees look similar as most or all the trees will use this strong predictor in the top split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8.2.3 Boosting\n",
    "* Ref: pg 321\n",
    "* Boosting is a general approach like Bagging but \n",
    "    * As we can recollect - Bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\n",
    "    * Notably, each tree is built on a bootstrap data set, independent of the other trees.\n",
    "\n",
    "* Boosting works in a similar way, except that the trees are grown sequentially:\n",
    "* each tree is grown using information from previously grown trees.\n",
    "* Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "* For algorithm check pg 323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Methods from Udemy (spark course ch-13.45)\n",
    "* Nodes - are the places where we split for value of a certain attribute\n",
    "* Edges - outcome of a split to next node\n",
    "* Root - It is the node that performs the first split\n",
    "* Leaves - Terminal nodes that predict the outcome\n",
    "---\n",
    "* <b>Entropy</b> and <b>Information Gain</b> are the Mathematical Methods of choosing the best split.\n",
    "* Entropy:  $H(S) = -\\sum\\limits_ip_i(S)log_2p_i(S)$\n",
    "* Information Gain $IG(S, A) = H(S) -\\sum\\limits_{v \\in Values(A)}\\frac{|S_v|}{S}H(S_v)$\n",
    "* For more details check above blocks and pg 311-314 of \"Introduction to Statistical Learning\" pdf.\n",
    "---\n",
    "* In Random Forest - a <u>random sample of features</u> is chosen <b>for every single tree at every single split</b>. It is used for both classification and regression.\n",
    "* If random sample of features is not slected then the most strong feature will eb used by most of the trees as the top split, resulting in an ensemble of similat trees that are highly correlated, which will not reduce the variance by much, hence less accuracy in prediction will be the result.\n",
    "* Randomly leaving out candidate features from each split, decorrelates the treesso that the averagign process can reduce the variance.\n",
    "\n",
    "---\n",
    "<b>Gradient Boosting</b> invokves three elements\n",
    "1. A <i>loss function</i> to be optimised\n",
    "    1. It is a function/equation that we will use to determine how \"far-off\" our predictions are.\n",
    "    2. Regression may use a squared error and Classification may use logarithmic loss\n",
    "    3. Spark has already implemented these algorithms and we need to simply call the function, we do not need to deal with this directly.\n",
    "2. A <i>weak learner</i> to make predictions\n",
    "    1. Decision trees are used as weak learner in gradient boosting.\n",
    "    2. We can constraint the weak learners, such as maximum number of layers, nodes, splits or leaf nodes.\n",
    "3. An <i>additive model</i> to add weak learners to milimize the loss function.\n",
    "    1. Trees are added one at a time, and existing trees in the model are not changed.\n",
    "    2. A <i>gradient descent procedure</i> is used to minimize the loss when adding trees\n",
    "---\n",
    "Steps needed by us when using gradient boosted trees:\n",
    "1. Traina weak model m using data samples drawn according to some weight distriution (each sample with some weight).\n",
    "2. Increase the weight of samples that are misclassified by model m, and decrease the weight of samples that are classified correctly by model m.\n",
    "3. Train the nest weak model using samples drawn according to the updated weight distribution.\n",
    "\n",
    "In this way, the algorithm always trains models using data samples that are \"difficult\" to learn in previous rounds, which results an ensemble of models that are good at learnign different \"parts\" of training data.\n",
    "* Basically it is \"Boosting\" the weights of sampels that were difficult to get correct. Thus the algo name is gradient boosting.\n",
    "* Refer: https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "---\n",
    "* Spark handles all these algorithms and we can continue with default parameter.\n",
    "* But we can play around the parameters with the help of Statistical Learning pdf book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
