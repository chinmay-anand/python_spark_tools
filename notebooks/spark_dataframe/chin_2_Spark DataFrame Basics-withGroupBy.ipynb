{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPICS COVERED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to import a module from anotehr jupyter notebook file (.ipynb)\n",
    "* Use the .read() method from the spark session to create a DataFrame from a local file (.csv, .json etc)\n",
    "* Set a custom schema to the DataFrame through the sparkSession.read.json() operation\n",
    ">* Use StructField, StructType and various base types IntegerType, StringType from pyspark.sql.types to change the schema of a json being imported into a spark DataFrame.\n",
    ">* Use StructField(fieldName, StringType(), True) to define json type of a field of String type. For int type use IntegerType()\n",
    ">* Use a list of StructField entries representing all the columns of the json file\n",
    ">* Create the StructType schema (i.e. StructType(fields=list_of_StructFields)) and pass it as \"schema\" parameter to the sparksession read method (i.e. .read.json(jsonFile,schema=new_json_schema))\n",
    ">* The DataFrame returned above will have specific data types instead of Strings. Validate it using sparkDF.printSchema()\n",
    "* Selecting / filtering data from DataFrame.\n",
    ">* sparkSessn.sql(...) : Use direct Select SQL statements on the dataframe using sparkDF.sql(SQL_SELECT_STMT) e.g. \"select col1, col2 from table1 where age > 60 and duration < 120\"\n",
    ">* sDF.select(...).where(...) : This returns subset of DataFrame(sDF) with specified columns(in select clause)  satisfying the condition passed is where clause.\n",
    ">* sDF.filter(...) : This returns subset of DataFrame(sDF) with all columns satisfying the condition passed as filter e.g. \"age > 18\"\n",
    "* Use \"groupBy('col1')\" to group the data by the specified column in a DataFrame and run aggregate functions on the groupby object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark API Documentation:\n",
    "* http://spark.apache.org/docs/latest/\n",
    "* http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enabling Jupyter shell to print multiple results form a single shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable the shell to print multiple results (instead of only the last result)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Standard Imports for Spark\n",
    "* from pyspark.sql import SparkSession   ### Starting point for Spark - used to get the <b>spark session instance</b> using builder pattern\n",
    "* from pyspark.sql.types import StructField, IntegerType, StringType, StructType   ### For using <b>custom json schema</b>\n",
    "* from pyspark.sql.functions import concat, countDistinct, mean, avg, stddev       ### For using <b>aggregate groupby functions</b> with Spark DataFrames\n",
    "* from pyspark.sql.functions import col ### Returns a <b>column with name</b> specified by the parament\n",
    "* from pyspark.sql.functions import date_format, year,month,dayofmonth,hour,dayofweek,dayofyear,weekofyear,format_number   ### For handling <b>Date and Timestamp</b>\n",
    "* from .defs.Chinmay_Utilities import getCallLogXmlFromSuperbackup, getSparkDFfromPandasDF, getJsonFromSparkDF, printTextFile, getPandasDFfromSparkDF  ### My personal <b>utilities from another ipynb</b> jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import modules form another ipynb (jupuyter notebook written by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs  # Boilerplate required\n",
    "\n",
    "# Do a full import\n",
    "# from .full.Chinmay_Utilities import foo\n",
    "\n",
    "# Do a definitions-only import\n",
    "\n",
    "### Spark supproting methods\n",
    "from .defs.Chinmay_Utilities import getCallLogXmlFromSuperbackup, getSparkDFfromPandasDF, \\\n",
    "                                        getJsonFromSparkDF, getPandasDFfromSparkDF, getMaskedSparkDF\n",
    "\n",
    "### Printing methods\n",
    "from .defs.Chinmay_Utilities import printTextFile, getBold, getUnderlined, getColorInverted, printHighlighted\n",
    "\n",
    "\n",
    "# We can \"import ipynb.fs.defs.Chinmay_Utilities\" instead of two imports \"import ipynb.fs\" followed by \".defs.Chinmay_Utilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import the tool from Chinmay as an alternate to import of ipynb file\n",
    "* Convert the ipynb files into .py python scripts, add them to python path and import that file in the desired code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/nishita/exercises_udemy')\n",
    "from tools.chinmay_tools import printHighlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHelpOnFormattedText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All Spark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession   ### Starting point for Spark - used to get the <b>spark session instance</b> using builder pattern\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType   ### For using <b>custom json schema</b>\n",
    "from pyspark.sql.functions import concat, countDistinct, mean, avg, stddev, col    ### For using <b>aggregate groupby functions</b> with Spark DataFrames\n",
    "from pyspark.sql.functions import date_format, year,month,dayofmonth,hour,dayofweek,dayofyear,weekofyear,format_number   ### For handling <b>Date and Timestamp</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EXECUTE UPTO THIS POINT ALWAYS\n",
    "###### ----------------- The below points can be executed based on requirement ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame Basics Experiments\n",
    "###### (Section 8.1 to 8.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer Documentation for pyspqrk.sql package at https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting help on a method in a builder pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the help of a method / attribute in a builder pattern, \n",
    "#     split the pattern just before that method\n",
    "#     set a variable with the builder pattern result just before that method call, \n",
    "#         so that the method call can be performed on the variable.\n",
    "#     Now execute (SHIFT+ENTER) the help syntax i.e. \"method?\"\" NOT \"method()?\"\" on that variable\n",
    "#     \n",
    "# Below is an example for getting help on getOrCreate() method in \"SparkSession.builder.appName('Basics').getOrCreate()\"\n",
    "#     \n",
    "bld = SparkSession.builder.appName('Basics')\n",
    "# bld.getOrCreate??  ### Uncomment this line to get the help (\"?\") and code implementation (\"??\")\n",
    "#     \n",
    "# Here we can not use \"SparkSession.builder.appName('Basics').getOrCreate?\", because there is a use input involved (i.e. parameter of appName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment specific function below and run this shell to get help\n",
    "# getCallLogXmlFromSuperbackup?\n",
    "# getSparkDFfromPandasDF?\n",
    "# getJsonFromSparkDF?\n",
    "# printTextFile??\n",
    "# getPandasDFfromSparkDF?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read a json using SparkSession and analyse the result databrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a SparkSession\n",
    "sparkSesnBasic = SparkSession.builder.appName('Basics').getOrCreate()\n",
    "\n",
    "my_json_people = \"test_data/people.json\"\n",
    "printTextFile(my_json_people)\n",
    "sdf = sparkSesnBasic.read.json(my_json_people)\n",
    "\n",
    "# Types of files that can be read csv/format/jdbc/json/load/option/options/orc/parquet/schema/table/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printSchema automatically decides the schema based on data.\n",
    "sdf.printSchema()\n",
    "\n",
    "sdf.columns\n",
    "\n",
    "sdf.describe\n",
    "\n",
    "sdf.describe()\n",
    "\n",
    "# describe() given summary of numeric columns in the dataframe\n",
    "sdf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modify the JsonSchema using a user defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many times spark can not determine the data types in a json correctly and specifies each of the fields as String.\n",
    "# In this case we can define a schema and attach it to the json\n",
    "from pyspark.sql.types import StructField, IntegerType, StringType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StructField(field_name, field_type(), is_field_nullable)\n",
    "# To enforce the user defined scheme to a json pass a list of structfields one for each column\n",
    "data_schema = [StructField ('age', IntegerType(), False), \n",
    "                StructField('name', StringType(), True)]\n",
    "\n",
    "final_type = StructType(fields=data_schema)\n",
    "\n",
    "final_type\n",
    "final_type[\"age\"]\n",
    "final_type[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StructField?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the same json using the user defined schema (earlier it was the default one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTextFile(my_json_people)\n",
    "sdf2 = sparkSesnBasic.read.json(my_json_people, schema=final_type)\n",
    "\n",
    "sdf2.printSchema()\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer back the spark dataframe with builtin default schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking a method to get the sample call log using a module from another ipynb file in the same folder\n",
    "* ###### sparkCallSession.createDataFrame(p_df) --> converts pandas dataframe into spark dataframe\n",
    "* ###### s_df.select(\"*\").toPandas() --> converts spark dataframe into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the call log xml data in a Pandas DataFrame\n",
    "dfCallLogs = getCallLogXmlFromSuperbackup(\"test_data/calllogs_20200512130135.xml\")   ## Calling from Chinmay_Utilities.ipynb\n",
    "\n",
    "# Convert the Pandas DataFrame into Spark DataFrame\n",
    "sdfCallLogs = getSparkDFfromPandasDF(dfCallLogs)   ## Calling from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Select statements with Spark\n",
    "* We can use limited select statement (upto selection of columns, adding computed columns and without any where clause)\n",
    "* * spark dataframe can use complex where clause as explained below\n",
    "* To use full version of select sql along with where clause we need to register the Spark DataFrame as a table using the method below.\n",
    "* * sparkDF.createOrReplaceTempView(pseudo_tableView_name)\n",
    "* * This is used only with sparkSessn.sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the spark dataframe as a table/view to be used like standard sql using sparkSession.sql()\n",
    "sdfCallLogs.createOrReplaceTempView(\"call_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using pure SQL with DataFrames\n",
    "* Register the dataframe as a table (sdf.createOrReplaceTempView(table1)) and\n",
    "* Use sparkSession1.sql(SQL_STMT_using_table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering using SQL\n",
    "* ###### sparkSesn.sql(full_sql)\n",
    "* ###### This works but supports limited where clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Using pure SQL with DataFrames\n",
    "* Register the dataframe as a table and\n",
    "* Use sparkSession1.sql(SQL_STMT_table) E.g. sparkSesnBasic.sql(\"SELECT * FROM call_logs WHERE dur > 100 ORDER BY dur DESC\").show()\n",
    "* The where clause of this sql is not supporting LIKE clause\n",
    "* This complex where clause (LIKE clause) is possible through direct \"where\" clause on spark dataframe (next statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Quick filtering: (one of the two alternate ways on sparkDF)\n",
    "* ###### sparkDF.select(*).where(my_condition)\n",
    "* ###### sparkDF.filter(my_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_where_clause = \"upper(name) like '%SEEC%QA%' AND dur > 50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the Spark DataFrame directly with WHERE clause\n",
    "# This sort of complex where clauses (e.g. LIKE clauses) are not possible with pandas dataframe\n",
    "\n",
    "# sdfCallLogs.select(\"*\").where(\"upper(name) like '%SEEC%QA%'\").show()\n",
    "\n",
    "# Convert the filtered data into pandas data frame which can be processed or outputted into a file\n",
    "df_qa = sdfCallLogs.select(\"*\").where(my_where_clause).toPandas()\n",
    "df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfCallLogs.filter(my_where_clause).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns before converting to Pandas DataFrame\n",
    "df_qa2 = sdfCallLogs.filter(my_where_clause).select('name', 'number', 'dur', 'time') \\\n",
    "    .withColumnRenamed('dur', 'Duration (Sec)').withColumnRenamed('name', 'Name')\\\n",
    "    .withColumnRenamed('time', 'Date').withColumnRenamed('number', 'Phone Number').toPandas()\n",
    "\n",
    "# Below line inserts a new column with value double of the current 'dur' columns value\n",
    "df_qa2B = sdfCallLogs.filter(my_where_clause).select('name', 'number', 'dur', 'time') \\\n",
    "            .withColumn('double_duration',sdfCallLogs['dur']*2).toPandas()\n",
    "\n",
    "# To show a pd.DataFrame without column index\n",
    "df_qa2.style.hide_index()\n",
    "df_qa2B.style.hide_index()\n",
    "\n",
    "# To write an dataframe to an excel file without index column\n",
    "df_qa2.to_excel('1.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfCallLogs.filter(my_where_clause).filter((sdfCallLogs['dur']>300) & ~(sdfCallLogs['dur']<1000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfCallLogs.filter(my_where_clause).filter('dur>300 and dur>=1000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Convert the Spark DataFrame into Json\n",
    "###### Convert a Pandas DataFrame into Json by first converting into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getJsonFromSparkDF(sdfCallLogs)   ## Calling from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get Json from a pandas DataFrame fist convert itinto a Spark DataFrame and then get Json from it\n",
    "getJsonFromSparkDF(getSparkDFfromPandasDF(dfCallLogs))   ## Calling from Chinmay_Utilities.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check types of DataFrame, Columns and displaying selected columns as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sdf)\n",
    "type(sdf['Age'])\n",
    "#sdf['Age'].show()         # This line does nto work as we can not display columns\n",
    "sdf.select('Age').show()   # This returns a dataframe of selected columns\n",
    "type(sdf.select('Age'))\n",
    "\n",
    "sdf[sdf['Age']>0].show()    # This works similar to regular pandas dataframe filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display rows form top of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.head(10) # display atmost 10 rows from top of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming a column and Inserting a computed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = sdf.withColumnRenamed('age', 'old_age')\n",
    "type(sdf1)\n",
    "sdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming a column and Inserting a computed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = sdf.withColumn('double_age',sdf['age']*2)\n",
    "type(sdf2)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with GroupBy and Aggregates\n",
    "###### (Section 8.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSesnGrpby = SparkSession.builder.appName(\"chin_groupby\").getOrCreate()\n",
    "\n",
    "my_csv_sales_info = \"test_data/sales_info.csv\"\n",
    "sdf_sales = sparkSesnGrpby.read.csv(my_csv_sales_info, inferSchema=True, header=True)\n",
    "# allow spark to assume first row as the column names and to decide the data type from data value\n",
    "\n",
    "sdf_sales.printSchema()\n",
    "\n",
    "sdf_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales.groupBy(\"Company\").sum().collect()  # Returns a list of Row objects which can be used in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data_by_company = sdf_sales.groupBy('Company')\n",
    "group_data_by_company.sum().show()\n",
    "sdf_sales.groupBy('Company').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales.columns\n",
    "getJsonFromSparkDF(group_data_by_company.sum())  # using function from Chinmay_Utilities.ipyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using \".agg\" function (two params - 1: col name, 2:aggregate func name)\n",
    "* ###### Getting aggregate across entire table (or data frame)\n",
    "* ###### Getting aggregates for each unique value of the group by column\n",
    "* * The \".agg(col_name : agg_func_name)\" can also be used as \".agg_func(col_name)\"\n",
    "* * E.g:  sdf.groupby(\"Company\").agg({\"Company\" : \"max\"}) gives same result as sdf.groupby(\"Company\").max(\"Company\")\n",
    "* * VArious pre defined aggregate functions are: mean / min / max / sum / count etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales.agg({\"Sales\":\"count\"}).show()\n",
    "sdf_sales.groupBy(\"Company\").agg({\"Sales\":\"count\"}).show()\n",
    "\n",
    "# You can try with other aggregate functions mean / min / max / sum / count etc..\n",
    "\n",
    "# When used directly on a DataFrame, it gives the overall result across the dataframe\n",
    "# When used on top of a groupby result of a DataFrame, it gives the aggregate results for each unique value of groupby columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data_by_company = sdf_sales.groupBy('Company') ### Both are same\n",
    "\n",
    "group_data_by_company.agg({\"Sales\":\"max\"}).show()  # parameter format(\"col_name\" : \"aggregate_func_name\")\n",
    "# Various aggregate functions are: mean / min / max / aum / count etc..\n",
    "## This agg() format is more generalized and we can put in a for loop by passing aggregate func name in a param\n",
    "\n",
    "group_data_by_company.sum(\"Sales\").show()\n",
    "## This is more Rigid as we need to call the aggregate functions explicitly\n",
    "\n",
    "### sdf_sales.groupBy('Company') can also be used in place of group_data_by_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales.groupBy(\"Company\")\n",
    "# Aggregators on top of GroupedData returns a spark DataFrame for our consumption\n",
    "# Various gorubby aggregator functions: mean/sum/max/min/count\n",
    "sdf_sales.groupBy(\"Company\").mean().withColumnRenamed('avg(Sales)','Average Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data_by_company.agg({\"Sales\":\"mean\"}).show()\n",
    "group_data_by_company.agg({\"Sales\":\"max\"}).show()\n",
    "group_data_by_company.agg({\"Sales\":\"min\"}).show()\n",
    "group_data_by_company.agg({\"Sales\":\"sum\"}).show()\n",
    "group_data_by_company.agg({\"Sales\":\"count\"}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sales.groupBy('Company').mean(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Functions from Spark\n",
    "* Compute aggregate operations within \".select\" clause using functions\n",
    "* Cormat the result through a second select on first result\n",
    "* Sort data in a dataframe using column name (by default ascending order)\n",
    "* Sort data in dataframe in descending (using a descend ordered column)\n",
    "* Sort data in dataframe with different orders for different columns (uses column but not names, desc() applied to individual columns as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import a function from pyspark.sql.functions and use that function inside a SELECT statement like sdf.select(avg('Sales')),show() -- very useful in handling data, timestamps etc\n",
    "* sdf.select(avg('Sales'),alias(\"Average Sales\")),show()  -- This renames the column heading of the result\n",
    "* Other functions such as \".corr\" for correlatin, \".stddev\" for standard deviation etc can eb used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, countDistinct,avg, stddev\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getColorInverted(getUnderlined(getBold(\"Created a spark session and Loaded Spark DataFrame from a csv file\"))))\n",
    "\n",
    "sparkSesn3 = SparkSession.builder.appName(\"chin_groupby\").getOrCreate()\n",
    "\n",
    "# REPEATING DEFINITION (for readability)\n",
    "my_csv_sales_info = \"test_data/sales_info.csv\"\n",
    "\n",
    "# allow spark to assume first row as the column names and to decide the data type from data value\n",
    "sdf_sales = sparkSesn3.read.csv(my_csv_sales_info, inferSchema=True, header=True)\n",
    "\n",
    "sdf_sales.printSchema()\n",
    "\n",
    "sdf_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply aggregation functions and format the result\n",
    "* Aggregation functions are applied through functios within select clause of dataframe\n",
    "* Format the orderBy ressult\n",
    "* * sdfaggregted = sdf.select(aggr_func('col_name'))\n",
    "* * sdfFinal = sdf_aggregted.select(format_number('col_name', n))     # n = decoimal precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlight(\"Unique Sales Records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of distinct valeus in 'Sales' column\n",
    "printHighlighted(\"Unique Sales Records\")\n",
    "sdf_sales.select(countDistinct('Sales')).show()\n",
    "\n",
    "printHighlighted(\"Unique Sales Records - Column renamed\")\n",
    "sdf_sales.select(avg('Sales').alias('Unique Sales Count')).show()\n",
    "\n",
    "printHighlighted(\"Total Sales Records\")\n",
    "sdf_sales.agg({'Sales':'count'}).show()\n",
    "\n",
    "printHighlighted(\"Average Sales Records\")\n",
    "sdf_sales.select(avg('Sales')).show()\n",
    "\n",
    "printHighlighted(\"Standard deviation Sales Records\")\n",
    "sdf_sales.select(stddev('Sales')).show()\n",
    "\n",
    "printHighlighted(\"Standard deviation Sales (formatted precision)\")\n",
    "printHighlighted(\"\\t * format_number() should be applied through a second select on the result data frame from the first select\")\n",
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "sdf_sales.select(stddev('Sales').alias('stddev')).select(format_number('stddev',2).alias('std_dev')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorting data in a DataFrame\n",
    "* * ASCENDING:  sdf.orderBy(col_name)\n",
    "* * DESCENDING: sdf.orderBy(df[col_name].desc())\n",
    "* CASE#1: Ascending (default) order of a single column [use orderBy on a column name]\n",
    "* CASE#2: Ascending (default) order of multiple columns [use orderBy on a column name]\n",
    "* CASE#3: Descending order [Use orderBy on a column instead of a column_name], use desc() on the column\n",
    "* CASE#4: Different order for different columns order [Use orderBy on a multiple columns instead of a column_names], use desc() on the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted(\"Original dataframe\")\n",
    "sdf_sales.show()\n",
    "printHighlighted(\"CASE#1: Sorting a dataframe based on one column\")\n",
    "sdf_sales.orderBy('Sales').show()\n",
    "printHighlighted(\"CASE#2: Sorting a dataframe based on two columns (Look for 'Sales' of 350)\")\n",
    "sdf_sales.orderBy('Sales', 'Person').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted(\"CASE#3: Sorting (Descending order) a dataframe based on one column\")\n",
    "sdf_sales.orderBy(sdf_sales['Sales'].desc()).show()\n",
    "\n",
    "printHighlighted(\"CASE#4: Sorting (different order for different columns) a dataframe based on one column (Look for 'Sales' of 350)\")\n",
    "sdf_sales.orderBy(sdf_sales['Sales'].desc(), sdf_sales['Person']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missign data in spark DataFrames\n",
    "3 ways to handle\n",
    "* Delete the entrire records where value is mising for ateleast one column [sdf.na.drop]\n",
    "* Set the missign vales to \"null\"\n",
    "* Set the missign vales to a pre-decided value\n",
    "* Set the missign vales to a computed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted(\"Created a spark session and Loaded Spark DataFrame from acsv file\")\n",
    "sparkSesn4 = SparkSession.builder.appName(\"chin_miss\").getOrCreate()\n",
    "\n",
    "my_csv_contains_null = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/ContainsNull.csv\"\n",
    "# allow spark to assume first row as the column names and to decide the data type from data value\n",
    "\n",
    "sdf_missing = sparkSesn3.read.csv(my_csv_contains_null, inferSchema=True, header=True)\n",
    "sdf_missing.printSchema()\n",
    "sdf_missing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dropping null records\n",
    "* sdf.na.drop(how='any') --> drops rows with null in any field\n",
    "* sdf.na.drop() --> same result as sdf.na.drop(how='any')\n",
    "* sdf.na.drop(how='all') --> drops rows with null in all the fields - a very rare case\n",
    "* sdf.na.drop(threes=n) --> Keeps rows with atleast n non-null fields, i.e. drops rows with upto n-1 null fields\n",
    "* * if n > number of cols, then all rows are dropped - results is an empty data frame\n",
    "* * if n < 2, then no row is dropped - results is same as sdf.show()\n",
    "* sdf.na.drop(subset='MyCol') --> drops rows with null in column 'MyCol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_temp = \"* sdf.na.drop(how='any') --> drops rows with null in any field\"\n",
    "printHighlighted(str_temp)\n",
    "sdf_missing.na.drop(how='any').show() #drop if any field is null\n",
    "\n",
    "str_temp = \"* sdf.na.drop() --> same result as sdf.na.drop(how='any')\"\n",
    "printHighlighted(str_temp)\n",
    "sdf_missing.na.drop().show()   #.drop() is same as .drop(how='any') i.e. any non-null field\n",
    "\n",
    "str_temp = \"* sdf.na.drop(how='all') --> drops rows with null in all the fields - a very rare case\"\n",
    "printHighlighted(str_temp)\n",
    "sdf_missing.na.drop(how='all').show() #drop if all fields are null\n",
    "\n",
    "str_temp = \"* sdf.na.drop(threes=2) --> Keeps rows with atleast 2 non-null fields, i.e. drops rows with upto 2-1 null fields\"\n",
    "printHighlighted(str_temp)\n",
    "sdf_missing.na.drop(thresh=2).show() # drop if there are less than n (here less than 2) null fields in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_temp = \"* sdf.na.drop(subset='Name') --> drops rows with null in column 'Name'\"\n",
    "str_temp += \"\\n* sdf.na.drop(subset=('Name', 'Sales')) --> drops rows with null in any of the two columns 'Name' or 'Sales'\"\n",
    "str_temp += \"\\n* sdf.na.drop(subset='Name') --> EQUIVALENT to sdf.na.drop(subset=('Name'))\"\n",
    "printHighlighted(str_temp)\n",
    "sdf_missing.na.drop(subset=('Name')).show()\n",
    "sdf_missing.na.drop(subset=('Name', 'Sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Replacing null valeus with a fixed value\n",
    "* sdf.na.fill('MY STRING') --> Replaces all null values in all the String columns with given string\n",
    "* sdf.na.fill(n) --> Replaces all null values in Numeric columns with given number\n",
    "* sdf.na.fill('MY STRING', subset=('col1', 'col2')) --> replaces the null values with given string only for the list of columns supplied\n",
    "* * A passed in column is ignored if its type does not match with the type of value passed as first parameter.\n",
    "* * .fill('val', subset=('col1', 'col2')) is EQUIVALENT to .fill('val', ['col1', 'col2'])\n",
    "* * .fill('val', subset='col1') is EQUIVALENT to .fill('val', subset=('col1')) - is a special case of above one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf_missing.na.fill(999).na.fill('CHINMAY').show()\n",
    "sdf_missing.na.fill('ANAND', subset='Name').show()\n",
    "sdf_missing.na.fill('ANAND', ['Name', 'Sales']).show()  # integer column is ignored as passed value is of string type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Replace the null values in a numeric column with it's mean i.e mean of non-null values in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdf_missing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3e9a5a376676>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# mean_sales = sdf_missing.agg({'Sales':'mean'}).collect()[0][0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# mean_sales = sdf_missing.select(avg(sdf_missing['Sales'])).collect()[0][0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmean_sales\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msdf_missing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdf_missing\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msdf_missing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_sales\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sdf_missing' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, avg\n",
    "\n",
    "# mean_sales = sdf_missing.agg({'Sales':'mean'}).collect()[0][0]\n",
    "# mean_sales = sdf_missing.select(avg(sdf_missing['Sales'])).collect()[0][0]\n",
    "mean_sales = sdf_missing.select(mean(sdf_missing['Sales'])).collect()[0][0]\n",
    "sdf_missing.na.fill(mean_sales, ['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Dates and Timestamps in Spark DataFrame (for apple stock)\n",
    "* ###### Ref: https://obstkel.com/spark-sql-date-functions\n",
    "* ###### Ref: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "sparkSessnStock = SparkSession.builder.appName(\"chin_date\").getOrCreate()\n",
    "my_csv_app_stocks = \"test_data/appl_stock.csv\"\n",
    "sdf_stock = sparkSessnStock.read.csv(my_csv_app_stocks, header=True, inferSchema=True)\n",
    "sdf_stock.columns\n",
    "sdf_stock['Date', 'Open'].head(3)\n",
    "#sdf_stock.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find Average closing price of the stock per year\n",
    "1. Create a new column 'Year' in the DF with the value of year (use sdf.withColumn())\n",
    "2. Call \".groupBy('year).avg()\" on result of step-1 -- each averaged numeric column 'col_name' will be named avg(col_name)\n",
    "3. Call \".select('Year', 'avg(Close)')\" on result of step-2\n",
    "4. Call a second select with same set of columns but the formatting_needed_columns passed through format_number()\n",
    "    * \".select('Year', format_number(avg(Close), nPrecision).alias('Avg Close'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted(\"Added computed columns 'Year', 'Month', 'Day', 'Week Day' to the table\")\n",
    "\n",
    "sdf_stock_new = sdf_stock.select(\n",
    "    year('Date').alias('Year'),\n",
    "    month('Date').alias('Month'),\n",
    "    dayofmonth('Date').alias('Day'),\n",
    "    dayofweek('Date',).alias('Week Day'),\n",
    "    date_format('Date','yyyy-MM-dd'),'*'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Different aggregations on different columns on a groupby object and formatting the result and displaing in multi-sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted(\"Added computed columns 'Year', 'Month', 'Day', 'Week Day' to the table\")\n",
    "\n",
    "grpby_year_stock = sdf_stock_new.groupBy('Year')\n",
    "agg_stock_dataframe = \\\n",
    "    grpby_year_stock.agg({'Close':'avg', 'Open':'min', 'High':'max', 'Low':'min'})\\\n",
    "                    .select('Year', \\\n",
    "                        format_number('avg(Close)', 2).alias('avg_close'), \\\n",
    "                        format_number('min(Open)', 2).alias('min_open'), \\\n",
    "                        format_number('max(High)', 2).alias('max_high'), \\\n",
    "                        format_number('min(Low)', 2).alias('min_low'))\n",
    "printHighlighted(\"Grouped by 'Year'\")\n",
    "agg_stock_dataframe.orderBy('Year').show()\n",
    "\n",
    "printHighlighted(\"Grouped by 'Year', sorted by 'Year' ascending and 'avg_close' descending - cascaded orderBy\")\n",
    "agg_stock_dataframe.orderBy('Year').orderBy(agg_stock_dataframe['avg_close'].desc()).show()  ##cascaded orderby\n",
    "\n",
    "printHighlighted(\"Grouped by 'Year', sorted by 'Year' ascending and 'avg_close' descending - 1st as col name, 2nd as column\")\n",
    "agg_stock_dataframe.orderBy('Year',agg_stock_dataframe['avg_close'].desc()).show()  ## mixed ordering\n",
    "\n",
    "printHighlighted(\"Grouped by 'Year', sorted by 'Year' ascending and 'avg_close' descending - both as columns\")\n",
    "agg_stock_dataframe.orderBy(agg_stock_dataframe['Year'],agg_stock_dataframe['avg_close'].desc()).show() # descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
