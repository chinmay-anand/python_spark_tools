{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFER: Chapter Sec 4-4.3 of \"Introduction to Statistical Learning\" by Gareth James"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark API Documentation:\n",
    "* http://spark.apache.org/docs/latest/\n",
    "* http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/\n",
    "\n",
    "## [Introduction to Statistical Learning](<https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "* Logistic Regression is a classificaiton algorithm, where we try to predict discrete categories\n",
    "* Linear Regresssion is used for predicting continuous data\n",
    "* whereas Logistic Regression used for classification or grouping into discrete categories (in contrast to word \"regression\" pointing at continuous data)\n",
    "* For understanding concepts behind the evaluation methods and metrics behind classificaiton\n",
    ">* REFER sections 4 - 4.3 in Introduction to Statistical Learning by Gareth James\n",
    "* EXAMPLES of Binary classification: (classification between two classes)\n",
    ">* \"Spam\" vs \"Ham\" emails (Finding a mail whether it is bad (ham) or good (ham)\n",
    ">* Loan Default (Yes/No) - Finding whether a customer will default loan or not\n",
    ">* Disease Diagnosis - e.g. whether a patient will be diagnosed with cancer or not based on certain body parameters\n",
    "* To classify into one of the two classes we need a function that fits binary categorical data. We can not use Linear Regression as a lot of poitns will not fit into it.\n",
    "* There is a funciton called \"Sigmoid Function\" that returns value between 0 and 1 for any input value.\n",
    "* So to do binarry classification (say between class 0 and class 1) we can follow the below appraoch:\n",
    ">1. We take the results from Linear Regression.\n",
    ">2. Pass the linear regression result into Sigmoid Function to get values between 0 and 1.\n",
    ">3. Then we set the cut-off point at 0.5\n",
    ">4. Anything below the cut-off points results in class 0 and anything above will be classified as class 1.\n",
    "* Below block explains in brief how Sigmoid Function works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function (aka Logistic)\n",
    "\n",
    "# $\\phi(x) = \\frac{e^x}{e^x+1}  =  \\frac{1}{1+e^{-x}}$\n",
    "This sigmoid function always returns values between 0 to 1 for any value of x\n",
    "* So we can take our Linear Regression solution and place it into the Sigmoid Function\n",
    "\n",
    "* <h6 style=\"display: inline\"></h6>Linear Model\n",
    "<h2 style=\"display: inline\">$ y = b_0 + b_1x$</h2>\n",
    "\n",
    "* <h6 style=\"display: inline\"></h6>Logistic Model\n",
    "<h2 style=\"display: inline\">$\\phi(y) = \\frac{1}{1+e^{-y}} = \\frac{1}{1+e^{-(b_0+b_1x)}}$</h2>\n",
    "* So no matter what is the result of Linear Regression, the result will always be between 0 to 1\n",
    "* This results in a probability from 0 to 1 belonging to one class.\n",
    "* Then we set the cut-off point at 0.5, anything below it results in class 0 and anything above will be classified as class 1.\n",
    "* ![](./logistic_regression_steps.png)\n",
    "* <table><tr>\n",
    "    <td><img src=\"logistic_regression_steps.png\" width=\"500\" /></td>\n",
    "    <td><img src=\"confusiton_matrix_metrics_ratios.png\" width=\"400\" /></td>\n",
    "  </tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Metrics in Logistic Regression\n",
    "* Legends:  T-True, F-False, P-Positive, N-Negative\n",
    "* The metrics is measured by using a confusion matrix.\n",
    "* Accuracy is $\\frac{TP + TN}{Total Cases}$\n",
    "* Misclassification rate is $\\frac{FP + FN}{Total Cases}$\n",
    "* Precision = $\\frac{True Positives}{Predicted Positives}$ = $\\frac{TP}{TP + FP}$\n",
    "* Sensitivity or Recall = $\\frac{True Positives}{Conditional Positives}$\n",
    "* Specificity = $\\frac{False Positives}{Atual Positives}$ = $\\frac{FP}{TP + FN}$\n",
    "* Type-I error = False Positives, Tyupe-II error = False Negatives\n",
    "<img src=\"metrics_from_confusion_matrix.png\" width=\"700\" height=\"100\" />\n",
    "* Receiver Operator Curve (ROC Curve) is the plot between Sensitivity (y-axis) vs 1-Specificiity (x-axis) i.e. graph of True positive vs False positive rate\n",
    ">* Area under the curve is a metric for how well the model fit the data.\n",
    ">* Above the red random guess lime means our model is performing better than random guess. If below then model is performing worse than the random guess. <img src=\"roc_curve_plot.png\" width=\"500\" height=\"100\" />\n",
    "* Other type of metric used in BinaryClassificationEvaluator is PR curve or Precision Recall curve.\n",
    "<hr/>\n",
    "* Metrics for binary classification are 'areadUnderROC' or 'areaUnderPR' (default: areaUnderROC)\n",
    "* Metrics for multiclass classification are f1|weightedPrecision|weightedRecall|accuracy) (default: f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators\n",
    "* Evaluation DataFrame - Evaluation DataFrames are the dataframes returned by model.evaluate(test_data) method\n",
    "* Evaluators are similar to Machine Learning Algorithm objects but are designed to take evaluation dataframes.\n",
    "* Evaluators being experimental, we should be cautious while using them on production code.\n",
    "* We have two types of evaluators:\n",
    ">1. BinaryClassificationEvaluator\n",
    "        * Metrics for binary classification are 'areadUnderROC' or 'areaUnderPR' (default: areaUnderROC)\n",
    "        * Expects 'rawPrediction' field (a +ve or -ve float value) in input evaluation dataframe\n",
    ">2. MulticlassClassificationEvaluator\n",
    "        * Metrics for multiclass classification are f1|weightedPrecision|weightedRecall|accuracy) (default: f1)\n",
    "        * Expects 'prediction' field (0.0 or 1.0 a binary number) in input evaluation dataframe\n",
    "* Both these evaluators take evaluation dataframe as their parametr to .evaluate() method and that method returns a value between 0.0 to 1.0 (i.e. %0 to 100%) indicating the mathc percentage between the observed values ('label' column) and predicted values('prediction' column)\n",
    "* The inout dataframe to these evaluator are the \"predictions\" dataframe member of the result of fitted_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps in Linear Regression (flashback)\n",
    "* Load CSV file into DataFrame (spark dataframe)\n",
    "  * To load simple text files use sparksessn.read.csv(...) etc\n",
    "  * To load vectorized text files use sparksessn.read.format('libsvm').load(...)\n",
    "    * When we load a vector formatted or libsvm text then we need not worry about VectorAssembler as the data is alreayd having vectorized features column. We can directly jump to train-test splitfor our requirement.\n",
    "* Convert participating string feature columns into numeric using StringIndexer. Use .fit() and .transform() to get the converted dataframe.\n",
    "* Create a VectorAssembler with this indexed dataframe including the numeric feature columns and generating a new vectorised combined feature column\n",
    "* Get the final dataframe using the combined vectorized feature and numeric label column.\n",
    "* Split the final data into training data frame (70%) and test data frame (30%)\n",
    "* Create a LinearRegression with the combined feature and label columns, do a .fit(sdf) to get the model.\n",
    "* Do a .evaluate() on the model with test data to get comparison of test_result (evaluation) and test_data.label col to find various parameters.\n",
    "* Then you can run .transform on the model with a unlabeled data (could be test data minus the labels) to get the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps in Logistic Regression\n",
    "* Load CSV file into DataFrame (spark dataframe)\n",
    "  * ??? To load simple text files use sparksessn.read.csv(...) etc\n",
    "  * To load vectorized text files use sparksessn.read.format('libsvm').load(...)\n",
    "    * When we load a vector formatted or libsvm text then we need not worry about VectorAssembler as the data is alreayd having vectorized features column. We can directly jump to train-test splitfor our requirement.\n",
    "* ??? Convert participating string feature columns into numeric using StringIndexer. Use .fit() and .transform() to get the converted dataframe.\n",
    "* ??? Create a VectorAssembler with this indexed dataframe including the numeric feature columns and generating a new vectorised combined feature column\n",
    "* Get the final dataframe using the combined vectorized feature and numeric label column.\n",
    "* Split the final data into training data frame (70%) and test data frame (30%)\n",
    "* Create a LogisticRegression with the combined feature and label columns, do a .fit(sdf) to get the model.\n",
    "* Do a .evaluate() on the model with test data to get BinaryLogisticRegressionSummary.\n",
    "* To get a BinaryLogisticRegressionTrainingSummary object get the summary attribute of fitted model itself at the beginning before calling fitted_model.evaluate().\n",
    "* Check the \"prediction\" dataframe member of this above summary field (i.e. the result of fitted_model.evaluate(test_data))\n",
    "* The explore this evaluation dataframe we can evaluate using \"BinaryClassificationEvaluator\" or \"MulticlassClassificationEvaluator\". The metric for evaluation also differes based on the evaluator used.\n",
    ">* REFER \"<b>Evaluators</b>\" section above for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import convenient utilities from my personal library\n",
    "* The \"import *\" brings in all y modules along with the setup that allows multiple implicit prints in a notebook shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/nishita/exercises_udemy/tools/')\n",
    "from chinmay_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mLoad the libsvm or vector formatted text\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('Load the libsvm or vector formatted text')\n",
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.appName('logireg_1').getOrCreate()\n",
    "training = spark1.read.format('libsvm').load('Logistic_Regression/sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                100|\n",
      "|   mean|               0.57|\n",
      "| stddev|0.49756985195624287|\n",
      "|    min|                0.0|\n",
      "|    max|                1.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()\n",
    "training.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mInstantiate a LogisticRegression model to be fitted / trained next\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('Instantiate a LogisticRegression model to be fitted / trained next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Create the model\n",
    "# lr = LogisticRegression()   ## This line is same as below one as the parameter values are default ones\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mFirst fit the entire dataset and check the \"predictions\" dataframe from summary of the fitted model\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('First fit the entire dataset and check the \"predictions\" dataframe from summary of the fitted model')\n",
    "model_fitted_all = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[19.8534775947478...|[0.99999999761359...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.377398194908...|[1.41321555111056...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.401459284891...|[1.25804865126979...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-18.862741612668...|[6.42710509170303...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.483011833009...|[1.27157209200604...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[19.8506078990277...|[0.99999999760673...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.337256674833...|[1.47109814695581...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.595579753418...|[3.08850168102631...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[19.2708803215613...|[0.99999999572670...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.6202328360422...|[0.99999999994480...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-24.385235147661...|[2.56818872776510...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.3082522490179...|[0.99999999999624...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[25.8329060318703...|[0.99999999999396...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-19.794609139086...|[2.53110684529575...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.0260440948067...|[0.99999999926123...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-22.764979942873...|[1.29806018790941...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[21.5049307193954...|[0.99999999954235...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[31.9927184226421...|[0.99999999999998...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-20.521067180414...|[1.22409115616505...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-22.245377742755...|[2.18250475400332...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\u001b[4m\u001b[1mWe can observe that the \"prediction\" column or predicted results is mostly mattching with the \"label\" (the actual values) \u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_fitted_all.summary.predictions.printSchema()\n",
    "model_fitted_all.summary.predictions.show()\n",
    "printUnderlined('We can observe that the \"prediction\" column or predicted results is mostly mattching with the \"label\" (the actual values) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That was a basic way to do a Logistic Regression\n",
    "#### We will now introduce concept of 'evaluators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mTo evaluate the model we need to split the entire data into train_data and test_data, train a new model with train_data and evaluate the test_data with the trained model.\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('To evaluate the model we need to split the entire data into train_data and test_data, '\n",
    "                    + 'train a new model with train_data and evaluate the test_data with the trained model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|                69|\n",
      "|   mean|0.5652173913043478|\n",
      "| stddev|0.4993602044724247|\n",
      "|    min|               0.0|\n",
      "|    max|               1.0|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             label|\n",
      "+-------+------------------+\n",
      "|  count|                31|\n",
      "|   mean|0.5806451612903226|\n",
      "| stddev| 0.501610310127101|\n",
      "|    min|               0.0|\n",
      "|    max|               1.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.BinaryLogisticRegressionSummary"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = training.randomSplit([0.7, 0.3])\n",
    "train_data.describe().show()\n",
    "test_data.describe().show()\n",
    "\n",
    "# final_model = LogisticRegression(featuresCol='features', labelCol='label', predictionCol='prediction')\n",
    "final_model = LogisticRegression()\n",
    "model_fitted_train_data = final_model.fit(train_data)\n",
    "predictions_and_labels_summary = model_fitted_train_data.evaluate(test_data)\n",
    "type(predictions_and_labels_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[23.9967265226565...|[0.99999999996212...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[32.2218848363852...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[31.0461877476375...|[0.99999999999996...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[20.8145774182799...|[0.99999999908726...|       0.0|\n",
      "|  0.0|(692,[125,126,127...|[23.3025129766084...|[0.99999999992416...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[23.4852932650678...|[0.99999999993683...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[25.5661709794507...|[0.99999999999211...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[29.5303545329574...|[0.99999999999985...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.8600559943781...|[0.99999999967915...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[19.2979032615241...|[0.99999999584063...|       0.0|\n",
      "|  0.0|(692,[128,129,130...|[20.5111516165922...|[0.99999999876371...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[28.3775724158964...|[0.99999999999952...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[17.3662388121604...|[0.99999997129627...|       0.0|\n",
      "|  1.0|(692,[100,101,102...|[4.61287647234435...|[0.99017426970475...|       0.0|\n",
      "|  1.0|(692,[119,120,121...|[0.30211769168229...|[0.57496012235295...|       0.0|\n",
      "|  1.0|(692,[123,124,125...|[-26.832741842560...|[2.22171467385479...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.981850763946...|[7.04103689097961...|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-19.831232228571...|[2.44008678411899...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-21.037368476630...|[7.30444051231184...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-22.306220900643...|[2.05367327070884...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\u001b[7m\u001b[1mThe result comes pretty fine i.e. actual \"Label\" values are almost identical to \"prediction\" col values because of well created documentation example data\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions_and_labels_summary.predictions.show()\n",
    "printHighlighted('The result comes pretty fine i.e. actual \"Label\" values are almost identical to \"prediction\" col values because of well created documentation example data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mNow we will explore the evaluation of this prediction using an evaluator object.\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('Now we will explore the evaluation of this prediction using an evaluator object.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>BinaryClassificationEvaluator</b> expects 3 parameters rawPredictionCol, labelCol and metricName with default values 'rawPrediction', 'label' and 'areaUnderROC' respevtively.\n",
    "* Other values for metricName can be 'areaUnderPR' ROC for Receiver Operator Characteristics curve and PR stamds for Precision Recall curve.\n",
    "* BinaryClassificationEvaluator.evaluate() expects an evaliuation dataframe with labels/observations and predictions\n",
    "    * The evaluation dataframe is the \"predictions\" dataframe member of the BinaryLogisticRegressionSummary object returned from fitted_model.evaluate(test_data)\n",
    "* binary_evaluator.evaluate(predictions_dataframe) returns a value between 0 to 100% indicating the match between the 'label' value and 'prediction' value\n",
    "* It is a bit difficult to grab accuracy, precision or recall etc directly from the BinaryClassificaitonEvaluator, and for this we use <b>MulticlassClassificationEvaluator</b>\n",
    "* BinaryClassificationEvaluator expects <i>rawPrediction</i>, where as MulticlassClassificationEvaluator expects <i>prediction</i> column (binary value of 0 or 1) which has already taken care of 'probability'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming default values 'rawPrediction', 'label' and default metricName is 'areaUnderROC' (other metric is 'areaUnderPR') in its parameter\n",
    "bin_evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryClassificationEvaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[23.9967265226565...|[0.99999999996212...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[32.2218848363852...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[31.0461877476375...|[0.99999999999996...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[20.8145774182799...|[0.99999999908726...|       0.0|\n",
      "|  0.0|(692,[125,126,127...|[23.3025129766084...|[0.99999999992416...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[23.4852932650678...|[0.99999999993683...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[25.5661709794507...|[0.99999999999211...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[29.5303545329574...|[0.99999999999985...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.8600559943781...|[0.99999999967915...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[19.2979032615241...|[0.99999999584063...|       0.0|\n",
      "|  0.0|(692,[128,129,130...|[20.5111516165922...|[0.99999999876371...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[28.3775724158964...|[0.99999999999952...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[17.3662388121604...|[0.99999997129627...|       0.0|\n",
      "|  1.0|(692,[100,101,102...|[4.61287647234435...|[0.99017426970475...|       0.0|\n",
      "|  1.0|(692,[119,120,121...|[0.30211769168229...|[0.57496012235295...|       0.0|\n",
      "|  1.0|(692,[123,124,125...|[-26.832741842560...|[2.22171467385479...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.981850763946...|[7.04103689097961...|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-19.831232228571...|[2.44008678411899...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-21.037368476630...|[7.30444051231184...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-22.306220900643...|[2.05367327070884...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_and_labels_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_evaluator.evaluate(predictions_and_labels_summary.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The metrics useed for BinaryClassificationEvalator are 'areaUnderROC'(default) or 'areaUnderPR'\n",
    "    * ROC means Receiver Operator Curvve\n",
    "    * PR means Precision-Recall curve\n",
    "* The value of 1 for 'areaUnderROC' indicates that our model perfectly fits under ROC curve and for every record of test_data evaluation predicted values ('prediction') is identical to observer value('label'). The values are 0 or 1 (bimnary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating again with a non efault metric 'areaUnderPR' also resulted in a perfect fit as well.\n",
    "BinaryClassificationEvaluator(metricName='areaUnderPR').evaluate(predictions_and_labels_summary.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mevaluate() method of Binary evaluator returned 1 indicating exact match between two columns \"label\" and \"prediction\" \u001b[0m\u001b[0m\n",
      "\u001b[7m\u001b[1mSo it was a very good fit - may be due to the fact that data was from documentation example.\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('evaluate() method of Binary evaluator returned 1 indicating exact match between two columns \"label\" and \"prediction\" ')\n",
    "printHighlighted('So it was a very good fit - may be due to the fact that data was from documentation example.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain the parameter details of the multi class evaluator using multi_evaluator.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labelCol: label column name. (default: label)\\nmetricName: metric name in evaluation (f1|weightedPrecision|weightedRecall|accuracy) (default: f1)\\npredictionCol: prediction column name. (default: prediction)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_evaluator.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mPrint scores based on default f1, or weighted precision score, or weighted recall scoe or accuracy\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9358904852263485"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9358904852263485"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9440860215053763"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.935483870967742"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9354838709677419"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printHighlighted('Print scores based on default f1, or weighted precision score, or weighted recall scoe or accuracy')\n",
    "multi_evaluator.evaluate(predictions_and_labels_summary.predictions)\n",
    "multi_evaluator.evaluate(predictions_and_labels_summary.predictions, {multi_evaluator.metricName:'f1'})\n",
    "\n",
    "# Predicted positives = True Positives + False Positives = TP + FP\n",
    "# Actual positives = True Positives + False Negatives = TP + FN\n",
    "\n",
    "# precision = TP/(TP+FP) i.e. TP / Total positives predicted i.e. perentage of true positives predicted\n",
    "multi_evaluator.evaluate(predictions_and_labels_summary.predictions, {multi_evaluator.metricName:'weightedPrecision'})\n",
    "\n",
    "# recall or sensitivity TP/(TP+FN) i.e. TP / Actual positives predicted i.e. perentage of tru positives predicted\n",
    "multi_evaluator.evaluate(predictions_and_labels_summary.predictions, {multi_evaluator.metricName:'weightedRecall'})\n",
    "\n",
    "# accuracy (TP+TN) / Total population i.e. perentage of truth in the prediction either positive or negative i.e. (TP+TN)/(TP+TN+FP+FN)\n",
    "multi_evaluator.evaluate(predictions_and_labels_summary.predictions, {multi_evaluator.metricName:'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"predictions\" dataframe in the summary (BinaryLogisticRegressionTrainingSummary or BinaryLogisticRegressionSummary) has the following fields\n",
    ">1. 'label' -- actual labels from input\n",
    ">2. 'features'\n",
    ">1. 'rawPrediction' -- result from Logistic Regression\n",
    ">1. 'probability' -- probability for the raw prediction\n",
    ">1. 'prediction' -- label as predicted by the model.\n",
    "* Here Label and Prediction are binary either 0 and 1, representing one of the two classes in binary classification.\n",
    "\n",
    "* The fitted_model.summary gives BinaryLogisticRegressionTRAININGSummary')\n",
    "* The fitted_model.evaluate(test_data) returns BinaryLogisticRegressionSummary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted('VALIDATING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_fitted.summary.accuracy\n",
    "predictions_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
