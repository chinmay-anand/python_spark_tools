{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TITANIC CLASSIFICATION EXAMPLE\n",
    "###### Project Requirements\n",
    "* There are a lot of examples on Titanic for different machine learning libraries\n",
    "* We wil predict which passengers survived Titanic crash based solely on passenger's features (age, cabin, how many children, male/female etc.)\n",
    "* Actual conclusion was peopple who are male or from a lower class such as third class tended not to survive.\n",
    "* People with higher classes (e.g. first class) or those who are female had  a higher likelihood of survival.\n",
    "* We will explore some better ways to deal with categorical data in a two-step process.\n",
    "* We will demomstrate a way on how to use pielines to set stages and build models that can be easily used again.\n",
    "* We will also deal with a lot of missign data.\n",
    "\n",
    "#### Instructor uses DataBricks's notebook setup ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/nishita/exercises_udemy/MyTrials/tools/')\n",
    "from chinmay_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load titanic data from csv file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_titanic = SparkSession.builder.appName('chin_titanic').getOrCreate()\n",
    "\n",
    "sdf_titanic = spark_titanic.read.csv('Logistic_Regression/titanic.csv', inferSchema=True, header=True)\n",
    "\n",
    "sdf_titanic.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns:\n",
    "* Sex -- Male / Female\n",
    "* SibSp -- indicates Siblings and Spouses they have onboarded\n",
    "* Parch -- indicates Parent and Children they have onboarded\n",
    "* Fare -- ticket price paid by the passengers\n",
    "* Cabin -- Cabin occupied by the passenger\n",
    "* Embarked -- It is the city name where the passnenger has embarked - actual string is a single letter\n",
    "<br/><br/>\n",
    "* PassengerID is just a index column and is not useful for our prediction\n",
    "<br/>\n",
    "\n",
    "###### We will select only th ecolumns that are useful to us\n",
    "\n",
    "* Now we sill select the columns 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'\n",
    "* We will think about 'Name' column late to decide whether he/she is a doctor or a mr or a mrs etc while usingg feature engineering.\n",
    "* What about 'Cabin'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data for null values and drop null records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mSelect desired columns and check null record counts\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printHighlighted('Select desired columns and check null record counts')\n",
    "sdf_titanic_myfields = sdf_titanic.select('Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked')\n",
    "\n",
    "# Fields with null: Age:177, Sex:2\n",
    "sdf_titanic.count()\n",
    "sdf_titanic_myfields.count()\n",
    "sdf_titanic_myfields.filter('Embarked is null OR Age is null').count()\n",
    "\n",
    "sdf_titanic_myfields.filter('Survived is null OR Pclass is null OR Sex is null OR Age is null '\n",
    "                            +' OR SibSp is null OR Parch is null OR Fare is null OR Embarked is null').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dealing with Null records\n",
    "* There are 177 records with null in 'Age' field and 2 records with null in 'Embarked' column\n",
    "* We will drop these 179 records from total of 891 titanic records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_titanic_myfields2 = sdf_titanic_myfields.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "707         0       3  female  39.0      0      5  29.1250        Q\n",
       "708         0       2    male  27.0      0      0  13.0000        S\n",
       "709         1       1  female  19.0      0      0  30.0000        S\n",
       "710         1       1    male  26.0      0      0  30.0000        C\n",
       "711         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[712 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_titanic_myfields2.count()\n",
    "\n",
    "df_titanic_myfields = sdf_titanic_myfields2.toPandas()\n",
    "\n",
    "df_titanic_myfields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the titanic dataframe there are two string fields, 'Sex' and 'Embarked', we need to convert them first to numeri (using StringIndexer) and then into a vector (using OneHotEncoder), and that vector will be part of final vectorized features column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dealing with String categorical fields\n",
    "* Convert the string field into numeric fields using StringIndexer with 0, 1, 2, ... upto number of unique values\n",
    "* Convert the numeric field into vector field using OneHotEncoder\n",
    "* <u>Example of StringIndexer and OneHotEncoder</u>\n",
    "* Suppose there are 3 unique values in a string field A, B, C\n",
    "* StringIndexer coverts the string column into numeric with values in [0, numLabels]\n",
    ">* most frequent label gets index 0 and next frequent gets 1 and so on as the default valeu of stringOrderType is 'frequencyDesc'\n",
    "<pre>\n",
    "<hr/>\n",
    "STRING:  A  B  C\n",
    "<hr/>\n",
    "NUMERIC: 0  1  2     (after StringIndexer)\n",
    "<hr/>\n",
    "A:   [1, 0, 0]       (after OneHotEncoder)\n",
    "B:   [0, 1, 0]\n",
    "C:   [0, 0, 1]\n",
    "</pre>\n",
    "* Say there are n categories or n unique values in the column then post OneHotEncoder each category will get a unique vector with n binary elements (o or 1) having at most single one-value.\n",
    "* Most frequent label will get first element as 1 and remaining n-1 elements as zeroes\n",
    "* Next frequent label will get second element as 1 and remaining (includign first one) will be zeroes  and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use StringIndexer to convert categorical values to categorical index which is a number in [0, numValues]\n",
    "#### Then use OneHotEncoder to convert that index into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler  #,  VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex', outputCol='SexVec')\n",
    "\n",
    "embark_indexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex', outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+--------+-----------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|SexIndex|EmbarkIndex|\n",
      "+--------+------+------+----+-----+-----+-------+--------+--------+-----------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|     0.0|        0.0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|     1.0|        1.0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|     1.0|        0.0|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|     1.0|        0.0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|     0.0|        0.0|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|     0.0|        0.0|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|     0.0|        0.0|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|     1.0|        0.0|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|     1.0|        1.0|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|     1.0|        0.0|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|     1.0|        0.0|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|     0.0|        0.0|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|     0.0|        0.0|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|     1.0|        0.0|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|     1.0|        0.0|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|     0.0|        2.0|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|     1.0|        0.0|\n",
      "|       0|     2|  male|35.0|    0|    0|   26.0|       S|     0.0|        0.0|\n",
      "|       1|     2|  male|34.0|    0|    0|   13.0|       S|     0.0|        0.0|\n",
      "|       1|     3|female|15.0|    0|    0| 8.0292|       Q|     1.0|        2.0|\n",
      "+--------+------+------+----+-----+-----+-------+--------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+------+------+----+-----+-----+-------+--------+--------+-----------+-------------+-------------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|SexIndex|EmbarkIndex|       SexVec|    EmbarkVec|\n",
      "+--------+------+------+----+-----+-----+-------+--------+--------+-----------+-------------+-------------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|     1.0|        1.0|    (1,[],[])|(2,[1],[1.0])|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|     1.0|        1.0|    (1,[],[])|(2,[1],[1.0])|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|     0.0|        2.0|(1,[0],[1.0])|    (2,[],[])|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|     1.0|        0.0|    (1,[],[])|(2,[0],[1.0])|\n",
      "|       0|     2|  male|35.0|    0|    0|   26.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       1|     2|  male|34.0|    0|    0|   13.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|       1|     3|female|15.0|    0|    0| 8.0292|       Q|     1.0|        2.0|    (1,[],[])|    (2,[],[])|\n",
      "+--------+------+------+----+-----+-----+-------+--------+--------+-----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_gi = gender_indexer.fit(sdf_titanic_myfields2).transform(sdf_titanic_myfields2)\n",
    "sdf_ei = embark_indexer.fit(sdf_gi).transform(sdf_gi)\n",
    "sdf_ge = gender_encoder.transform(sdf_ei)\n",
    "sdf_ee = embark_encoder.transform(sdf_ge)\n",
    "\n",
    "sdf_ei.show()\n",
    "sdf_ee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- SexIndex: double (nullable = false)\n",
      " |-- EmbarkIndex: double (nullable = false)\n",
      " |-- SexVec: vector (nullable = true)\n",
      " |-- EmbarkVec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_ee.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SWITCH here to use index column instead of vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(inputCols=['Pclass', 'SexIndex', 'Age', 'SibSp', 'Parch', 'Fare', 'EmbarkIndex'], \n",
    "#                             outputCol='features')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Pclass', 'SexVec', 'Age', 'SibSp', 'Parch', 'Fare', 'EmbarkVec'], \n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vec = assembler.transform(sdf_ee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(final_vec)\n",
    "final_vec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|Survived|\n",
      "+--------------------+--------+\n",
      "|[3.0,1.0,22.0,1.0...|       0|\n",
      "|[1.0,0.0,38.0,1.0...|       1|\n",
      "|(8,[0,2,5,6],[3.0...|       1|\n",
      "|[1.0,0.0,35.0,1.0...|       1|\n",
      "|[3.0,1.0,35.0,0.0...|       0|\n",
      "|[1.0,1.0,54.0,0.0...|       0|\n",
      "|[3.0,1.0,2.0,3.0,...|       0|\n",
      "|[3.0,0.0,27.0,0.0...|       1|\n",
      "|[2.0,0.0,14.0,1.0...|       1|\n",
      "|[3.0,0.0,4.0,1.0,...|       1|\n",
      "|(8,[0,2,5,6],[1.0...|       1|\n",
      "|[3.0,1.0,20.0,0.0...|       0|\n",
      "|[3.0,1.0,39.0,1.0...|       0|\n",
      "|(8,[0,2,5,6],[3.0...|       0|\n",
      "|(8,[0,2,5,6],[2.0...|       1|\n",
      "|[3.0,1.0,2.0,4.0,...|       0|\n",
      "|[3.0,0.0,31.0,1.0...|       0|\n",
      "|[2.0,1.0,35.0,0.0...|       0|\n",
      "|[2.0,1.0,34.0,0.0...|       1|\n",
      "|(8,[0,2,5],[3.0,1...|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = final_vec.select('features', 'Survived')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|           Survived|\n",
      "+-------+-------------------+\n",
      "|  count|                533|\n",
      "|   mean| 0.3921200750469043|\n",
      "| stddev|0.48868187046102685|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|         Survived|\n",
      "+-------+-----------------+\n",
      "|  count|              179|\n",
      "|   mean|0.441340782122905|\n",
      "| stddev|0.497940016085883|\n",
      "|    min|                0|\n",
      "|    max|                1|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logi_reg_titanic = LogisticRegression(labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = logi_reg_titanic.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.1541, -2.8507, -0.0395, -0.2415, -0.2194, 0.0014, 0.3856, 0.7592])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.754659843854747"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.coefficients\n",
    "fitted_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|            features|Survived|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|(8,[0,1,2,5],[2.0...|     0.0|[2.63746670590744...|[0.93323429294680...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     0.0|[2.29903633934555...|[0.90879719757418...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     0.0|[2.37662469385721...|[0.91502736386518...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     1.0|[2.69246786166459...|[0.93658072394292...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     0.0|[2.77143450570510...|[0.94111253677795...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     0.0|[2.81091782772536...|[0.94326295943713...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     0.0|[4.11386745439376...|[0.98391840393830...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|     0.0|[4.33102572550516...|[0.98701673437556...|       0.0|\n",
      "|(8,[0,1,2,6],[1.0...|     0.0|[0.36486075486602...|[0.59021657439017...|       0.0|\n",
      "|(8,[0,1,2,6],[1.0...|     0.0|[0.40434407688628...|[0.59973092237048...|       0.0|\n",
      "|(8,[0,1,2,6],[3.0...|     0.0|[2.59406683494580...|[0.93047875372223...|       0.0|\n",
      "|(8,[0,1,2,6],[3.0...|     0.0|[3.10735002120911...|[0.95719491014473...|       0.0|\n",
      "|(8,[0,2,3,5],[1.0...|     1.0|[-2.1822276956640...|[0.10135783924347...|       1.0|\n",
      "|(8,[0,2,4,5],[3.0...|     0.0|[1.30357282041601...|[0.78643566878644...|       0.0|\n",
      "|(8,[0,2,5],[2.0,3...|     1.0|[-1.2792984279881...|[0.21766967031927...|       1.0|\n",
      "|(8,[0,2,5],[3.0,1...|     1.0|[-0.7114054400167...|[0.32928836373033...|       1.0|\n",
      "|(8,[0,2,5],[3.0,1...|     1.0|[-0.6715073555927...|[0.33815940087359...|       1.0|\n",
      "|(8,[0,2,5],[3.0,1...|     1.0|[-0.6715307639474...|[0.33815416192528...|       1.0|\n",
      "|(8,[0,2,5],[3.0,1...|     0.0|[-0.5911624220234...|[0.35636818518701...|       1.0|\n",
      "|(8,[0,2,5],[3.0,1...|     1.0|[-0.5532618972532...|[0.36510795554176...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted_model.summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label_summary = fitted_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8236708860759491"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|            features|Survived|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|(8,[0,1,2,5],[3.0...|       0|[2.53454620767601...|[0.92652843038914...|       0.0|\n",
      "|(8,[0,1,2,5],[3.0...|       0|[3.14652606489752...|[0.95877162064999...|       0.0|\n",
      "|(8,[0,1,2,6],[1.0...|       0|[0.44382739890653...|[0.60917064444963...|       0.0|\n",
      "|(8,[0,1,2,6],[3.0...|       0|[1.92285036060147...|[0.87245594889079...|       0.0|\n",
      "|(8,[0,1,2,6],[3.0...|       1|[2.15975029272299...|[0.89657639618747...|       0.0|\n",
      "|(8,[0,2,5,6],[1.0...|       1|[-3.1374359091208...|[0.04158920274159...|       1.0|\n",
      "|(8,[0,2,5,6],[1.0...|       1|[-2.9229681275681...|[0.05102977546114...|       1.0|\n",
      "|(8,[0,2,5,6],[1.0...|       1|[-2.9327800127527...|[0.05055671542530...|       1.0|\n",
      "|(8,[0,2,5,6],[1.0...|       1|[-2.6219961018900...|[0.06773613501948...|       1.0|\n",
      "|(8,[0,2,5,6],[2.0...|       1|[-1.4289570389486...|[0.19326124166262...|       1.0|\n",
      "|(8,[0,2,5,6],[2.0...|       1|[-1.2748784200472...|[0.21842329085483...|       1.0|\n",
      "|(8,[0,2,5,6],[2.0...|       1|[-1.0743079897081...|[0.25458468727895...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       0|[-1.1362909043651...|[0.24300200612588...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       0|[-0.9782466018117...|[0.27323983469394...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       0|[-0.8232043156154...|[0.30508389628798...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       1|[-0.7805146096867...|[0.31420898649133...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       0|[-0.7018633476699...|[0.33139922921605...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       1|[-0.6624910401221...|[0.34018025613819...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       1|[-0.6625902803322...|[0.34015798126719...|       1.0|\n",
      "|(8,[0,2,5,6],[3.0...|       1|[-0.6231069583120...|[0.34907515247895...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_label_summary.accuracy\n",
    "prediction_label_summary.areaUnderROC\n",
    "prediction_label_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mHere we had the Actual values in \"Survived\" column and predicted values in \"prediction\" column \u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('Here we had the Actual values in \"Survived\" column and predicted values in \"prediction\" column ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mNow to find the accuracy, precision etc we need to use evaluators\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "printHighlighted('Now to find the accuracy, precision etc we need to use evaluators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator(labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mEvaluating the predictions using BinaryClassificationEvaluator using metrics \"(NONE|areaUnderROC|areaUnderPR)\" \u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8236708860759491"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8236708860759491"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8333082122957263"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printHighlighted('Evaluating the predictions using BinaryClassificationEvaluator using metrics \"(NONE|areaUnderROC|areaUnderPR)\" ')\n",
    "bin_eval.evaluate(prediction_label_summary.predictions)\n",
    "BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC').evaluate(prediction_label_summary.predictions)\n",
    "BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderPR').evaluate(prediction_label_summary.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8236708860759491"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_label_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mEvaluating the predictions using MulticlassClassificationEvaluator using metrics \"(NONE|f1|weightedPrecision|weightedRecall|accuracy)\" \u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7812198595203173"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7812198595203173"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7815584938489967"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.782122905027933"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                                                         (NONE|f1|weightedPrecision|weightedRecall|accuracy)')\n",
    "printHighlighted('Evaluating the predictions using MulticlassClassificationEvaluator using metrics \"(NONE|f1|weightedPrecision|weightedRecall|accuracy)\" ')\n",
    "multi_eval.evaluate(prediction_label_summary.predictions)\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"f1\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"weightedPrecision\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"weightedRecall\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7815584938489967"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.782122905027933"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_label_summary.weightedPrecision\n",
    "prediction_label_summary.weightedRecall\n",
    "prediction_label_summary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following evaluation is by using StringIndexer alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mEvaluating the predictions using BinaryClassificationEvaluator using metrics \"(NONE|areaUnderROC|areaUnderPR)\" \u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8236708860759491"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8236708860759491"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8333082122957263"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printHighlighted('Evaluating the predictions using BinaryClassificationEvaluator using metrics \"(NONE|areaUnderROC|areaUnderPR)\" ')\n",
    "bin_eval.evaluate(prediction_label_summary.predictions)\n",
    "BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC').evaluate(prediction_label_summary.predictions)\n",
    "BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderPR').evaluate(prediction_label_summary.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[7m\u001b[1mEvaluating the predictions using MulticlassClassificationEvaluator using metrics \"(NONE|f1|weightedPrecision|weightedRecall|accuracy)\" \u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7812198595203173"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7812198595203173"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7815584938489967"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.782122905027933"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                                                         (NONE|f1|weightedPrecision|weightedRecall|accuracy)')\n",
    "printHighlighted('Evaluating the predictions using MulticlassClassificationEvaluator using metrics \"(NONE|f1|weightedPrecision|weightedRecall|accuracy)\" ')\n",
    "multi_eval.evaluate(prediction_label_summary.predictions)\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"f1\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"weightedPrecision\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"weightedRecall\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following evaluation is by using StringIndexer and OneHotEncoder both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printHighlighted('Evaluating the predictions using BinaryClassificationEvaluator using metrics \"(NONE|areaUnderROC|areaUnderPR)\" ')\n",
    "bin_eval.evaluate(prediction_label_summary.predictions)\n",
    "BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC').evaluate(prediction_label_summary.predictions)\n",
    "BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderPR').evaluate(prediction_label_summary.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "multi_eval = MulticlassClassificationEvaluator(labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                         (NONE|f1|weightedPrecision|weightedRecall|accuracy)')\n",
    "printHighlighted('Evaluating the predictions using MulticlassClassificationEvaluator using metrics \"(NONE|f1|weightedPrecision|weightedRecall|accuracy)\" ')\n",
    "multi_eval.evaluate(prediction_label_summary.predictions)\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"f1\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"weightedPrecision\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"weightedRecall\"})\n",
    "multi_eval.evaluate(prediction_label_summary.predictions, {multi_eval.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
