{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY POINTS FROM THE COURSE LESSONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_3_Linear_Reggression_Consulting_Project.ipynb\n",
    "------------------------------------------------------------------------------\n",
    "PURPOSE\n",
    "* This project is to determine number of crew members needed for future ships being built.\n",
    "* Given are various parameters of the ship like number of cruise_line, passengers, cabin count, age of ship, length and weight of ship etc.\n",
    "* Also given the condition that the number of crews may depend on the cruise_line name.\n",
    "\n",
    "STEPS:\n",
    "* We are going to create a model to predict 'crew' which is the crew count.\n",
    "* This has a dependency on a string field 'cruise_line'.\n",
    "* For the Spark ML algorithm to work we need to prepare the data consisting of two columns:\n",
    ">1. The features column should be a vector containing details from all the desired numeric fields of the input data and \n",
    ">2. a label field, the 'crew' numeric field to predict.\n",
    "* 'Cruise_line' string field being one of the predictors, we need to convert it from categorical value to a numeric field using \"StringIndexer\".\n",
    ">* str_Indexer = StringIndexer(inputCol='Cruise_line', outputCol='Cruise_line_indexed')\n",
    ">* sdf_cruises_indexed = str_Indexer.fit(sdf_cruises).transform(sdf_cruises)\n",
    "* Then use this indexed field along with other participating numeric fields to generate the vectorized 'features' column, using VectorAssembler, for spark ML libs to process further.\n",
    ">* assembler = VectorAssembler(inputCols=feature_input_col_list, outputCol='features')\n",
    ">* sdf_cruises_indexed_vec_data = assembler.transform(sdf_cruises_indexed)\n",
    "* Pull out the vector column ('features') amd label columne ('crew') into a dataframe and that will be inut to the spark ML library.\n",
    "* split this final data into training data and test data.\n",
    ">* final_data = sdf_cruises_indexed_vec_data.select('features', 'crew')\n",
    ">* train_data, test_data = final_data.randomSplit([0.7, 0.3])\n",
    "* Train the LinearRegression (lr) instance with training data i.e. lr_model = lr.fit(training_data)and \n",
    ">* lr_ship = LinearRegression(featuresCol='features', labelCol='crew', predictionCol='prediction')\n",
    ">* lr_ship_model = lr_ship.fit(train_data)\n",
    "* Evaluate it on test data i.e. test_Result = lr_model.evaluate(test_data)\n",
    "* Cross validate the error metrics from the result (test_result.rootMeanSquaredError, .r2, .residuals.show() ) and from model print .coefficients, .intercept etc\n",
    "* The R-squared (i.e. .r2) was a bit higher side indicating a good fit, hence we became suspicious and tried to find the cause of good result by trying to find the correlation of the label column ('crew') with few of the ship features like 'passengers' and 'cabins' in the original input dataframe. We found the corr values to be high in the original data and hence the R-squared indicated a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Linear_Regression_Code_Along.ipynb\n",
    "-----------------------------------------\n",
    "* Decide which numeric columns will be treated as features and which numeric field will be our label, based on the oinput data.\n",
    "* Create a VectorAssembler from the feature columns ([inputCols]) to get a combined vectorized column (outputCol)\n",
    "* Then invoke transform method using vectorAsembler on the input dataframe which will add a new vectorized column to the dataframe and that new field will have the data from all the \"inputCols\"\n",
    "* Now extract a dataframe with the vector column and label columns as this two columns are only needed by spark MLlib to predictionusing Supervised model. This is our final_data for input to our model.\n",
    ">* Create a Vector assembler with a list parameter containing the names of numeric columns and a outputCol holding th ename of the vectorized column that will be added to the result by this assempber.transform() method.\n",
    "* Get the combined merged feature column and the label field as a two-column dataframe and call it 'final_data' to be passed into our model for further operations.\n",
    "* Split the final input data into train_data and test_data in 70:30 ratio.\n",
    "* Train the model on train_data, evaluate the model on test_Data. Let the test_results.residuals be the evaluated labels.\n",
    "* Check the accuracy performance of the model evaluation by checking various parameters of test_result, such as .r2.residuals, rootMeanSquaredError, .meanSuaredError, .meanAbsoluteError etc.\n",
    "* We don't ave any unlabeled data, so we will make the test_data unlabeled so as to predict the labels for those features. As this test_data is not used in training the model, model is not aware of this.\n",
    "* Predict the labels for this unlabeled dataand compare the 'prediction' column of the predicted_result against the test_result,r2,residuals.\n",
    "* Here I have used sparkSession.sql (\"SELECT.  .. .. \") beautifully to combine data from multiple dataframes and to find the deviations in our predictions outout from the actual data in test_data. For this we first registered the participating dataframes as views and then performed complex select statements using these views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Linear_Regression_Example.ipynb\n",
    "-----------------------------------------\n",
    "* Loading libsvm formatted text files (contians the verctorized text data using sparkSessn.read.format('libsvm').load(\"libsvm_formatted_text_file.txt\")\n",
    "* The libsvm formatted text file does nto have any column name defined in the file. It contained 'label' in the first column and the trailing columns were in the format 1:float1 2:float2 3:float3 etc. the libsvm format reader of sparkSessin automatically merges all these training columns intelligently and creates a merged column that contains te veector of all these feature columns\n",
    "* In real scenarios we need to prepare the vector column for all features merged into one using VectorAssembler (explained in next chapter)\n",
    "* For me direct session.read.format().load() did not work and I had to use session.read.format('libsvm').option('numFeatures', nFeatrureCount).load(...)\n",
    "* Split this data in 70:30 ratio to traindata and test_data respectively\n",
    "* Instantiate a LinearRegression model and train or fit it with training data.\n",
    "* Now evaluate the model against test data, which compares the predicted labels (from te result test_result) against the labels in test_data.\n",
    "* Check the results by using test)result, .rootMeanSquaredError, .r2.residuals and fw other error metrics such as . meanSquaredError, .meanAbsoluteError etc.\n",
    "* Create a dataset without labels (prepare mimiced data from test_data, which is not seen by the model), by stripping off the labels from test_Data and applying model.transform on this unlabeled data to get the prediction values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Spark DataFrame Basics-withGroupBy.ipynb\n",
    "---------------------------------------------------------------------------\n",
    "* printing formatted text in python using '\\\\x1B\\[Nm' where N is a integer (check chinmay_tools.py)\n",
    "\n",
    "* Converting Pandas DataFrame to Spark DataFrame and vice versa. (check chinmay_tools.py)\n",
    "* Using inferSchema attribute during spark DataFrame creation from file to decide teh data types automatically.\n",
    "* Additionally we can customize json schema to change the datatypes in dataframe while loading from json files (repeat)\n",
    "* Renaming a column in a DataFrame using sdf.withColumnRename(old_col_name, new_col_name)\n",
    "* Adding or replacing a column with a different computed value or a masked value e.g. using sdf.withColumn(colName, sdf['col3']+5) or replacing with a string literal using .lit(const_string) function\n",
    "* Using groupBy() on spark DataFrames.\n",
    "* Using aggregates on groupBy objects.\n",
    "* If a DataFrame has multipel numeric fields then we can apply multiple aggregates on a single groBy object with one aggr fn per numeric column.\n",
    "* If multiple aggr fns are applied on a single column then last function on that column is resulted and all previous funcs on that column are ignored. grpByCompany.agg({'col1':'aggfn1', 'col2':'aggfn2', 'col3':'aggfn3'})\n",
    "* Sorting dataframe in ascending order by using orderBy with column name. sdf.orderBy('col1') sorts in ascending order of col1.\n",
    "* Sorting dataframe in descending order by using orderBy with column (but not column name). sdf.orderBy(sdf['col1'].desc()) sorts in descending order of col1.\n",
    "* If we want to sort a dataframe with different orders for each field, then we must use columns for descending ordering but we can use either columns or just column names for thecolumns with ascending order.`\n",
    "* Handling missing data in a DataFrame\n",
    "* Dropping the records if all fields are null, or if any field is null or there are less than n null fields (thres=n). sdf.na.drop(...), \"subset\" attribute can decide which columns to check for null. Here null means na.\n",
    "* Replacing numm values with specified string using sdf.na.fill(...), for any field, for the field / fields listed suing subset or list array. Also replacing with a computed value from an aggregate result of remaining nun-null values.\n",
    "* Handling Date fields using functions from pyspark.sql.functions e.g. year(), month() etc.. or date_format('date_field_name', my_format_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Spark DataFrame Basics.ipynb\n",
    "--------------------------------------------------------\n",
    "* Using modules or functions from another custom python file by adding th efile path to \"sys.path\"\n",
    "* Using modules or functions from another ipynb jupyter notebook (less convenient)\n",
    "* Getting help on any mdule that we are using through question marks\n",
    "* Using a custom defined schema (to change data type) while importing a json into a spark DataFrame\n",
    "* Using dataframe.select().where(), dataframe.where(), dataframe.filter() to get subsets of a dataframe with filtered conditions\n",
    "* Using sparkSessn.sql() to get results from multiple DataFrames by registering the dataframes as views and performing compex select statements on those views.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chinmay_tools.ipynb\n",
    "-------------\n",
    "* Generating python file from jupyter notebook files (.ipynb) using \"ipynb-py-convert\" application (installe using pip)\n",
    "* Generating html or pdf files from jupyter notebook files using syntax \"jupyter nbconvert -to=html test.ipynb\", which produces test.html. To generate pdf, print the html using system wide pdf rinter. Don't use \"--to=pdf\" which needs a lot of tools to be installed.\n",
    "* Using our python modules or functions in another python / ipynb notebook by adding the fodler location of the python file to \"sys.path\" list\n",
    "* Enabling notebook shells to display multiple print results (without using explicit print statement by setting InteractiveShell.ast_node.interactivity to \"all\".\n",
    "* Print formatted text on console concatenating \"\\\\x1B\\[Nm\" and \"\\\\x1B\\[0m\" at begining and at end of a string.\n",
    "* Printing contents of a text file (.txt, .csv, .xml etc)\n",
    "* Convert Pandas DataFrame into a Spark DataFrame and vice versa.\n",
    "* Convert the contents of a Spark DataFrame into Json.\n",
    "* Mask certain colums of a Spark DataFrame using lit() function and sdf.withColumn() function.\n",
    "* Converts the content of a xml into a Pandas DataFrame. The xml format is hardcoded to the call log xml generated by an android applicaiton named \"SuperBackUp\".\n",
    "* Print some LaTeX characters\n",
    "* Print multiple header lines like H1, H2, H3 etc in a single line using <i>'style=\"display: inline\"'</i> css style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
