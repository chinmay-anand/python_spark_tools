{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY POINTS FROM THE COURSE LESSONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_NaturalLanguageProcessing_Exercises.ipynb\n",
    "---------------\n",
    "STEPS:\n",
    "* Tokenized the sentences.\n",
    "* Used \"StopwordsRemover\" from spark to filter out the common stop words from our list of tokens or words\n",
    "* OPTIONAL: Used \"udf\" to define a user defined function with lambda -- a useful utility function -- an example of creating the utility functions on the fly and reuse it repatedly.\n",
    "* OPTIONAL:  Used \"NGram\" to show sequence of \"n\" consecutive words -- Useful in advanced NLP -- useful when we are trying to find te relationship between the words which may for example appear next to each other etc.\n",
    "* We can generate vectorized 'features using <b>TF-IDF</b> as explained below.\n",
    "    1. Using HashingTF generate TF (Term Frequency) from the list of words (without stopwords). Pass a number higher than the vocabulary size as \"numFeatures\" parameter. Vocabulary is the list of tokens across all the documents (i.e. all records of the DF)\n",
    "    2. Generate TF-IDF (by applying IDF on TF resulted column) in a field 'features'\n",
    "* Alternately we can use <b>CountVectorizer</b> (in place of <b>HashingTF</b>) to generate 'features' column by supplying directly the list of words, a number higher than the vocabulary size as \"vocabSize\" parameter and a \"minDF\" parameter to ensure inclusion of the token in the vocabulary only if that token appears in atleast those many documents.\n",
    "* <b>To compute TF-IDF we need to compute IDF on top of CountVectorizer or TF.</b>\n",
    "* This 'features' field is the one we will need to use in our ML algoritms that we plan to use.\n",
    "\n",
    "<details><summary><u>Explanation for working of CountVectorizer model</u></summary>\n",
    "    \n",
    "* Suppose we have a sentence dataframe with two sentences which are tokenized into [a, b, c] and [a, b, b, c, a] and CountVectorizer has generated the 'features' column as \"(3,[0,1,2],[1.0,1.0,1.0])\" and \"(3,[0,1,2],[2.0,2.0,1.0])\" respectively for id or label's of 0 and 1.\n",
    "* Now we can explain the 'features' column as below\n",
    "###### The 'features' field for second sentence (id==1) can be explained as below\n",
    "* Second document (id==1) is:  [a, b, b, c, a]\n",
    "* features(for second document) is: (3,[0,1,2],[2.0,2.0,1.0])\n",
    "    * First element in 'features' is 3 - it represents the actual vocabulary size, i.e. number of distinct terms : a, b, c\n",
    "    * Second element is [0,1,2] -- it is a subset of the entire set (i.e. vocabulary) of words -- it shows which terms form the vocabulary appear in 'words' column of the associated sentence or df record.\n",
    "    * Third element is [2.0,2.0,1.0] -- it shows frequency of the occurring terms in order. In this example the term frequency for [0th, 1st, 2nd] terms i.e. [a,b,c] are [2.0, 2.0, 1.0)] respectively. I.e. in the second document i.e for sdf_result[id==1]. That means a appears twice, b appears twice, but c appears once.\n",
    "* This is essentially the \"Bag of Words\" method works.\n",
    "</details>\n",
    "<br/>\n",
    "<details><summary><b>SMS Spam Classification using NaiveBayes algo with NLP</b></summary>\n",
    "\n",
    "* DATA PREPARATION STEPS\n",
    "    * csv--(read, rename columns)-->data_in--(StringIndexer)-->data_labeled--(Tokenizer)-->data_words--(StopWordsRemover)-->data_filtered\n",
    "    * data_filtered--(CountVectorizer)-->data_TF--(IDF)-->data_TFIDF--(VectorAssembler)-->data_features\n",
    "    * data_features.select(['label', 'features'])-->data_clean\n",
    "* SPAM PREDICTION (ML PROCESSIGN & EVALUATION)\n",
    "    * clean_data--(randomSplit)-->train_data,test_data\n",
    "    * NaiveBays().fit(train_data)-->spam_predictor\n",
    "    * spam_predictor.transform(test_data)-->predicted_test_results\n",
    "    * MulticlassClassificationEvaluator().evaluate(predicted_test_results)-- compares 'prediction' column against 'label'\n",
    "</details>\n",
    "\n",
    "<b>[Comparison between HashingTF and CountVectorizer (towardsdatascience)](<https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e>)</b><br/>\n",
    "<b>[Comparison between HashingTF and CountVectorizer (stackexchange)](<https://stackoverflow.com/questions/35205865/what-is-the-difference-between-hashingtf-and-countvectorizer-in-spark>)</b><br/>\n",
    "<b>[NaiveBayes Algorithm (analyticsvidhya)](<https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/>)</b>\n",
    "\n",
    "\n",
    "# ASK Ashish Lal: HOW TO PRINT CountVectorizer vocabulary in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Sustems reference links\n",
    "\n",
    "###### Refereces: \n",
    "<b>\n",
    "<a href=\"https://towardsdatascience.com/how-to-build-a-simple-recommender-system-in-python-375093c3fb7d\">How to build a Simple Recommender System in Python ~ towardsdatascience.com</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/beginners-recommendation-systems-with-python-ee1b08d2efb6\">Beginnerâ€™s Recommendation Systems with Python (with the TMDB 5000 movies dataset) ~ towardsdatascience.com</a>\n",
    "\n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/\">Comprehensive Guide to build a Recommendation Engine from scratch (in Python)  ~ Analytics Vidya</a>\n",
    "\n",
    "<a href=\"https://realpython.com/build-recommendation-engine-collaborative-filtering/\">Build a Recommendation Engine With Collaborative Filtering ~ RealPython.com</a>\n",
    "\n",
    "<a href=\"https://www.datacamp.com/community/tutorials/recommender-systems-python\">Recommender Systems in Python: Beginner Tutorial ~datacamp</a>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101\">Recommender Systems in Python 101 ~ Kaggle</a>\n",
    "\n",
    "<a href=\"https://www.google.com/search?q=how+to+build+a+great+recommnder+system+using+python\">Google search for \"how to build a great recommnder system using python\"</a>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_Recommender_System_Exercises.ipynb\n",
    "---------------\n",
    "STEPS (Recommender Systems)\n",
    "* Load the dataset check the fields (recommender algo ALS - ALternating Least Squares needs few columns )\n",
    "* 3 most important parameters are: userCol, itemCol, ratingCol in addition to maxIter and regParam\n",
    "fields (For more details check https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html)\n",
    "    * For input data we don't need 'features' vectorized column and hence no vectorAssembler needed.\n",
    "    * Parameters to 'usrCol' and 'itemCol' serve as input. Compare these two against 'features' vector column in case of regression or classifiation algorithms.\n",
    "    * 'ratingCol' is similar to label field (in regression or classification algorithms). Here the actual or observed rating is supplied which is given by the user (userCol=) to the movie (itemCol-)\n",
    "* Process is the same .fit(train_data) on the ALS model that gets us the fitted_model i.e.als_model\n",
    "* fitted_model.transform(test_Data) returns predictionsDF dataframe which gives us the predictions about the user rating to movies in test_data based on the training given to the machine on the training data movies.\n",
    "* Evaluate by measuring metrics - RegressionEvaluator(metricName='rmse', ratingCol='rating', predictionCol='prediction').evaluate(predictionsDF)\n",
    "* In this result \"predictionsDF\" dataframe the field 'prediction' tells the recommended ratings for respective movies\n",
    "* To give recommendation to a specific user (i.e. unlabeled data),\n",
    "    * To simulate unlabeled data we may filter the test dataframe to pick all the fields except the field used for 'ratingCol' i.e. 'rating' field) for that specific userid and let us store that object in a sDF called 'single_user'.\n",
    "    * Transform this unlabeled data using trained_moded, i.e. do a fitted_model<b>.transform</b>(single_user) to get a prediction column in the result.\n",
    "    * sort the result by decreasing order of 'prediction' values { result.orderBy('prediction', ascending='false') }\n",
    "    * The movie with highest prediction is the recommended movie for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised ML Algorithms\n",
    "* <b>Clustering:</b> K-Means Clutering\n",
    "\n",
    "## chin_Clustering_Exercises.ipynb\n",
    "-----------\n",
    "STEPS (K-means clustering)\n",
    "* Load Data\n",
    "* Prepare Data\n",
    "    * null handling (na handling),\n",
    "    * category indexing (StringIndexer) and binary vectoriation (OneHotEncoder)\n",
    "    * vecor assembly transformation from input columns to 'features', the combined vector column (.transform)\n",
    "    * scaling of combined 'features' column to 'scaledFeatures' using StandardScaler (.fit.transform)\n",
    "        * Scaling is very useful when there is a big difference between features and scaledFeatures.\n",
    "        * Scaling is also useful when fields vary in orders of large magnitudes, say one column expressed in thousands of miles and another column in milimeters, then scaling helps.\n",
    "* Create KMeans model with 'scaledFeatures' and desired K value (matching with number of suspected clusters) in the constructor.\n",
    "* Train this model with the vectorized scaled input dataframe and that is our final kmodel (.fit)\n",
    "* kmodel.clusterCenters() gives an list of feature arrays, each array representing a centroid of a cluster\n",
    "    * Each array represents a single point of n-dimension and is called a centroid.\n",
    "    * The number of arrays will be same as value of k used, i.e. number of clusters.\n",
    "    * The count of elements in each array is equal to the number of features.\n",
    "* Run trained_model.transform(sdfScaledVectorFeatures) to get the result dataframe with predictions, where we can check the 'prediction' column. We can also check the 'prediction' column from the model (kmodel.summary.predictions.select('prediction'))\n",
    "* We can evaluate the model using one of the following ways.\n",
    "    1. kmodel.computeCost(sdfScaledVectorFeatures)\n",
    "    2. ClusteringEvaluator('prediction', 'scaledFeatures').evaluate(kmodel.summary.predictions)\n",
    "* In the chapter end consulting project we were expected to find out number of hackers (either 2 or 3 how many?) attacked the software organization, given that number of attacks by each hacker was evenly distributed.\n",
    "    * We clustered with KMeans model with k=2 (for 2 hacker or 2 clusters) and another KMeans model with k=3 represenging 3 hacker clusters.\n",
    "    * We were supposed to find the actual number of hackers so that the number of attacks from each of the clusters are same.\n",
    "    * When we did a groupBy('prediction') for the output dataframe from kmeans_model.transform(scaled_vectoried_hacks_dataframe), we noticed that the number of attacks from each cluster were same for k=2 but the attacks were not evnly distributed fo rk=3. So we concluded that there were two attackers.\n",
    "    * Here another important information was that the Location was unreliable data as hackers used VPN to misrepresent their origin location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_Tree_Methods_Exercises.ipynb  \n",
    "-----------------------------------------\n",
    "CLASSIFIATION: <b>DecisionTreeClassifier, RandomForestClassifier, GBTClassifier for GradientBoostedTree</b>\n",
    "* Load data -- If there are categorical values convert them into categorical indices using StringIndexeer and use them as a feature if input field and label if outout field -- vectorie with VectorAssembler -- test_train_split -- Import the classifiers (DecisionTreeClassifier, RandomForestClassifier, GBTClassifier for GradientBoostedTree) -- instantiate each model and fit the model to train_data -- Then do tree_mnodel.transform(test_data) to get tree_predictions DF -- evaluate using the two evaluators binary and multiclass to get various metric values\n",
    "* Project PURPOSE: A dog food with multiple chemical componets are getting spoiled due to a cemical component, we need to find out the chemical accountable for the sopil.\n",
    "    * Here the 'Spoiled' field is the label and the chemical component fields A,B,C and D are the features which are vectorized by us using Vector Assembler to get the vector column 'features'.\n",
    "    * Here we need to find out featureImporatances against label field to find out the mose impactful feature towards the label the 'Spoiled'.\n",
    "    * So we need not do a train_test_split.\n",
    "    * Create a RandomForestClassifier model and fit it to the full set of data ('features', 'Spoiled') and find the .featureImportances fo rthe trained model. The component with the highest value of featureImportance is most accountable for the spoiled dog food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_3_Logistic_Regression_Consulting_Project.ipynb\n",
    "-----------------------------------------\n",
    "CLASSIFICATION: <b>Logistic Regression</b>\n",
    "* Project PURPOSE\n",
    "To devise a ML prediction model to find customer churn (who will stop buying their service) so as to assign managers to those risky customers\n",
    "* Here 'Churn' field is the label and numeric ones from other fields are the features from which the vectoried 'features' field is generated using VectorAssembler -- Doa trail_test_split and fit the logistic model to the train_data -- get the test_result by evaluating the trained_model against the test_data\n",
    "* Use the evaluators (binary and multiclass) for 'predicrion' column and label field from the test_result to find metrics AUC, acuracy, precision, recall -- Also compare the results from 'prdiction' column against the label field to find manually the difference.\n",
    "* If AUC comes 0.5 or less it is similar to random guess. It should be much above 0.5.\n",
    "* To predict 'Churn' values of a comoletely brand new customer data:\n",
    "    * Retrain the instantiated initial logistic regression model with the pre-split full data\n",
    "    * Load and vectorize the new customer file and treat it as new test data. Note this new data does nto have label i.e. no 'Churn' is there in new data. so we can't run '.evaluate()'. Use '.transform()' on the new fulldata_trained moded to predict on new unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Logistic_Regression_Code_Along.ipynb\n",
    "-----------------------------------------\n",
    "CLASSIFICATION: <b>Logistic Regression</b>\n",
    "\n",
    "* Load Data -- Manually detect categorical fields (Gender & Embarked City) -- Catgory text to index conversion using StringIndexer -- Category index to binary vector conversion using OneHotEncoder (as the catgory values appear to be unrelatd to teach other) -- Use the binary encoded categories along with other participatinf numeric values to create 'features' vector column using VectorAssembler - fit the model to training data and evaluate the trained_model against test_data -- Find the metrics from the evaluated result like AUC, accuracy etc.\n",
    "* In a SECOND approach add to Pipeline, all the stages (indexer for gender and embark, binary encoder for gender and embark, vecror assembler, the instantiated logistic regression model itself. -- Call my_pipeline.fit(train_data) and then pipeline_fitted_model.transform(test_data) to get the final prediction.\n",
    "* Use the evaluators (binary and multiclass) for 'predicrion' column and label field from the test_result to find metrics AUC, acuracy, precision, recall -- Also compare the results from 'prdiction' column against the label field to find manually the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Logistic_Regression_Example.ipynb\n",
    "-----------------------------------------\n",
    "CLASSIFICATION: <b>Logistic Regression</b>\n",
    "\n",
    "Logistic Regression -- Sigmoid function -- Confusion matrix -- Metrics AUC, acuracy, precision, recall -- BinaryClssificationEvaluator -- MulticlassClassificationEvaluator -- Comparison of \"predictions\" DF from test_result from trained_model.evaluate(test_data)\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* Understanding Logistic Regression\n",
    "* Understanding Sigmoid function explaining how to classify any value into binary of 0 or 1.\n",
    "* Understanding confusion matrix, True/False positives, True/False negatives, Accurary, Precision, Recall/Sensitivity, Specificity\n",
    "* Using Evaluators (BinaryClassificationEvaluator, MulticlassClassificationEvaluator) to find AUC, accuracy, recall, precision etc\n",
    "* load data, prepare data (numerify-StringIndexer, vectorize-VectorAssembler, extract features and label), train_test_split-sdf.randomSplit, fit the model to training_data, evaluate on test_data to get test_results with \"predictions\" DF and evaluate this prdictions DF using the two evaluators to find the assocaited metric value\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_3_Linear_Reggression_Consulting_Project.ipynb\n",
    "------------------------------------------------------------------------------\n",
    "CLASSIFICATION: <b>Linear Regression</b>\n",
    "\n",
    "Convert catgorical values into numeric to make a predictor in the ML algorithm (using StringIndexer) -- Then vectorize all numeric fields into a vectorized 'features' column (using VectorAssembler) -- train_test_split -- train or fit the model on train dat -- evaluate the trained model against test data and comapre the results with actual values in test_data -- for very good result be suspicious and find correlate the lael column with some of the predictors or features.\n",
    "\n",
    "Project PURPOSE\n",
    "* This project is to determine number of crew members needed for future ships being built.\n",
    "* Given are various parameters of the ship like number of cruise_line, passengers, cabin count, age of ship, length and weight of ship etc.\n",
    "* Also given the condition that the number of crews may depend on the cruise_line name.\n",
    "\n",
    "\n",
    "<details><summary>For detailed project steps ...EXPAND</summary>\n",
    "\n",
    "Project STEPS:\n",
    "* We are going to create a model to predict 'crew' which is the crew count.\n",
    "* This has a dependency on a string field 'cruise_line'.\n",
    "* For the Spark ML algorithm to work we need to prepare the data consisting of two columns:\n",
    ">1. The features column should be a vector containing details from all the desired numeric fields of the input data and \n",
    ">2. a label field, the 'crew' numeric field to predict.\n",
    "* 'Cruise_line' string field being one of the predictors, we need to convert it from categorical value to a numeric field using \"StringIndexer\".\n",
    ">* str_Indexer = StringIndexer(inputCol='Cruise_line', outputCol='Cruise_line_indexed')\n",
    ">* sdf_cruises_indexed = str_Indexer.fit(sdf_cruises).transform(sdf_cruises)\n",
    "* Then use this indexed field along with other participating numeric fields to generate the vectorized 'features' column, using VectorAssembler, for spark ML libs to process further.\n",
    ">* assembler = VectorAssembler(inputCols=feature_input_col_list, outputCol='features')\n",
    ">* sdf_cruises_indexed_vec_data = assembler.transform(sdf_cruises_indexed)\n",
    "* Pull out the vector column ('features') amd label columne ('crew') into a dataframe and that will be inut to the spark ML library.\n",
    "* split this final data into training data and test data.\n",
    ">* final_data = sdf_cruises_indexed_vec_data.select('features', 'crew')\n",
    ">* train_data, test_data = final_data.randomSplit([0.7, 0.3])\n",
    "* Train the LinearRegression (lr) instance with training data i.e. lr_model = lr.fit(training_data)and \n",
    ">* lr_ship = LinearRegression(featuresCol='features', labelCol='crew', predictionCol='prediction')\n",
    ">* lr_ship_model = lr_ship.fit(train_data)\n",
    "* Evaluate it on test data\n",
    ">* test_Result = lr_model.evaluate(test_data)\n",
    "* Cross validate the error metrics from the result (test_result.rootMeanSquaredError, .r2, .residuals.show() ) and from model print .coefficients, .intercept etc\n",
    "* The R-squared (i.e. .r2) was a bit higher side indicating a good fit, hence we became suspicious and tried to find the cause of good result by trying to find the correlation of the label column ('crew') with few of the ship features like 'passengers' and 'cabins' in the original input dataframe. We found the corr values to be high in the original data and hence the R-squared indicated a good fit.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Linear_Regression_Code_Along.ipynb\n",
    "-----------------------------------------\n",
    "CLASSIFICATION: <b>Linear Regression</b>\n",
    "\n",
    "Use VectorAssembler to prepare 'features', a vectorized summary field with all numeric input values from the record, ML lib of spark uses it -- train_test_split -- fit the model with train data and evaluate trained model against test_Data -- Cross validate the test results (.residuals, .rootMeanSquaredError,...etc). -- Predict a unlabeled data to get 'prediction' column to compare.\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* Decide which numeric columns will be treated as features and which numeric field will be our label, based on the oinput data.\n",
    "* Create a VectorAssembler from the feature columns ([inputCols]) to get a combined vectorized column (outputCol)\n",
    "* Then invoke transform method using vectorAsembler on the input dataframe which will add a new vectorized column to the dataframe and that new field will have the data from all the \"inputCols\"\n",
    "* Now extract a dataframe with the vector column and label columns as this two columns are only needed by spark MLlib to predictionusing Supervised model. This is our final_data for input to our model.\n",
    ">* Create a Vector assembler with a list parameter containing the names of numeric columns and a outputCol holding th ename of the vectorized column that will be added to the result by this assempber.transform() method.\n",
    "* Get the combined merged feature column and the label field as a two-column dataframe and call it 'final_data' to be passed into our model for further operations.\n",
    "* Split the final input data into train_data and test_data in 70:30 ratio.\n",
    "* Train the model on train_data, evaluate the model on test_Data. Let the test_results.residuals be the evaluated labels.\n",
    "* Check the accuracy performance of the model evaluation by checking various parameters of test_result, such as .r2.residuals, rootMeanSquaredError, .meanSuaredError, .meanAbsoluteError etc.\n",
    "* We don't ave any unlabeled data, so we will make the test_data unlabeled so as to predict the labels for those features. As this test_data is not used in training the model, model is not aware of this.\n",
    "* Predict the labels for this unlabeled dataand compare the 'prediction' column of the predicted_result against the test_result,r2,residuals.\n",
    "* Here I have used sparkSession.sql (\"SELECT.  .. .. \") beautifully to combine data from multiple dataframes and to find the deviations in our predictions outout from the actual data in test_data. For this we first registered the participating dataframes as views and then performed complex select statements using these views.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Linear_Regression_Example.ipynb\n",
    "-----------------------------------------\n",
    "CLASSIFICATION: <b>Linear Regression</b>\n",
    "\n",
    "Load Data -- split into train and test sets -- instantiate a model and fit it with train data -- Evaluate the trained model with test data -- Compare the predicted information against the actual labels in test data\n",
    "<details><summary>Detailed steps...EXPAND</summary>\n",
    "    \n",
    "* Loading libsvm formatted text files (contians the verctorized text data using sparkSessn.read.format('libsvm').load(\"libsvm_formatted_text_file.txt\")\n",
    "* The libsvm formatted text file does nto have any column name defined in the file. It contained 'label' in the first column and the trailing columns were in the format 1:float1 2:float2 3:float3 etc. the libsvm format reader of sparkSessin automatically merges all these training columns intelligently and creates a merged column that contains te veector of all these feature columns\n",
    "* In real scenarios we need to prepare the vector column for all features merged into one using VectorAssembler (explained in next chapter)\n",
    "* For me direct session.read.format().load() did not work and I had to use session.read.format('libsvm').option('numFeatures', nFeatrureCount).load(...)\n",
    "* Split this data in 70:30 ratio to traindata and test_data respectively\n",
    "* Instantiate a LinearRegression model and train or fit it with training data.\n",
    "* Now evaluate the model against test data, which compares the predicted labels (from te result test_result) against the labels in test_data.\n",
    "* Check the results by using test)result, .rootMeanSquaredError, .r2.residuals and fw other error metrics such as . meanSquaredError, .meanAbsoluteError etc.\n",
    "* Create a dataset without labels (prepare mimiced data from test_data, which is not seen by the model), by stripping off the labels from test_Data and applying model.transform on this unlabeled data to get the prediction values.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Spark DataFrame Basics-withGroupBy.ipynb\n",
    "---------------------------------------------------------------------------\n",
    "Json datatype changing at runtime while building DataFrame -- using groupBy() on dataframes -- Applying simultaneous aggregates on multiple numeric fields in a groupBy object of a DF -- Ordering a DF ascending (by name or by col) and descending (by col) -- Missing data handling in a DF -- Handling Date fields in a DF\n",
    "\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* printing formatted text in python using '\\\\x1B\\[Nm' where N is a integer (check chinmay_tools.py)\n",
    "\n",
    "* Converting Pandas DataFrame to Spark DataFrame and vice versa. (check chinmay_tools.py)\n",
    "* Using inferSchema attribute during spark DataFrame creation from file to decide teh data types automatically.\n",
    "* Additionally we can customize json schema to change the datatypes in dataframe while loading from json files (repeat)\n",
    "* Renaming a column in a DataFrame using sdf.withColumnRename(old_col_name, new_col_name)\n",
    "* Adding or replacing a column with a different computed value or a masked value e.g. using sdf.withColumn(colName, sdf['col3']+5) or replacing with a string literal using .lit(const_string) function\n",
    "* Using groupBy() on spark DataFrames.\n",
    "* Using aggregates on groupBy objects.\n",
    "* If a DataFrame has multiple numeric fields then we can apply multiple aggregates on a single groBy object with one aggr fn per numeric column.\n",
    "* If multiple aggr fns are applied on a single column then last function on that column is resulted and all previous funcs on that column are ignored. grpByCompany.agg({'col1':'aggfn1', 'col2':'aggfn2', 'col3':'aggfn3'})\n",
    "* Sorting dataframe in ascending order by using orderBy with column name. sdf.orderBy('col1') sorts in ascending order of col1.\n",
    "* Sorting dataframe in descending order by using orderBy with column (but not column name). sdf.orderBy(sdf['col1'].desc()) sorts in descending order of col1.\n",
    "* If we want to sort a dataframe with different orders for each field, then we must use columns for descending ordering but we can use either columns or just column names for thecolumns with ascending order.`\n",
    "* Handling missing data in a DataFrame\n",
    "* Dropping the records if all fields are null, or if any field is null or there are less than n null fields (thres=n). sdf.na.drop(...), \"subset\" attribute can decide which columns to check for null. Here null means na.\n",
    "* Replacing numm values with specified string using sdf.na.fill(...), for any field, for the field / fields listed suing subset or list array. Also replacing with a computed value from an aggregate result of remaining nun-null values.\n",
    "* Handling Date fields using functions from pyspark.sql.functions e.g. year(), month() etc.. or date_format('date_field_name', my_format_string)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Spark DataFrame Basics.ipynb\n",
    "--------------------------------------------------------\n",
    "SQL operations on dataframes -- DataFrame methods .select(), .where(), .filter() -- using dataframes as rdbms tables in  sparkSessn.sql()\n",
    "<details><summary>For details ... EXPAND</summary>\n",
    "\n",
    "* Using modules or functions from personal custom python file by adding th efile path to \"sys.path\"\n",
    "* Using modules or functions from personal ipynb jupyter notebook (less convenient)\n",
    "* Getting help on any module that we are using through question marks\n",
    "* Using a custom defined schema (to change data type) while importing a json into a spark DataFrame\n",
    "* Using dataframe.select().where(), dataframe.where(), dataframe.filter() to get subsets of a dataframe with filtered conditions\n",
    "* Using sparkSessn.sql() to get results from multiple DataFrames by registering the dataframes as views and performing compex select statements on those views.\n",
    "* The \"appName\" parameter we use during the initiation of a SparkSession gets listed in Spark Web UI which is accessile at http://localhost:4040\n",
    "^ A very good documentation on Apache Spark: https://github.com/MingChen0919/learning-apache-spark\n",
    "* Official Spark API Documentation: http://spark.apache.org/docs/latest/api/python/index.html\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chinmay_tools.ipynb\n",
    "-------------\n",
    "Tricks for including methods form ipynb and python custom libraries -- Converting ipynb files into html -- displaying multiple outouts from a notebook cell -- Measuring time taken by a cell --Printing formatted text from python -- PRinting text  files -- GEtting DataFrame from xml files -- Spark DataFrame to Pandas DataFrame conversio and vice versa -- Masking few columns of a DataFrame -- Using LaTeX characters in Markdown document\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* Generating python file from jupyter notebook files (.ipynb) using \"ipynb-py-convert\" application (installe using pip)\n",
    "* Generating html or pdf files from jupyter notebook files using syntax \"jupyter nbconvert -to=html test.ipynb\", which produces test.html. To generate pdf, print the html using system wide pdf rinter. Don't use \"--to=pdf\" which needs a lot of tools to be installed.\n",
    "* Using our python modules or functions in another python / ipynb notebook by adding the fodler location of the python file to \"sys.path\" list\n",
    "* Enabling notebook shells to display multiple print results (without using explicit print statement by setting InteractiveShell.ast_node.interactivity to \"all\".\n",
    "* We can print the time taken by the commands in a cell using \"%%time\" as the first line. %%timeit takes multiple loop to average out the time.\n",
    "* Print formatted text on console concatenating \"\\\\x1B\\[Nm\" and \"\\\\x1B\\[0m\" at begining and at end of a string.\n",
    "* Printing contents of a text file (.txt, .csv, .xml etc)\n",
    "* Convert Pandas DataFrame into a Spark DataFrame and vice versa.\n",
    "* Convert the contents of a Spark DataFrame into Json.\n",
    "* Mask certain colums of a Spark DataFrame using lit() function and sdf.withColumn() function.\n",
    "* Converts the content of a xml into a Pandas DataFrame. The xml format is hardcoded to the call log xml generated by an android applicaiton named \"SuperBackUp\".\n",
    "* Print some LaTeX characters\n",
    "* Print multiple header lines like H1, H2, H3 etc in a single line using <i>'style=\"display: inline\"'</i> css style\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
