{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY POINTS FROM THE COURSE LESSONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_Tree_Methods_Exercises.ipynb\n",
    "-----------------------------------------\n",
    "* Load data -- If there are categorical values convert them into categorical indices using StringIndexeer and use them as a feature if input field and label if outout field -- vectorie with VectorAssembler -- test_train_split -- Import the classifiers (DecisionTreeClassifier, RandomForestClassifier, GBTClassifier for GradientBoostedTree) -- instantiate each model and fit the model to train_data -- Then do tree_mnodel.transform(test_data) to get tree_predictions DF -- evaluate using the two evaluators binary and multiclass to get various metric values\n",
    "* Project PURPOSE: A dog food with multiple chemical componets are getting spoiled due to a cemical component, we need to find out the chemical accountable for the sopil.\n",
    "    * Here the 'Spoiled' field is the label and the chemical component fields A,B,C and D are the features which are vectorized by us using Vector Assembler to get the vector column 'features'.\n",
    "    * Here we need to find out featureImporatances against label field to find out the mose impactful feature towards the label the 'Spoiled'.\n",
    "    * So we need not do a train_test_split.\n",
    "    * Create a RandomForestClassifier model and fit it to the full set of data ('features', 'Spoiled') and find the .featureImportances fo rthe trained model. The component with the highest value of featureImportance is most accountable for the spoiled dog food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_3_Logistic_Regression_Consulting_Project.ipynb\n",
    "-----------------------------------------\n",
    "* Project PURPOSE\n",
    "To devise a ML prediction model to find customer churn (who will stop buying their service) so as to assign managers to those risky customers\n",
    "* Here 'Churn' field is the label and numeric ones from other fields are the features from which the vectoried 'features' field is generated using VectorAssembler -- Doa trail_test_split and fit the logistic model to the train_data -- get the test_result by evaluating the trained_model against the test_data\n",
    "* Use the evaluators (binary and multiclass) for 'predicrion' column and label field from the test_result to find metrics AUC, acuracy, precision, recall -- Also compare the results from 'prdiction' column against the label field to find manually the difference.\n",
    "* If AUC comes 0.5 or less it is similar to random guess. It should be much above 0.5.\n",
    "* To predict 'Churn' values of a comoletely brand new customer data:\n",
    "    * Retrain the instantiated initial logistic regression model with the pre-split full data\n",
    "    * Load and vectorize the new customer file and treat it as new test data. Note this new data does nto have label i.e. no 'Churn' is there in new data. so we can't run '.evaluate()'. Use '.transform()' on the new fulldata_trained moded to predict on new unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Logistic_Regression_Code_Along.ipynb\n",
    "-----------------------------------------\n",
    "* Load Data -- Manually detect categorical fields (Gender & Embarked City) -- Catgory text to index conversion using StringIndexer -- Category index to binary vector conversion using OneHotEncoder (as the catgory values appear to be unrelatd to teach other) -- Use the binary encoded categories along with other participatinf numeric values to create 'features' vector column using VectorAssembler - fit the model to training data and evaluate the trained_model against test_data -- Find the metrics from the evaluated result like AUC, accuracy etc.\n",
    "* In a SECOND approach add to Pipeline, all the stages (indexer for gender and embark, binary encoder for gender and embark, vecror assembler, the instantiated logistic regression model itself. -- Call my_pipeline.fit(train_data) and then pipeline_fitted_model.transform(test_data) to get the final prediction.\n",
    "* Use the evaluators (binary and multiclass) for 'predicrion' column and label field from the test_result to find metrics AUC, acuracy, precision, recall -- Also compare the results from 'prdiction' column against the label field to find manually the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Logistic_Regression_Example.ipynb\n",
    "-----------------------------------------\n",
    "Logistic Regression -- Sigmoid function -- Confusion matrix -- Metrics AUC, acuracy, precision, recall -- BinaryClssificationEvaluator -- MulticlassClassificationEvaluator -- Comparison of \"predictions\" DF from test_result from trained_model.evaluate(test_data)\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* Understanding Logistic Regression\n",
    "* Understanding Sigmoid function explaining how to classify any value into binary of 0 or 1.\n",
    "* Understanding confusion matrix, True/False positives, True/False negatives, Accurary, Precision, Recall/Sensitivity, Specificity\n",
    "* Using Evaluators (BinaryClassificationEvaluator, MulticlassClassificationEvaluator) to find AUC, accuracy, recall, precision etc\n",
    "* load data, prepare data (numerify-StringIndexer, vectorize-VectorAssembler, extract features and label), train_test_split-sdf.randomSplit, fit the model to training_data, evaluate on test_data to get test_results with \"predictions\" DF and evaluate this prdictions DF using the two evaluators to find the assocaited metric value\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_3_Linear_Reggression_Consulting_Project.ipynb\n",
    "------------------------------------------------------------------------------\n",
    "Convert catgorical values into numeric to make a predictor in the ML algorithm (using StringIndexer) -- Then vectorize all numeric fields into a vectorized 'features' column (using VectorAssembler) -- train_test_split -- train or fit the model on train dat -- evaluate the trained model against test data and comapre the results with actual values in test_data -- for very good result be suspicious and find correlate the lael column with some of the predictors or features.\n",
    "\n",
    "Project PURPOSE\n",
    "* This project is to determine number of crew members needed for future ships being built.\n",
    "* Given are various parameters of the ship like number of cruise_line, passengers, cabin count, age of ship, length and weight of ship etc.\n",
    "* Also given the condition that the number of crews may depend on the cruise_line name.\n",
    "\n",
    "\n",
    "<details><summary>For detailed project steps ...EXPAND</summary>\n",
    "\n",
    "Project STEPS:\n",
    "* We are going to create a model to predict 'crew' which is the crew count.\n",
    "* This has a dependency on a string field 'cruise_line'.\n",
    "* For the Spark ML algorithm to work we need to prepare the data consisting of two columns:\n",
    ">1. The features column should be a vector containing details from all the desired numeric fields of the input data and \n",
    ">2. a label field, the 'crew' numeric field to predict.\n",
    "* 'Cruise_line' string field being one of the predictors, we need to convert it from categorical value to a numeric field using \"StringIndexer\".\n",
    ">* str_Indexer = StringIndexer(inputCol='Cruise_line', outputCol='Cruise_line_indexed')\n",
    ">* sdf_cruises_indexed = str_Indexer.fit(sdf_cruises).transform(sdf_cruises)\n",
    "* Then use this indexed field along with other participating numeric fields to generate the vectorized 'features' column, using VectorAssembler, for spark ML libs to process further.\n",
    ">* assembler = VectorAssembler(inputCols=feature_input_col_list, outputCol='features')\n",
    ">* sdf_cruises_indexed_vec_data = assembler.transform(sdf_cruises_indexed)\n",
    "* Pull out the vector column ('features') amd label columne ('crew') into a dataframe and that will be inut to the spark ML library.\n",
    "* split this final data into training data and test data.\n",
    ">* final_data = sdf_cruises_indexed_vec_data.select('features', 'crew')\n",
    ">* train_data, test_data = final_data.randomSplit([0.7, 0.3])\n",
    "* Train the LinearRegression (lr) instance with training data i.e. lr_model = lr.fit(training_data)and \n",
    ">* lr_ship = LinearRegression(featuresCol='features', labelCol='crew', predictionCol='prediction')\n",
    ">* lr_ship_model = lr_ship.fit(train_data)\n",
    "* Evaluate it on test data\n",
    ">* test_Result = lr_model.evaluate(test_data)\n",
    "* Cross validate the error metrics from the result (test_result.rootMeanSquaredError, .r2, .residuals.show() ) and from model print .coefficients, .intercept etc\n",
    "* The R-squared (i.e. .r2) was a bit higher side indicating a good fit, hence we became suspicious and tried to find the cause of good result by trying to find the correlation of the label column ('crew') with few of the ship features like 'passengers' and 'cabins' in the original input dataframe. We found the corr values to be high in the original data and hence the R-squared indicated a good fit.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Linear_Regression_Code_Along.ipynb\n",
    "-----------------------------------------\n",
    "Use VectorAssembler to prepare 'features', a vectorized summary field with all numeric input values from the record, ML lib of spark uses it -- train_test_split -- fit the model with train data and evaluate trained model against test_Data -- Cross validate the test results (.residuals, .rootMeanSquaredError,...etc). -- Predict a unlabeled data to get 'prediction' column to compare.\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* Decide which numeric columns will be treated as features and which numeric field will be our label, based on the oinput data.\n",
    "* Create a VectorAssembler from the feature columns ([inputCols]) to get a combined vectorized column (outputCol)\n",
    "* Then invoke transform method using vectorAsembler on the input dataframe which will add a new vectorized column to the dataframe and that new field will have the data from all the \"inputCols\"\n",
    "* Now extract a dataframe with the vector column and label columns as this two columns are only needed by spark MLlib to predictionusing Supervised model. This is our final_data for input to our model.\n",
    ">* Create a Vector assembler with a list parameter containing the names of numeric columns and a outputCol holding th ename of the vectorized column that will be added to the result by this assempber.transform() method.\n",
    "* Get the combined merged feature column and the label field as a two-column dataframe and call it 'final_data' to be passed into our model for further operations.\n",
    "* Split the final input data into train_data and test_data in 70:30 ratio.\n",
    "* Train the model on train_data, evaluate the model on test_Data. Let the test_results.residuals be the evaluated labels.\n",
    "* Check the accuracy performance of the model evaluation by checking various parameters of test_result, such as .r2.residuals, rootMeanSquaredError, .meanSuaredError, .meanAbsoluteError etc.\n",
    "* We don't ave any unlabeled data, so we will make the test_data unlabeled so as to predict the labels for those features. As this test_data is not used in training the model, model is not aware of this.\n",
    "* Predict the labels for this unlabeled dataand compare the 'prediction' column of the predicted_result against the test_result,r2,residuals.\n",
    "* Here I have used sparkSession.sql (\"SELECT.  .. .. \") beautifully to combine data from multiple dataframes and to find the deviations in our predictions outout from the actual data in test_data. For this we first registered the participating dataframes as views and then performed complex select statements using these views.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Linear_Regression_Example.ipynb\n",
    "-----------------------------------------\n",
    "Load Data -- split into train and test sets -- instantiate a model and fit it with train data -- Evaluate the trained model with test data -- Compare the predicted information against the actual labels in test data\n",
    "<details><summary>Detailed steps...EXPAND</summary>\n",
    "    \n",
    "* Loading libsvm formatted text files (contians the verctorized text data using sparkSessn.read.format('libsvm').load(\"libsvm_formatted_text_file.txt\")\n",
    "* The libsvm formatted text file does nto have any column name defined in the file. It contained 'label' in the first column and the trailing columns were in the format 1:float1 2:float2 3:float3 etc. the libsvm format reader of sparkSessin automatically merges all these training columns intelligently and creates a merged column that contains te veector of all these feature columns\n",
    "* In real scenarios we need to prepare the vector column for all features merged into one using VectorAssembler (explained in next chapter)\n",
    "* For me direct session.read.format().load() did not work and I had to use session.read.format('libsvm').option('numFeatures', nFeatrureCount).load(...)\n",
    "* Split this data in 70:30 ratio to traindata and test_data respectively\n",
    "* Instantiate a LinearRegression model and train or fit it with training data.\n",
    "* Now evaluate the model against test data, which compares the predicted labels (from te result test_result) against the labels in test_data.\n",
    "* Check the results by using test)result, .rootMeanSquaredError, .r2.residuals and fw other error metrics such as . meanSquaredError, .meanAbsoluteError etc.\n",
    "* Create a dataset without labels (prepare mimiced data from test_data, which is not seen by the model), by stripping off the labels from test_Data and applying model.transform on this unlabeled data to get the prediction values.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_2_Spark DataFrame Basics-withGroupBy.ipynb\n",
    "---------------------------------------------------------------------------\n",
    "Json datatype changing at runtime while building DataFrame -- using groupBy() on dataframes -- Applying simultaneous aggregates on multiple numeric fields in a groupBy object of a DF -- Ordering a DF ascending (by name or by col) and descending (by col) -- Missing data handling in a DF -- Handling Date fields in a DF\n",
    "\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* printing formatted text in python using '\\\\x1B\\[Nm' where N is a integer (check chinmay_tools.py)\n",
    "\n",
    "* Converting Pandas DataFrame to Spark DataFrame and vice versa. (check chinmay_tools.py)\n",
    "* Using inferSchema attribute during spark DataFrame creation from file to decide teh data types automatically.\n",
    "* Additionally we can customize json schema to change the datatypes in dataframe while loading from json files (repeat)\n",
    "* Renaming a column in a DataFrame using sdf.withColumnRename(old_col_name, new_col_name)\n",
    "* Adding or replacing a column with a different computed value or a masked value e.g. using sdf.withColumn(colName, sdf['col3']+5) or replacing with a string literal using .lit(const_string) function\n",
    "* Using groupBy() on spark DataFrames.\n",
    "* Using aggregates on groupBy objects.\n",
    "* If a DataFrame has multiple numeric fields then we can apply multiple aggregates on a single groBy object with one aggr fn per numeric column.\n",
    "* If multiple aggr fns are applied on a single column then last function on that column is resulted and all previous funcs on that column are ignored. grpByCompany.agg({'col1':'aggfn1', 'col2':'aggfn2', 'col3':'aggfn3'})\n",
    "* Sorting dataframe in ascending order by using orderBy with column name. sdf.orderBy('col1') sorts in ascending order of col1.\n",
    "* Sorting dataframe in descending order by using orderBy with column (but not column name). sdf.orderBy(sdf['col1'].desc()) sorts in descending order of col1.\n",
    "* If we want to sort a dataframe with different orders for each field, then we must use columns for descending ordering but we can use either columns or just column names for thecolumns with ascending order.`\n",
    "* Handling missing data in a DataFrame\n",
    "* Dropping the records if all fields are null, or if any field is null or there are less than n null fields (thres=n). sdf.na.drop(...), \"subset\" attribute can decide which columns to check for null. Here null means na.\n",
    "* Replacing numm values with specified string using sdf.na.fill(...), for any field, for the field / fields listed suing subset or list array. Also replacing with a computed value from an aggregate result of remaining nun-null values.\n",
    "* Handling Date fields using functions from pyspark.sql.functions e.g. year(), month() etc.. or date_format('date_field_name', my_format_string)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chin_1_Spark DataFrame Basics.ipynb\n",
    "--------------------------------------------------------\n",
    "SQL operations on dataframes -- DataFrame methods .select(), .where(), .filter() -- using dataframes as rdbms tables in  sparkSessn.sql()\n",
    "<details><summary>For details ... EXPAND</summary>\n",
    "\n",
    "* Using modules or functions from personal custom python file by adding th efile path to \"sys.path\"\n",
    "* Using modules or functions from personal ipynb jupyter notebook (less convenient)\n",
    "* Getting help on any module that we are using through question marks\n",
    "* Using a custom defined schema (to change data type) while importing a json into a spark DataFrame\n",
    "* Using dataframe.select().where(), dataframe.where(), dataframe.filter() to get subsets of a dataframe with filtered conditions\n",
    "* Using sparkSessn.sql() to get results from multiple DataFrames by registering the dataframes as views and performing compex select statements on those views.\n",
    "* The \"appName\" parameter we use during the initiation of a SparkSession gets listed in Spark Web UI which is accessile at http://localhost:4040\n",
    "^ A very good documentation on Apache Spark: https://github.com/MingChen0919/learning-apache-spark\n",
    "* Official Spark API Documentation: http://spark.apache.org/docs/latest/api/python/index.html\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chinmay_tools.ipynb\n",
    "-------------\n",
    "Tricks for including methods form ipynb and python custom libraries -- Converting ipynb files into html -- displaying multiple outouts from a notebook cell -- Measuring time taken by a cell --Printing formatted text from python -- PRinting text  files -- GEtting DataFrame from xml files -- Spark DataFrame to Pandas DataFrame conversio and vice versa -- Masking few columns of a DataFrame -- Using LaTeX characters in Markdown document\n",
    "<details><summary>For details...EXPAND</summary>\n",
    "\n",
    "* Generating python file from jupyter notebook files (.ipynb) using \"ipynb-py-convert\" application (installe using pip)\n",
    "* Generating html or pdf files from jupyter notebook files using syntax \"jupyter nbconvert -to=html test.ipynb\", which produces test.html. To generate pdf, print the html using system wide pdf rinter. Don't use \"--to=pdf\" which needs a lot of tools to be installed.\n",
    "* Using our python modules or functions in another python / ipynb notebook by adding the fodler location of the python file to \"sys.path\" list\n",
    "* Enabling notebook shells to display multiple print results (without using explicit print statement by setting InteractiveShell.ast_node.interactivity to \"all\".\n",
    "* We can print the time taken by the commands in a cell using \"%%time\" as the first line. %%timeit takes multiple loop to average out the time.\n",
    "* Print formatted text on console concatenating \"\\\\x1B\\[Nm\" and \"\\\\x1B\\[0m\" at begining and at end of a string.\n",
    "* Printing contents of a text file (.txt, .csv, .xml etc)\n",
    "* Convert Pandas DataFrame into a Spark DataFrame and vice versa.\n",
    "* Convert the contents of a Spark DataFrame into Json.\n",
    "* Mask certain colums of a Spark DataFrame using lit() function and sdf.withColumn() function.\n",
    "* Converts the content of a xml into a Pandas DataFrame. The xml format is hardcoded to the call log xml generated by an android applicaiton named \"SuperBackUp\".\n",
    "* Print some LaTeX characters\n",
    "* Print multiple header lines like H1, H2, H3 etc in a single line using <i>'style=\"display: inline\"'</i> css style\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
