{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using the functions from one .ipynb notenook in another .ipynb file\n",
    "* https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n",
    "* https://github.com/ipython/ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enable the shell to print multiple results (instead of only the last result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable the shell to print multiple results (instead of only the last result)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conversion from Spark DataFrame to a Pandas DataFrame and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def getSparkDFfromPandasDF(pandasDF):\n",
    "    \"\"\"\n",
    "    Takes a Pandas DataFrame and converts it into a Spark DataFrame\n",
    "    \"\"\"\n",
    "    tempSparkSession = SparkSession.builder.appName(\"chin_conv\").getOrCreate()\n",
    "\n",
    "    # Enable Arrow-based columnar data transfers\n",
    "    tempSparkSession.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    # on some machines it may give a warning which may be ignored\n",
    "\n",
    "    # Generate a random pandas DataFrame\n",
    "    # temp_pandasDF = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "    # Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "    sparkDF = tempSparkSession.createDataFrame(pandasDF)\n",
    "    \n",
    "    return sparkDF\n",
    "\n",
    "def getPandasDFfromSparkDF(sparkDF):\n",
    "    \"\"\"\n",
    "    Takes a Spark DataFrame and converts it into a Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "    result_pandasDF = sparkDF.select(\"*\").toPandas()\n",
    "    return result_pandasDF\n",
    "\n",
    "import json\n",
    "import pyspark\n",
    "\n",
    "\"\"\"\n",
    "Here I am importing full pyspark for readability\n",
    "in the method getJsonFromSparkDF(), I am validating whether the parameter is a spark DataFrame\n",
    "I could hev comapred with DataFrame after doing a selective import (from pyspark.sql.dataframe import DataFrame)\n",
    "But the users may get confuse it with pandas DataFrame if they accidentally miss to notice the import statement.\n",
    "\n",
    "Now I am using fully qualitied modules for comparison, thus improving readability\n",
    "\"\"\"\n",
    "\n",
    "def getJsonFromSparkDF(sparkDF):\n",
    "    \"\"\"\n",
    "    Takes a Spark DataFrame and processes its elements as a list of Spark Rows (which is returned from a sparkDF.collect()), i.e. type of each list member is pyspark.sql.types.Row\n",
    "    Converts the input into a Json and returns it.\n",
    "    If input is not a list of spark rows or invalid, it returns an empty json string\n",
    "    sdfCallLogs.collect()\n",
    "    \"\"\"\n",
    "    if not (isinstance(sparkDF, pyspark.sql.dataframe.DataFrame)):\n",
    "        return '{}'\n",
    "    sparkRowList = sparkDF.collect()               # The collect() method returns the sparkDF rows as a list of spark rows\n",
    "    if not (isinstance(sparkRowList, list)):\n",
    "        return '{}'\n",
    "    if not (isinstance(sparkRowList[0], pyspark.sql.types.Row)):\n",
    "        return '{}'\n",
    "\n",
    "    resultJson = json.dumps(sparkRowList[0].asDict())\n",
    "\n",
    "    # For second element onwards append the json for individual elements to the result with a comma separator\n",
    "    for x in range(len(sparkRowList)-1):\n",
    "        resultJson += (\", \" + json.dumps(sparkRowList[x+1].asDict()))  # This starts from second row onwards as first is already added to output\n",
    "    return resultJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading call logs (from Superbackup app) in python (where values are node attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "def getCallLogXmlFromSuperbackup(call_log_xml_file=\"calllogs_20200512130135.xml\"):\n",
    "    \"\"\"\n",
    "    Parses the exported call logs (xml) from the SuperBackUp android application. The xml lfile structure is \"alllogs^log\"\n",
    "    The each of the log record has attributes [\"number\", \"time\", \"date\", \"type\", \"name\", \"duration\"].\n",
    "    To read the xml we use the module \"xml.etree.ElementTree\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    call_log_xml_file : A string representing the xml file name (may be a full path) - This is the log file produced from teh SuperBackUp android application.\n",
    "\n",
    "    \"\"\"\n",
    "    tree = etree.parse(call_log_xml_file)\n",
    "    root = tree.getroot()\n",
    "    columns = [\"number\", \"time\", \"date\", \"type\", \"name\", \"dur\"] #The column list is closely tied to the call log xml\n",
    "    df_Calllogs = pd.DataFrame(columns = columns)\n",
    "\n",
    "    for node in root: \n",
    "        number = node.attrib.get(\"number\")\n",
    "        time = node.attrib.get(\"time\") # if node is not None else None\n",
    "        date = node.attrib.get(\"date\")\n",
    "        type = node.attrib.get(\"type\")\n",
    "        name = node.attrib.get(\"name\")\n",
    "        # name = node.find(\"name\")\n",
    "        dur = node.attrib.get(\"dur\")\n",
    "        df_Calllogs = df_Calllogs.append(pd.Series([number, time, date, type, name, dur], index = columns), ignore_index = True)\n",
    "    \n",
    "    return df_Calllogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Plain Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from codecs import open\n",
    "# Print contents of a file\n",
    "\n",
    "def printTextFile(file_name):\n",
    "    \"\"\"\n",
    "    Opens a text file (txt, csv, xml etc..) in 'utf-8' encoding format and prints its contents.\n",
    "    params\n",
    "    ------\n",
    "    It can accept a relative path or a full path to a file in the same file system as this utility\n",
    "    \"\"\"\n",
    "    f = open(file_name, 'r', encoding='utf-8')\n",
    "    file_contents = f.read()\n",
    "    print (file_contents)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion between Pandas DataFrame and Spark DataFrame\n",
    "###### https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html\n",
    "###### https://stackoverflow.com/questions/50958721/convert-a-spark-dataframe-to-pandas-df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.appName(\"chin_conv\").getOrCreate()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "* Enable Arrow-based columnar data transfers\n",
    "spark1.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "* Generate a pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "* Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "sdf = spark1.createDataFrame(df_Calllogs)\n",
    "\n",
    "* Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = sdf.select(\"*\").toPandas()\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_xml_file = \"calllogs_20200512130135.xml\"\n",
    "my_json_file = \"../Python-and-Spark-for-Big-Data-master/Spark_DataFrames/people.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTextFile(my_xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Calllogs[df_Calllogs[\"number\"]==\"+919052656567\"]\n",
    "#df_Calllogs[\"duration\"].max()\n",
    "# df_Calllogs\n",
    "\n",
    "### %timeit getCallLogXmlFromSuperbackup()\n",
    "### 7.65 s ± 201 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "p_df = getCallLogXmlFromSuperbackup()    # returns teh sample call log data as a pandas DataFrame\n",
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_df = getSparkDFfromPandasDF(p_df)\n",
    "s_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_p_df = getPandasDFfromSparkDF(s_df)\n",
    "temp_p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getJsonFromSparkDF([1,2,3])  # Testing for wrong input\n",
    "\n",
    "callog_json = getJsonFromSparkDF(s_df)\n",
    "callog_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
